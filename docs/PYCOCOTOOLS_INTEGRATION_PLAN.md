# PycocoTools Integration Plan for VideoAnnotator

## Executive Summary

**Objective**: Integrate pycocotools into VideoAnnotator pipelines to leverage industry-standard COCO format validation, evaluation, and utilities while maintaining support for non-visual data (audio).

**Strategy**: 
- âœ… **Tier 1**: Core COCO integration for visual pipelines (scene, person, face)
- ðŸ”„ **Tier 2**: Audio pipeline uses specialized audio standards
- ðŸš€ **Tier 3**: Optional advanced tools (Datumaro, FiftyOne)

## Pipeline Analysis & Integration Plan

### âœ… COCO-Compatible Visual Pipelines

#### 1. Scene Detection Pipeline
**Current Status**: âœ… Already using COCOImageAnnotation format
**COCO Applicability**: ðŸŸ¢ **PERFECT FIT**
- Scene classification as image-level labels 
- Full-frame bounding boxes for temporal segments
- Categories: indoor/outdoor, room types, environments

**Integration Actions**:
```python
# Add pycocotools validation
from pycocotools.coco import COCO
from pycocotools import mask
import pycocotools.cocoeval as cocoeval

# Validate COCO JSON output
def validate_scene_annotations(coco_json_path):
    coco = COCO(coco_json_path)
    # Automatic validation on load
    return coco

# Export with pycocotools compatibility
scene_annotations = scene_pipeline.process(video_path)
coco_dict = export_to_coco_json(scene_annotations, metadata)
validate_scene_annotations(coco_dict)
```

#### 2. Person Tracking Pipeline  
**Current Status**: ðŸ”„ Using custom PersonDetection schema
**COCO Applicability**: ðŸŸ¢ **EXCELLENT FIT**
- Person detection (COCO category_id: 1)
- Bounding boxes in COCO format
- Pose keypoints in COCO-17 format
- Tracking IDs as custom extension

**Integration Actions**:
```python
# Convert to COCO format
class COCOPersonDetection(BaseModel):
    id: str
    image_id: str  # frame identifier
    category_id: int = 1  # Person class in COCO
    bbox: List[float]  # [x, y, width, height]
    area: float
    score: float
    keypoints: Optional[List[float]] = None  # COCO-17 format
    track_id: Optional[int] = None  # VideoAnnotator extension
    
# Add pose keypoints in COCO format
def convert_to_coco_keypoints(yolo_keypoints):
    # COCO-17: [x1,y1,v1, x2,y2,v2, ...]
    # v = 0 (not labeled), 1 (labeled but not visible), 2 (labeled and visible)
    coco_keypoints = []
    for kp in yolo_keypoints:
        coco_keypoints.extend([kp.x, kp.y, 2 if kp.visible else 1])
    return coco_keypoints
```

#### 3. Face Analysis Pipeline
**Current Status**: ðŸ”„ Using custom FaceDetection schema  
**COCO Applicability**: ðŸŸ¡ **GOOD FIT** with extensions
- Face detection as objects (custom category)
- Facial landmarks as keypoints
- Emotion/demographics as custom attributes

**Integration Actions**:
```python
# Add face category to COCO
COCO_FACE_CATEGORY = {
    "id": 100,  # Custom category ID
    "name": "face",
    "supercategory": "person"
}

# Face landmarks in COCO keypoint format
FACE_LANDMARK_SKELETON = [
    [1, 2],  # left_eye - right_eye
    [3, 4],  # nose connections
    # ... facial landmark connections
]

class COCOFaceDetection(BaseModel):
    id: str
    image_id: str
    category_id: int = 100  # Face category
    bbox: List[float]
    area: float
    score: float
    keypoints: Optional[List[float]] = None  # Facial landmarks
    emotion: Optional[str] = None  # Custom attribute
    age: Optional[float] = None   # Custom attribute
```

### ðŸš« Non-COCO Pipeline: Audio Processing

#### Audio Pipeline Analysis
**Current Status**: Using custom audio schemas
**COCO Applicability**: ðŸ”´ **NOT APPLICABLE**
- Audio has no visual/spatial components
- Temporal data without bounding boxes
- Speech transcription, diarization, audio events

**Alternative Standards**:
```python
# Use specialized audio standards instead
AUDIO_STANDARDS = {
    "speech_recognition": "WebVTT/SRT format",
    "speaker_diarization": "RTTM (Rich Transcription Time Marked)",
    "audio_events": "AudioSet ontology",
    "music_analysis": "JAMS (JSON Annotated Music Specification)"
}

# Example: WebVTT for speech transcription
class WebVTTTranscript(BaseModel):
    start_time: float
    end_time: float
    text: str
    speaker_id: Optional[str] = None
    confidence: float

# Example: RTTM for speaker diarization  
class RTTMSegment(BaseModel):
    type: str = "SPEAKER"
    file_id: str
    channel: int = 1
    start_time: float
    duration: float
    speaker_id: str
    confidence: Optional[float] = None
```

## Implementation Roadmap

### Phase 1: Core PycocoTools Integration (Immediate)

**Week 1: Setup & Dependencies**
```bash
# Add to requirements.txt
pycocotools>=2.0.7
```

**Week 2: Scene Pipeline Enhancement**
- âœ… Scene pipeline already uses COCO format
- ðŸ”„ Add pycocotools validation
- ðŸ”„ Add evaluation metrics for scene classification

**Week 3: Person Pipeline Migration**
- ðŸ”„ Convert PersonDetection to COCOPersonDetection
- ðŸ”„ Implement COCO-17 pose keypoint format
- ðŸ”„ Add tracking ID as COCO extension
- ðŸ”„ Validate with pycocotools

**Week 4: Face Pipeline Migration**  
- ðŸ”„ Convert FaceDetection to COCOFaceDetection
- ðŸ”„ Map facial landmarks to COCO keypoint format
- ðŸ”„ Handle emotion/demographics as custom attributes

### Phase 2: Advanced Features (Next Month)

**Validation & Quality Assurance**
```python
# Comprehensive COCO validation
def validate_all_annotations(annotations_dir):
    results = {}
    for pipeline in ['scene', 'person', 'face']:
        coco_file = f"{annotations_dir}/{pipeline}_coco.json"
        try:
            coco = COCO(coco_file)
            results[pipeline] = "âœ… Valid COCO format"
        except Exception as e:
            results[pipeline] = f"âŒ Invalid: {e}"
    return results
```

**Evaluation Metrics**
```python
# COCO evaluation for person detection
def evaluate_person_detection(gt_coco, pred_coco):
    coco_eval = COCOeval(gt_coco, pred_coco, 'bbox')
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()
    return coco_eval.stats

# Pose evaluation  
def evaluate_pose_estimation(gt_coco, pred_coco):
    coco_eval = COCOeval(gt_coco, pred_coco, 'keypoints')
    coco_eval.params.kpt_oks_sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0
    coco_eval.evaluate()
    coco_eval.accumulate() 
    coco_eval.summarize()
    return coco_eval.stats
```

### Phase 3: Tool Ecosystem Integration (Future)

**FiftyOne Integration**
```python
# Load VideoAnnotator COCO data into FiftyOne
import fiftyone as fo

def load_into_fiftyone(video_path, annotations_dir):
    dataset = fo.Dataset()
    dataset.add_video_samples([video_path])
    
    # Load COCO annotations
    dataset.add_coco_labels(
        f"{annotations_dir}/person_coco.json",
        "person_detections"
    )
    
    # Visualize in FiftyOne App
    session = fo.launch_app(dataset)
    return session
```

**Datumaro Integration**
```python
# Convert between formats using Datumaro
from datumaro import Dataset

def export_all_formats(coco_json_path, output_dir):
    dataset = Dataset.import_from(coco_json_path, 'coco')
    
    # Export to multiple formats
    dataset.export(f"{output_dir}/cvat", 'cvat')
    dataset.export(f"{output_dir}/labelstudio", 'label_studio') 
    dataset.export(f"{output_dir}/yolo", 'yolo')
    dataset.export(f"{output_dir}/voc", 'voc')
```

## Updated Pipeline Architecture

### New Data Flow with PycocoTools
```mermaid
graph TD
    A[Video Input] --> B[Scene Pipeline]
    A --> C[Person Pipeline] 
    A --> D[Face Pipeline]
    A --> E[Audio Pipeline]
    
    B --> F[COCOImageAnnotation]
    C --> G[COCOPersonDetection] 
    D --> H[COCOFaceDetection]
    E --> I[AudioStandards]
    
    F --> J[pycocotools validation]
    G --> J
    H --> J
    
    J --> K[COCO JSON Export]
    K --> L[Tool Ecosystem]
    
    I --> M[Audio Format Export]
    M --> N[Audio Tools]
```

### File Structure Updates
```
src/
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ standards_compatible_schemas.py  # âœ… Already done
â”‚   â”œâ”€â”€ coco_extensions.py              # ðŸ”„ New: VideoAnnotator COCO extensions
â”‚   â””â”€â”€ audio_standards.py              # ðŸ”„ New: WebVTT, RTTM, AudioSet formats
â”œâ”€â”€ validation/
â”‚   â”œâ”€â”€ coco_validator.py               # ðŸ”„ New: pycocotools integration
â”‚   â””â”€â”€ audio_validator.py              # ðŸ”„ New: audio format validation
â””â”€â”€ evaluation/
    â”œâ”€â”€ coco_metrics.py                 # ðŸ”„ New: COCO evaluation metrics
    â””â”€â”€ audio_metrics.py                # ðŸ”„ New: audio evaluation metrics
```

## Benefits of This Approach

### âœ… Visual Pipelines (Scene, Person, Face)
- **Industry Standard**: Direct compatibility with 90% of CV tools
- **Zero Learning Curve**: Researchers already know COCO format
- **Rich Ecosystem**: pycocotools, FiftyOne, Detectron2, etc.
- **Evaluation Metrics**: Standard mAP, precision/recall calculations
- **Future Proof**: Evolves with computer vision community

### âœ… Audio Pipeline (Custom Standards)
- **Domain Appropriate**: Uses audio-specific standards (WebVTT, RTTM)
- **Tool Compatibility**: Works with speech/audio analysis tools
- **Temporal Focus**: Designed for time-series audio data
- **Specialized Metrics**: Audio-specific evaluation methods

### âœ… Overall Architecture
- **Best of Both Worlds**: Industry standards where applicable
- **No Force-Fitting**: Audio uses appropriate formats
- **Maintenance Reduction**: Leverage community-maintained tools
- **Professional Output**: Industry-grade annotation formats

## Migration Timeline

| Week | Visual Pipelines | Audio Pipeline | Integration |
|------|------------------|----------------|-------------|
| 1 | pycocotools setup | WebVTT/RTTM research | Requirements update |
| 2 | Scene validation | Audio standards implementation | Documentation |
| 3 | Person COCO migration | Audio format export | Testing |
| 4 | Face COCO migration | Audio validation | Performance benchmarks |

## Success Metrics

### Technical Validation
- âœ… All visual annotations pass pycocotools validation
- âœ… Audio annotations validate against WebVTT/RTTM standards
- âœ… 100% compatibility with target tools (FiftyOne, CVAT, etc.)
- âœ… Performance maintains <2x overhead vs custom formats

### Ecosystem Integration
- âœ… Direct import into 5+ annotation tools
- âœ… Compatible with 3+ ML training frameworks
- âœ… Community format adoption confirmed

## Conclusion

This plan provides VideoAnnotator with:

1. **ðŸŽ¯ Targeted Standards Adoption**: COCO for visual data, specialized standards for audio
2. **ðŸ”§ Industry Tool Compatibility**: Direct integration with professional tools
3. **ðŸ“ˆ Future Scalability**: Community-maintained standards that evolve
4. **ðŸ§¹ Simplified Maintenance**: Reduced custom format burden
5. **âš¡ Performance Optimization**: Leverage optimized industry parsers

The hybrid approach ensures we use the right standard for each data type while maximizing interoperability and minimizing maintenance overhead.
