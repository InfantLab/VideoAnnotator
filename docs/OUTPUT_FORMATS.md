# VideoAnnotator Output Formats

This document explains the standardized JSON data formats generated by VideoAnnotator pipelines. All outputs follow simplified, specification-compliant schemas that prioritize interoperability and ease of use.

## Overview

VideoAnnotator uses a modern JSON-based output system designed for research flexibility and annotation tool compatibility. All pipelines generate structured JSON files with:

- **Simple, clean schemas** with minimal validation overhead
- **Flexible ID support** (strings or integers as needed) 
- **Research-friendly formats** that match original specifications
- **Annotation tool compatibility** (CVAT, LabelStudio, ELAN)
- **Extensible design** allowing model-specific fields

## Core Data Format Principles

VideoAnnotator follows these key principles in its output formats:

### ✅ Simple JSON Structure
All outputs use `List[dict]` format for maximum interoperability:
```json
[
  {
    "type": "person_bbox",
    "video_id": "vid123",
    "t": 12.34,
    "bbox": [x, y, w, h],
    "person_id": 1,
    "confidence": 0.87
  }
]
```

### ✅ Flexible ID Support
- **String IDs**: `"face_001"`, `"speaker_001"` for human readability
- **Integer IDs**: `1`, `2`, `3` for computational efficiency  
- **Mixed usage**: Pipelines can use what works best

### ✅ Extension Friendly
Models can add custom fields:
```json
{
  "type": "facial_emotion",
  "emotion": "happy",
  "confidence": 0.91,
  "model_specific_score": 0.78,
  "custom_field": "value"
}
```

## Pipeline Output Formats

All pipelines output simple JSON arrays following these core schemas. Each annotation includes basic temporal and spatial information with pipeline-specific extensions.

### 1. Person Detection & Tracking

**File:** `{video_id}_person_tracking.json`

```json
[
  {
    "type": "person_bbox",
    "video_id": "example_video", 
    "t": 12.34,
    "bbox": [0.23, 0.45, 0.15, 0.25],
    "person_id": 1,
    "confidence": 0.87,
    "tracking_id": "track_001"
  }
]
```

**Core Fields:**
- `type`: Always "person_bbox"
- `video_id`: Video identifier
- `t`: Timestamp in seconds
- `bbox`: [x, y, width, height] in normalized coordinates (0-1)
- `person_id`: Unique person identifier (int or string)
- `confidence`: Detection confidence (0-1)

### 2. Face Analysis

**File:** `{video_id}_face_analysis.json`

```json
[
  {
    "type": "facial_emotion",
    "video_id": "example_video",
    "t": 12.34,
    "person_id": 1,
    "bbox": [0.45, 0.23, 0.12, 0.18],
    "emotion": "happy",
    "confidence": 0.91,
    "face_id": "face_001"
  }
]
```

**Core Fields:**
- `type`: Always "facial_emotion"
- `emotion`: Primary detected emotion
- `face_id`: Unique face identifier
- `person_id`: Link to person detection

### 3. Speech Recognition

**File:** `{video_id}_speech_recognition.json`

```json
[
  {
    "type": "transcript",
    "video_id": "example_video",
    "start": 12.0,
    "end": 14.2,
    "text": "Hello baby",
    "confidence": 0.92,
    "speaker_id": "speaker_1"
  }
]
```

**Core Fields:**
- `type`: Always "transcript"
- `start`/`end`: Time boundaries in seconds
- `text`: Transcribed speech
- `speaker_id`: Speaker identifier from diarization

### 4. Scene Detection

**File:** `{video_id}_scene_detection.json`

```json
[
  {
    "type": "scene_boundary",
    "video_id": "example_video",
    "t": 45.6,
    "scene_id": "scene_003",
    "boundary_type": "cut",
    "confidence": 0.95
  }
]
```

**Core Fields:**
- `type`: Always "scene_boundary" 
- `scene_id`: Unique scene identifier
- `boundary_type`: Type of transition (cut, fade, dissolve)
- `t`: Boundary timestamp
## Annotation Tool Compatibility

VideoAnnotator outputs are designed for seamless integration with popular annotation tools:

### CVAT Integration ✅

```python
# Export to CVAT format
from videoannotator.exporters import to_cvat_format

cvat_annotations = to_cvat_format(annotations)
# Upload directly to CVAT project
```

### LabelStudio Integration ✅

```python
# Export to LabelStudio format  
from videoannotator.exporters import to_labelstudio_format

labelstudio_tasks = to_labelstudio_format(annotations)
# Import into LabelStudio via JSON
```

### ELAN Integration ✅

```python
# Export to ELAN format for linguistic analysis
from videoannotator.exporters import to_elan_format

elan_file = to_elan_format(speech_annotations)
# Import .eaf file into ELAN
```

## Working with Outputs

### Loading Annotations

```python
import json

# Load any pipeline output
with open('video123_person_tracking.json', 'r') as f:
    annotations = json.load(f)

# Process annotations
for annotation in annotations:
    if annotation['type'] == 'person_bbox':
        person_id = annotation['person_id']
        bbox = annotation['bbox']
        timestamp = annotation['t']
        # Process detection...
```

### Filtering and Querying

```python
# Get all detections for a specific person
person_1_detections = [
    a for a in annotations 
    if a.get('person_id') == 1
]

# Get detections in time range
time_range_detections = [
    a for a in annotations
    if 10.0 <= a['t'] <= 20.0
]
```

### Combining Pipeline Outputs

```python
# Merge multiple pipeline outputs
face_data = load_json('video_face_analysis.json')
person_data = load_json('video_person_tracking.json') 
speech_data = load_json('video_speech_recognition.json')

# Link face detections to person tracks
for face in face_data:
    person_id = face['person_id']
    # Find corresponding person track...
```
