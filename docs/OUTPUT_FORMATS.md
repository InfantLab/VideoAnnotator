# VideoAnnotator Output Formats

This document explains the standardized JSON data formats generated by VideoAnnotator pipelines. All outputs include comprehensive metadata for versioning, reproducibility, and audit trails.

## Overview

VideoAnnotator uses a modern JSON-based output system that replaces legacy CSV/DataFrame approaches. All pipelines generate structured JSON files with:

- **Comprehensive metadata** (version, git commit, system info, model details)
- **Standardized schemas** using Pydantic v2 for validation
- **Timestamp precision** for accurate temporal alignment
- **Extensible formats** compatible with annotation tools (CVAT, LabelStudio)

## Common Metadata Structure

All output files include this metadata header:

```json
{
  "metadata": {
    "videoannotator": {
      "version": "1.0.0",
      "release_date": "2025-07-10",
      "build_date": "2025-07-10T21:04:10.170718",
      "git": {
        "commit_hash": "359d693e74d3de4427d3844db4f0a098546fad64",
        "branch": "master",
        "is_clean": false
      }
    },
    "system": {
      "platform": "Windows-11-10.0.26200-SP0",
      "python_version": "3.12.11",
      "architecture": "64bit"
    },
    "dependencies": {
      "opencv-python": "4.12.0",
      "ultralytics": "8.3.163",
      "pydantic": "2.11.7",
      "torch": "2.7.1+cpu"
    },
    "pipeline": {
      "name": "SceneDetectionPipeline",
      "processing_timestamp": "2025-07-10T21:04:16.638292",
      "processing_params": {
        "threshold": 30.0,
        "min_scene_length": 1.0
      }
    },
    "model": {
      "model_name": "PySceneDetect + CLIP",
      "model_path": null,
      "loaded_at": "2025-07-10T21:04:16.451969"
    }
  }
}
```

## Pipeline Output Formats

### 1. Scene Detection

**File:** `{video_id}_scenes.json`

```json
{
  "metadata": { /* Common metadata */ },
  "annotations": [
    {
      "video_id": "example_video",
      "timestamp": 0.0,
      "confidence": 1.0,
      "metadata": {},
      "created_at": "2025-07-10T21:04:16.485292",
      "scene_id": "scene_001",
      "start_time": 0.0,
      "end_time": 10.0,
      "change_type": "cut",
      "scene_type": "living_room",
      "transition_confidence": 0.95
    }
  ]
}
```

**Schema Fields:**
- `scene_id`: Unique identifier for the scene
- `start_time`/`end_time`: Scene boundaries in seconds
- `change_type`: Type of scene transition (cut, fade, dissolve)
- `scene_type`: Classified scene environment (optional)
- `transition_confidence`: Confidence of scene boundary detection

### 2. Person Tracking

**File:** `{video_id}_person_detections.json`

```json
{
  "metadata": { /* Common metadata */ },
  "pipeline": "person_tracking",
  "timestamp": "2025-07-10T21:04:17.643669",
  "detections": [
    {
      "video_id": "example_video",
      "timestamp": 0.0,
      "confidence": 1.0,
      "metadata": {
        "frame": 0,
        "model": "yolo11n-pose.pt"
      },
      "created_at": "2025-07-10T21:04:17.801318",
      "person_id": 1,
      "bbox": {
        "x": 0.233,
        "y": 0.495,
        "width": 0.376,
        "height": 0.505
      },
      "tracking_confidence": 1.0,
      "detection_confidence": 0.537,
      "age_estimate": null,
      "gender_estimate": null,
      "clothing_colors": null
    }
  ]
}
```

**Schema Fields:**
- `person_id`: Unique tracking identifier across frames
- `bbox`: Normalized bounding box (0-1 coordinates)
- `tracking_confidence`: Confidence in track assignment
- `detection_confidence`: Confidence in person detection
- `age_estimate`/`gender_estimate`: Optional demographic estimates

### 3. Person Trajectories

**File:** `{video_id}_person_tracks.json`

```json
{
  "metadata": { /* Common metadata */ },
  "trajectories": [
    {
      "video_id": "example_video",
      "timestamp": 0.0,
      "confidence": 1.0,
      "metadata": {},
      "created_at": "2025-07-10T21:04:17.734532",
      "person_id": 1,
      "detections": [
        { /* PersonDetection objects */ }
      ],
      "first_seen": 0.0,
      "last_seen": 4.9,
      "track_status": "active"
    }
  ]
}
```

**Schema Fields:**
- `detections`: Array of PersonDetection objects forming the trajectory
- `first_seen`/`last_seen`: Temporal bounds of the track
- `track_status`: Status of tracking (active, lost, completed)
- `duration`: Computed property (last_seen - first_seen)

### 4. Face Analysis

**File:** `{video_id}_faces.json`

```json
{
  "metadata": { /* Common metadata */ },
  "detections": [
    {
      "video_id": "example_video",
      "timestamp": 1.5,
      "confidence": 0.92,
      "face_id": "face_001",
      "bbox": {
        "x": 0.45,
        "y": 0.23,
        "width": 0.12,
        "height": 0.18
      },
      "emotion": {
        "dominant": "happy",
        "scores": {
          "happy": 0.78,
          "neutral": 0.15,
          "surprise": 0.07
        }
      },
      "demographics": {
        "age": 25,
        "gender": "female",
        "confidence": 0.85
      },
      "landmarks": {
        "left_eye": {"x": 0.47, "y": 0.28},
        "right_eye": {"x": 0.52, "y": 0.28},
        "nose": {"x": 0.495, "y": 0.32},
        "mouth": {"x": 0.495, "y": 0.36}
      }
    }
  ]
}
```

### 5. Audio/Speech Processing

**File:** `{video_id}_speech.json`

```json
{
  "metadata": { /* Common metadata */ },
  "transcription": {
    "text": "Hello, how are you today?",
    "language": "en",
    "segments": [
      {
        "id": 0,
        "start": 1.2,
        "end": 3.8,
        "text": "Hello, how are you today?",
        "confidence": 0.94,
        "speaker_id": "speaker_1",
        "tokens": [
          {"text": "Hello", "start": 1.2, "end": 1.6},
          {"text": "how", "start": 1.8, "end": 2.0},
          {"text": "are", "start": 2.0, "end": 2.2},
          {"text": "you", "start": 2.2, "end": 2.5},
          {"text": "today", "start": 2.6, "end": 3.2}
        ]
      }
    ]
  },
  "speaker_diarization": [
    {
      "speaker_id": "speaker_1", 
      "start": 1.2,
      "end": 3.8,
      "confidence": 0.89
    }
  ]
}
```

## Directory Structure

Output files are organized by video and pipeline:

```
output/
├── {video_id}/
│   ├── {video_id}_scenes.json          # Scene detection
│   ├── {video_id}_person_detections.json # Person tracking
│   ├── {video_id}_person_tracks.json   # Person trajectories
│   ├── {video_id}_faces.json           # Face analysis
│   ├── {video_id}_speech.json          # Audio processing
│   └── {video_id}_summary.json         # Combined summary
└── processing_logs/
    ├── {video_id}_processing.log        # Processing logs
    └── {video_id}_errors.json           # Error reports
```

## Data Validation

All outputs use Pydantic v2 schemas for validation:

- **Type safety** with automatic validation
- **Serialization consistency** with `model_dump(mode='json')`
- **DateTime handling** with ISO format strings
- **Schema evolution** support for backwards compatibility

## Integration with Annotation Tools

The JSON formats are designed for compatibility with:

- **CVAT** - Import/export support for video annotations
- **LabelStudio** - Direct JSON ingestion
- **Custom tools** - Standardized schema for easy parsing
- **ML pipelines** - Direct loading into training frameworks

## Versioning and Reproducibility

Every output file includes:

- **Software version** and build information
- **Git commit hash** for exact code version
- **Model information** and parameters used
- **Processing timestamp** and duration
- **System environment** details

This ensures full reproducibility and audit capability for research and production use.

## Performance Considerations

- **Streaming output** for large videos
- **Chunked processing** to manage memory usage
- **Parallel processing** support across pipelines
- **Incremental updates** for long-running processes
- **Compression** options for large datasets

## Future Extensions

The schema design supports:

- **Custom pipeline integration**
- **Multi-modal fusion** (combining outputs)
- **Real-time streaming** annotations
- **Cloud storage** backends
- **Distributed processing** workflows
