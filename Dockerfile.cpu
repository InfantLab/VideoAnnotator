# VideoAnnotator Production Docker Image - CPU Version
# This image does NOT include models/weights - they download automatically on first use
#
# Usage:
#   docker build -f Dockerfile.cpu -t videoannotator:cpu .
#   docker run --rm -p 8000:8000 -v ${PWD}/data:/app/data videoannotator:cpu

FROM python:3.13-slim

# Use bash with pipefail so RUN commands that use a pipe fail when any stage does
SHELL ["/bin/bash","-o","pipefail","-lc"]

# Install base packages and locales in a single RUN to reduce layers and avoid
# pulling recommended packages unnecessarily (DL3015, DL3059)
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl python3 python3-venv python3-pip git \
    libgl1-mesa-dri libglib2.0-0 libsm6 libxext6 libxrender1 libgomp1 locales \
    && locale-gen en_US.UTF-8 \
    && update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 \
    && rm -rf /var/lib/apt/lists/*

# Export UTF-8 locale for all processes
ENV LANG=en_US.UTF-8
ENV LC_ALL=en_US.UTF-8

# Install uv, copy sources, and install dependencies and CPU PyTorch in grouped RUNs
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.local/bin:${PATH}"

WORKDIR /app

# Copy source code (excluding models via .dockerignore)
COPY . .

# Install dependencies (excluding torch to avoid conflicts) and install CPU PyTorch
RUN uv sync --frozen --no-editable \
    && uv pip install "torch==2.8.0+cpu" "torchvision==0.21.0+cpu" "torchaudio==2.8.0+cpu" --index-url https://download.pytorch.org/whl/cpu

# Verify CPU setup (no GPU packages installed)
RUN uv run python3 -c "\
import torch; \
print(f'[CPU BUILD] CUDA available: {torch.cuda.is_available()}'); \
print(f'[CPU BUILD] PyTorch version: {torch.__version__}'); \
print('[CPU BUILD] Production image ready - models will download on first use');"

# Set environment for production
ENV PYTHONUNBUFFERED=1

# Create directories for mounted volumes
RUN mkdir -p /app/data /app/output /app/logs

EXPOSE 8000

CMD ["uv", "run", "python3", "api_server.py", "--log-level", "info"]
