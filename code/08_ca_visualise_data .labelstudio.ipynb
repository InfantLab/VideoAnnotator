{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Reviewing videoannotations in Label Studio\n",
    "\n",
    "[Label Studio](https://labelstud.io/) is a tool for creating training data for machine learning. It is browser based and can be used to annotate video, images and other data. \n",
    "\n",
    "We will use it to review the annotations we made in the previous steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ultralytics\n",
    "import fiftyone as fo\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path and import utils\n",
    "project_root = os.path.join(\"..\")\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src.config import PATH_CONFIG\n",
    "from src.utils.io_utils import getProcessedVideos, saveProcessedVideos, getFaceData, getSpeechData, getKeyPoints, getVideoProperty\n",
    "from src.utils.notebook_utils import display_config_info, ensure_dir_exists\n",
    "from src.utils.keypoint_utils import normalize_keypoints\n",
    "from src.processors.keypoint_processor import process_keypoints_for_modeling\n",
    "from src.processors.face_processor import normalize_facial_keypoints, match_faces_to_poses\n",
    "\n",
    "# Get paths from config\n",
    "videos_in = PATH_CONFIG['videos_in']\n",
    "data_out = PATH_CONFIG['data_out']\n",
    "\n",
    "# Ensure output directory exists\n",
    "if ensure_dir_exists(data_out):\n",
    "    print(f\"Created output directory: {data_out}\")\n",
    "\n",
    "# Display configuration information\n",
    "display_config_info(videos_in, data_out, \"Processing Configuration\")\n",
    "\n",
    "# Use the configured filename from PATH_CONFIG\n",
    "processedvideos = getProcessedVideos(data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Launch Label Studio\n",
    "\n",
    "Let's build the command to launch Label Studio inside a docker container.\n",
    "Getting the folder paths correct is a bit tricky. There is some guidance here - [Label Studio Documentation â€” Start commands for Label Studio](https://labelstud.io/guide/start#Run_Label_Studio_on_Docker_and_use_local_storage)\n",
    "\n",
    "THis is how i set it up.\n",
    "\n",
    "$PWD = C:/Users/caspar/OneDrive\n",
    "\n",
    "OneDrive/ls/data/\n",
    "\n",
    "OneDrive/ls/files/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's build a Power Shell command to launch Label Studio inside a docker container.\n",
    "command = [\n",
    "    'docker run -it',\n",
    "    '-p', \"8080:8080\", #port it runs on\n",
    "    '-e', \"LABEL_STUDIO_LOCAL_FILES_SERVING_ENABLED=true\",  #necessary environment variables\n",
    "    '-e', \"LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT=/label-studio\",\n",
    "    '-v', \"$PWD/ls/data:/label-studio/data\", #data volume location and mount point\n",
    "    '-v', \"$PWD/ls/files:/label-studio/files\", #files volume location and mount point\n",
    "    'heartexlabs/label-studio:latest', 'label-studio' #docker image to use\n",
    "]\n",
    "# Use shell=True for Windows and pass the command as a string\n",
    "command_str = ' '.join(command)\n",
    "result = subprocess.run(command_str, check=True, shell=True, capture_output=True, text=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL where Label Studio is accessible and the API key for your user account\n",
    "LABEL_STUDIO_URL = 'http://localhost:8080'\n",
    "API_KEY = 'd6f8a2622d39e9d89ff0dfef1a80ad877f4ee9e3'\n",
    "\n",
    "# Import the SDK and the client module\n",
    "from label_studio_sdk.client import LabelStudio\n",
    "\n",
    "# Connect to the Label Studio API and check the connection\n",
    "ls = LabelStudio(base_url=LABEL_STUDIO_URL, api_key=API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 Is dataset already created?\n",
    "\n",
    "FiftyOne may aleady have a dataset created. Let's check. And reload it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = fo.list_datasets()\n",
    "if len(datasets) == 0:\n",
    "    print(\"No datasets found. Load in step 7.1.2\")\n",
    "else:\n",
    "    print(\"Loading saved datasets: \", datasets[0])\n",
    "    dataset = fo.load_dataset(datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.delete_datasets(\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can add our metadata classifications. Recalling that each video demos one joke type `[Peekaboo,TearingPaper,NomNomNom,ThatsNotAHat,ThatsNotACat]` and has rating of how funny the baby found it `[Not Funny, Slightly Funny, Funny, Extremely Funny]` and whether they laughed `[Yes, No]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the frame by frame annotations directly onto the videos inside fiftyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the speech as temporal annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def framerange_from_timestamps(timestamps, fps, max_frames):\n",
    "    start = max(int(timestamps[0]*fps)+1 ,1)\n",
    "    end =  min(int(timestamps[1]*fps)+1, max_frames )\n",
    "    return start, end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataset:\n",
    "    videoname = os.path.basename(sample.filepath)\n",
    "    fps = sample.metadata[\"frame_rate\"]\n",
    "    max_frames = sample.metadata[\"total_frame_count\"]\n",
    "    print(fps)\n",
    "    speechdata = utils.getSpeechData(processedvideos,videoname)\n",
    "    if speechdata is None:\n",
    "        print(f\"Speech data not found for {videoname}\")\n",
    "        continue\n",
    "    phrases = []\n",
    "    for phrase in speechdata[\"segments\"]:\n",
    "        start, end = framerange_from_timestamps([phrase[\"start\"],phrase[\"end\"]], fps, max_frames)\n",
    "        print (start, end)\n",
    "        phrases.append(fo.TemporalDetection(label=phrase[\"text\"],\n",
    "                                        support=[start,end]))\n",
    "        print(phrase[\"text\"])\n",
    "        \n",
    "    sample[\"Speech\"] = fo.TemporalDetections(detections=phrases)\n",
    "    sample[\"Speech\"] = phrases\n",
    "    sample.save()\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset.first()\n",
    "\n",
    "\n",
    "dets =[\n",
    "        fo.TemporalDetection(label=\"meeting\", support=[10, 20]),\n",
    "        fo.TemporalDetection(label=\"party\", support=[30, 60]),\n",
    "    ]\n",
    "\n",
    "sample[\"events\"] = fo.TemporalDetections(\n",
    "    detections= dets\n",
    ")\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataset:\n",
    "    videoname = os.path.basename(sample.filepath)\n",
    "    speechdata = utils.getSpeechData(processedvideos,videoname)\n",
    "    if speechdata is None:\n",
    "        print(f\"Speech data not found for {videoname}\")\n",
    "        continue\n",
    "    \n",
    "    subtitles = speechdata[\"segments\"]\n",
    "    # Create a list of text annotations\n",
    "    text_annotations = [\n",
    "        fo.Detection(\n",
    "            text=sub[\"text\"],\n",
    "            start_time=sub[\"start\"],\n",
    "            end_time =sub[\"end\"]\n",
    "        )\n",
    "        for sub in subtitles\n",
    "    ]\n",
    "    sample[\"subtitles\"] = fo.Detections(detections=text_annotations)    \n",
    "    sample.save()\n",
    "\n",
    "14.299\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session.selected contains the indices of the dataset samples clicked on in the UI.\n",
    "if len(session.selected) == 0:\n",
    "    print(\"No samples selected. Click the checkbox in the top left of each video to select it.\")\n",
    "else:\n",
    "    print(dataset[session.selected[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Draw annotated timeline for a select video \n",
    "\n",
    "A group of visualisations to see what happens in a video. \n",
    "\n",
    "In each frame let's find the `centre of gravity` for each person (the average of all the high-confidence marker points). This is handy for time series visualisation. For example plotting the cog.x for each person over time shows how they move closer and further from each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the keypoint data and calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotionColors = {\"angry\":{\"color\":\"red\",\"arousal\":0.9,\"valence\":-0.2},\n",
    "                 \"fear\":{\"color\":\"orange\",\"arousal\":0.2,\"valence\":-0.9},\n",
    "                 \"happy\":{\"color\":\"yellow\",\"arousal\":0.2,\"valence\":0.9},\n",
    "                 \"neutral\":{\"color\":\"grey\",\"arousal\":0,\"valence\":0},\n",
    "                 \"sad\":{\"color\":\"blue\",\"arousal\":-0.2,\"valence\":-0.9},\n",
    "                 \"surprise\":{\"color\":\"green\",\"arousal\":0.9,\"valence\":0.2},\n",
    "                 \"disgust\":{\"color\":\"purple\",\"arousal\":-0.7,\"valence\":-0.7}}\n",
    "who = [\"child\", \"adult\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCoGrav = True\n",
    "plotStDev = True\n",
    "plotSpeech = True\n",
    "plotEmotions = True\n",
    "\n",
    "#numerical sum of boolean flags\n",
    "subplots = sum([plotCoGrav, plotStDev, plotSpeech, plotEmotions])\n",
    "\n",
    "if len(session.selected) == 0:\n",
    "    print(\"No video selected\")\n",
    "    exit()\n",
    "\n",
    "VideoID = dataset[session.selected[0]][\"VideoID\"]\n",
    "keypoints = utils.readKeyPointsFromCSV(processedvideos,VideoID)\n",
    "FPS = utils.getVideoProperty(processedvideos, VideoID, \"FPS\")\n",
    "xmax = keypoints[\"frame\"].max()\n",
    "#this bit of pandas magic calculates average x and y for all the rows.\n",
    "keypoints[[\"cogx\",\"cogy\"]] = keypoints.apply(lambda row: calcs.rowcogs(row.iloc[8:59]), axis=1, result_type='expand')\n",
    "keypoints[[\"stdx\",\"stdy\"]] = keypoints.apply(lambda row: calcs.rowstds(row.iloc[8:59]), axis=1, result_type='expand')\n",
    "\n",
    "#going to add a subplot foe each of the above flags\n",
    "plt.figure(figsize=(20, 5*subplots))\n",
    "plt.suptitle(\"Video Time Line Plots\")\n",
    "pltidx = 0\n",
    "if plotCoGrav:\n",
    "    ax = plt.subplot(subplots, 1, pltidx + 1)\n",
    "    pltidx += 1\n",
    "    ax.set_xlabel(\"Time (seconds)\")\n",
    "    ax.set_ylabel(\"Horizontal Position\")\n",
    "    ax.set_xlim(0, xmax/FPS)\n",
    "    child = keypoints[keypoints[\"person\"]==\"child\"]\n",
    "    adult = keypoints[keypoints[\"person\"]==\"adult\"]\n",
    "    #a plot of child's centre of gravity frame by frame\n",
    "    childplot = ax.plot(child[\"frame\"], child[\"cogx\"], c=\"red\", alpha=0.5)\n",
    "    ## add line of adult's centre of gravity\n",
    "    adultplot = ax.plot(adult[\"frame\"], adult[\"cogx\"], c=\"blue\", alpha=0.5)\n",
    "    #add legend\n",
    "    ax.legend(['child', 'adult'], loc='upper left')\n",
    "\n",
    "if plotStDev:\n",
    "    ax = plt.subplot(subplots, 1, pltidx + 1)\n",
    "    pltidx += 1\n",
    "    ax.set_xlabel(\"Time (seconds)\")\n",
    "    ax.set_ylabel(\"Horizontal Position\")\n",
    "    ax.set_xlim(0, xmax/FPS)\n",
    "    child = keypoints[keypoints[\"person\"]==\"child\"]\n",
    "    adult = keypoints[keypoints[\"person\"]==\"adult\"]\n",
    "    #a plot of child's centre of gravity frame by frame\n",
    "    childplot = ax.plot(child[\"frame\"], child[\"stdx\"], c=\"red\", alpha=0.5)\n",
    "    ## add line of adult's centre of gravity\n",
    "    adultplot = ax.plot(adult[\"frame\"], adult[\"stdx\"], c=\"blue\", alpha=0.5)\n",
    "    #add legend\n",
    "    ax.legend(['child', 'adult'], loc='upper left')\n",
    "\n",
    "if plotSpeech:\n",
    "    ax2 = plt.subplot(subplots, 1, pltidx + 1)\n",
    "    pltidx += 1\n",
    "    ax2.set_xlabel(\"Time (seconds)\")\n",
    "    ax2.set_ylabel(\"Identified Speech\")\n",
    "    speechjson = utils.getSpeechData(processedvideos,VideoID)\n",
    "    if speechjson is not None:\n",
    "        nsegs = len(speechjson[\"segments\"])\n",
    "        ax2.set_xlim(0, xmax/FPS)\n",
    "        ax2.set_ylim(0, nsegs)\n",
    "        #let's plot the speech segments as boxes\n",
    "        #label each one with the text\n",
    "        for idx, seg in enumerate(speechjson[\"segments\"]):\n",
    "            # #rectangle with the start and end times as x coordinates and nsegs - idx as y coordinates\n",
    "            #fill the rectangle\n",
    "            ax2.fill([seg[\"start\"], seg[\"end\"], seg[\"end\"], seg[\"start\"]], [nsegs - idx - 1, nsegs - idx - 1, nsegs - idx, nsegs - idx], 'r', alpha=0.5)\n",
    "            ax2.text(seg[\"start\"], nsegs- idx -.5 , seg[\"text\"])\n",
    "\n",
    "if plotEmotions:\n",
    "    ax3 = plt.subplot(subplots, 1, pltidx + 1)\n",
    "    pltidx += 1\n",
    "    ax3.set_xlabel(\"Time (seconds)\")\n",
    "    ax3.set_ylim(0, 2)\n",
    "    ax3.set_xlim(0, xmax/FPS)  \n",
    "    emotions = utils.getFaceData(processedvideos,VideoID)\n",
    "    emotions[\"ticker\"] = 1\n",
    "    for index in range(2):\n",
    "        ems = emotions[emotions[\"index\"]==index]\n",
    "        #who is the person we are plotting\n",
    "        # key gives the emotion name, data gives the actual values (also labels)\n",
    "        for key, data in ems.groupby('emotion'):\n",
    "            #plot scatter plot of emotion occurances\n",
    "            ax3.scatter(data[\"frame\"], data[\"ticker\"] + index, label=key, c=emotionColors[key][\"color\"], alpha=0.5, s=100)\n",
    "\n",
    "        \n",
    "    #show legend with emotion colours\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the captions.\n",
    "Go through the speechjson. For each speech segment add a horizotal line with the text. Start and End times from the speechjson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a timeline for the emotions of the participants.\n",
    "We'll experiment to find best visualisation. \n",
    "Note this assumes that faces are correctly assigned to correct indviduals. \n",
    "TODO - Code that uses bounding boxes to assign faces to individuals.\n",
    "\n",
    "First we will try a 'scatter' graph. Color coded for each emotion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def scan_folder(source_folder):\n",
    "    data_list = []\n",
    "    id_counter = 1\n",
    "\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp4'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                audio_path = f\"/data/local-files/?d={file_path}\"\n",
    "                video_path = f\"/data/local-files/?d={file_path}\"\n",
    "\n",
    "                entry = {\n",
    "                    \"id\": id_counter,\n",
    "                    \"data\": {\n",
    "                        \"audio\": audio_path,\n",
    "                        \"video\": video_path\n",
    "                    }\n",
    "                }\n",
    "                data_list.append(entry)\n",
    "                id_counter += 1\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filejson = scan_folder(r\"C:\\Users\\caspar\\OneDrive\\data\\LookitLaughter.videos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyjokes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
