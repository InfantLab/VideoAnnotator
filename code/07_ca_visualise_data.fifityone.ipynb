{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT WORKING AT THE MOMENT (APRIL 2025)\n",
    "\n",
    "_ANNOTATIONS NOT APPEARING ON VIDOES IN THE FIFTYONE GUI_\n",
    "_AUDIO NOT PLAYING IN THE FIFTYONE GUI_\n",
    "\n",
    "# 7 Visualise activity in a video.\n",
    "\n",
    "\n",
    "\n",
    "We have extracted all the features we plan to use. Overlaying these on the video was useful.\n",
    "But watching annotated videos is inefficient and not always informative.. \n",
    "\n",
    "To help with understanding we build a few tools that let's see at a glance what happens over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ultralytics\n",
    "import fiftyone as fo\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path and import utils\n",
    "project_root = os.path.join(\"..\")\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src.config import PATH_CONFIG\n",
    "from src.utils.io_utils import getProcessedVideos, saveProcessedVideos, getFaceData, getSpeechData, getKeyPoints, getVideoProperty\n",
    "from src.utils.notebook_utils import display_config_info, ensure_dir_exists\n",
    "from src.utils.keypoint_utils import normalize_keypoints\n",
    "from src.processors.keypoint_processor import process_keypoints_for_modeling\n",
    "from src.processors.face_processor import normalize_facial_keypoints, match_faces_to_poses\n",
    "\n",
    "# Get paths from config\n",
    "videos_in = PATH_CONFIG['videos_in']\n",
    "data_out = PATH_CONFIG['data_out']\n",
    "\n",
    "# Ensure output directory exists\n",
    "if ensure_dir_exists(data_out):\n",
    "    print(f\"Created output directory: {data_out}\")\n",
    "\n",
    "# Display configuration information\n",
    "display_config_info(videos_in, data_out, \"Processing Configuration\")\n",
    "\n",
    "# Use the configured filename from PATH_CONFIG\n",
    "processedvideos = getProcessedVideos(data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Use FiftyOne for examining videos\n",
    "\n",
    "FiftyOne is a powerful tool for visualizing, exploring, and analyzing media datasets. We'll use it to view our videos with all the extracted features overlaid.\n",
    "\n",
    "Reference: https://docs.voxel51.com/user_guide/dataset_creation/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.1 Define helper functions\n",
    "\n",
    "These functions will help us manage our FiftyOne dataset and add annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2person(idx):\n",
    "    \"\"\"Convert index to person label\"\"\"\n",
    "    idx = int(idx) if not isinstance(idx, str) else idx\n",
    "    if idx == 0 or idx == \"0\" or idx == \"child\":\n",
    "        return \"Child\"\n",
    "    elif idx == 1 or idx == \"1\" or idx == \"adult\":\n",
    "        return \"Adult\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def framerange_from_timestamps(timestamps, fps, max_frames):\n",
    "    \"\"\"Convert time stamps to frame numbers\"\"\"\n",
    "    start = max(int(timestamps[0]*fps)+1, 1)\n",
    "    end = min(int(timestamps[1]*fps)+1, max_frames)\n",
    "    return start, end\n",
    "\n",
    "def xyxy2ltwh(bbox):\n",
    "    \"\"\"Convert bounding box from [x1, y1, x2, y2] to [left, top, width, height]\"\"\"\n",
    "    return [\n",
    "        bbox[0],              # left\n",
    "        bbox[1],              # top\n",
    "        bbox[2] - bbox[0],    # width\n",
    "        bbox[3] - bbox[1]     # height\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(videos_dir, dataset_name=\"BabyJokes\"):\n",
    "    \"\"\"Create a new FiftyOne dataset or load existing one\"\"\"\n",
    "    logging.info(f\"Creating/Loading FiftyOne dataset: {dataset_name}\")\n",
    "    \n",
    "    # Check if dataset exists\n",
    "    if dataset_name in fo.list_datasets():\n",
    "        logging.info(f\"Loading existing dataset: {dataset_name}\")\n",
    "        dataset = fo.load_dataset(dataset_name)\n",
    "    else:\n",
    "        # Create new dataset\n",
    "        logging.info(f\"Creating new dataset: {dataset_name}\")\n",
    "        dataset = fo.Dataset(dataset_name)\n",
    "    \n",
    "        # Add videos\n",
    "        logging.info(f\"Adding videos from {videos_dir}\")\n",
    "        video_paths = [os.path.join(videos_dir, f) for f in os.listdir(videos_dir) if f.endswith((\".mp4\", \".avi\", \".mov\"))]\n",
    "    \n",
    "        for video_path in video_paths:\n",
    "            try:\n",
    "                sample = fo.Sample(filepath=video_path)\n",
    "                dataset.add_sample(sample)\n",
    "                logging.info(f\"Added video {video_path} to dataset\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error adding video {video_path}: {str(e)}\")\n",
    "            \n",
    "        # Ensure frames are extracted and metadata is computed\n",
    "        logging.info(\"Extracting frames and computing metadata...\")\n",
    "        dataset.compute_metadata()\n",
    "        dataset.ensure_frames()\n",
    "    \n",
    "        # Add sample fields for metadata\n",
    "        dataset.add_sample_field(\"VideoID\", fo.StringField, description=\"Video identifier\")\n",
    "        dataset.add_sample_field(\"JokeType\", fo.StringField, description=\"What joke is being told?\")\n",
    "        dataset.add_sample_field(\"HowFunny\", fo.StringField, description=\"How funny is the joke?\")\n",
    "        dataset.add_sample_field(\"LaughYesNo\", fo.BooleanField, description=\"Did the child laugh?\")\n",
    "        dataset.add_sample_field(\"ChildSide\", fo.IntField, description=\"Is the child on left (-1) or right (1) of adult or on lap (0)?\")\n",
    "    \n",
    "        # Add frame field for people detections using proper field type\n",
    "        dataset.add_frame_field(\n",
    "            \"People\", \n",
    "            fo.EmbeddedDocumentField,\n",
    "            embedded_doc_type=fo.Detections,\n",
    "            description=\"People detections\"\n",
    "        )\n",
    "    \n",
    "        # Add frame field for pose keypoints\n",
    "        dataset.add_frame_field(\n",
    "            \"Poses\",\n",
    "            fo.EmbeddedDocumentField,\n",
    "            embedded_doc_type=fo.Keypoints,\n",
    "            description=\"Human pose keypoints\"\n",
    "        )\n",
    "    \n",
    "        # Add frame field for dominant emotion\n",
    "        dataset.add_frame_field(\n",
    "            \"DominantEmotion\",\n",
    "            fo.StringField,\n",
    "            description=\"Dominant facial emotion\"\n",
    "        )\n",
    "    \n",
    "        # Add frame field for speech captions\n",
    "        dataset.add_frame_field(\n",
    "            \"Captions\",\n",
    "            fo.StringField,\n",
    "            description=\"Speech captions\"\n",
    "        )\n",
    "    \n",
    "        logging.info(f\"Created dataset with {len(dataset)} videos\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metadata_to_samples(dataset, processedvideos):\n",
    "    \"\"\"Add video metadata (joke type, how funny, etc.) to samples\"\"\"\n",
    "    logging.info(\"Adding metadata to samples...\")\n",
    "    \n",
    "    for sample in dataset:\n",
    "        try:\n",
    "            videoname = os.path.basename(sample.filepath)\n",
    "            phrase = processedvideos[processedvideos[\"VideoID\"]==videoname]\n",
    "            if len(phrase) == 0:\n",
    "                logging.warning(f\"Video {videoname} not found in processed videos.\")\n",
    "                continue\n",
    "            sample[\"VideoID\"] = phrase[\"VideoID\"].values[0]\n",
    "            sample[\"JokeType\"] = phrase[\"JokeType\"].values[0]\n",
    "            sample[\"HowFunny\"] = phrase[\"HowFunny\"].values[0]\n",
    "            sample[\"LaughYesNo\"] = (phrase[\"LaughYesNo\"].values[0] == \"Yes\")\n",
    "            sample.save()\n",
    "            logging.info(f\"Added metadata to {videoname}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error adding metadata to {sample.filepath}: {str(e)}\")\n",
    "            import traceback\n",
    "            logging.error(traceback.format_exc())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_people_bounding_boxes(dataset, processedvideos):\n",
    "    \"\"\"Add people bounding boxes and pose keypoints as frame-level detections\"\"\"\n",
    "    logging.info(\"Adding people bounding boxes and poses...\")\n",
    "    \n",
    "    for sample in dataset:\n",
    "        try:\n",
    "            videoname = os.path.basename(sample.filepath)\n",
    "            keypoints = getKeyPoints(processedvideos, videoname)\n",
    "            if keypoints is None or keypoints.empty:\n",
    "                logging.warning(f\"No keypoints found for {videoname}\")\n",
    "                continue\n",
    "            \n",
    "            for frame_number, frame in sample.frames.items():\n",
    "                rows = keypoints[keypoints[\"frame\"] == frame_number - 1]\n",
    "                if rows.empty:\n",
    "                    continue\n",
    "                \n",
    "                detections = []\n",
    "                pose_keypoints = []\n",
    "                for _, row in rows.iterrows():\n",
    "                    if 'bbox.x1' not in row or pd.isna(row['bbox.x1']):\n",
    "                        continue\n",
    "                    bbox = [row[\"bbox.x1\"], row[\"bbox.y1\"], row[\"bbox.x2\"], row[\"bbox.y2\"]]\n",
    "                    bbox51 = xyxy2ltwh(bbox)\n",
    "                    detection = fo.Detection(\n",
    "                        label=idx2person(row[\"person\"]),\n",
    "                        bounding_box=bbox51,\n",
    "                        confidence=row.get(\"confidence\", 0.9)\n",
    "                    )\n",
    "                    detections.append(detection)\n",
    "                    \n",
    "                    # Add keypoints if available\n",
    "                    keypoint_columns = [col for col in row.index if col.startswith(\"x_\") or col.startswith(\"y_\")]\n",
    "                    if keypoint_columns:\n",
    "                        points = []\n",
    "                        num_keypoints = len(keypoint_columns) // 2\n",
    "                        for i in range(num_keypoints):\n",
    "                            x_col = f\"x_{i}\"\n",
    "                            y_col = f\"y_{i}\"\n",
    "                            if x_col in row and y_col in row and not pd.isna(row[x_col]) and not pd.isna(row[y_col]):\n",
    "                                points.append([row[x_col], row[y_col], 2])  # [x, y, visibility]\n",
    "                        if points:\n",
    "                            keypoints_obj = fo.Keypoint(label=idx2person(row[\"person\"]), points=points)\n",
    "                            pose_keypoints.append(keypoints_obj)\n",
    "                \n",
    "                if detections:\n",
    "                    frame[\"People\"] = fo.Detections(detections=detections)\n",
    "                if pose_keypoints:\n",
    "                    frame[\"Poses\"] = fo.Keypoints(keypoints=pose_keypoints)\n",
    "            sample.save()\n",
    "            logging.info(f\"Added people detections and poses to {videoname}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {sample.filepath}: {str(e)}\")\n",
    "            import traceback\n",
    "            logging.error(traceback.format_exc())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_speech_annotations(dataset, processedvideos):\n",
    "    \"\"\"Add speech transcripts as frame-level annotations\"\"\"\n",
    "    logging.info(\"Adding speech annotations...\")\n",
    "    \n",
    "    for sample in dataset:\n",
    "        try:\n",
    "            videoname = os.path.basename(sample.filepath)\n",
    "            speechdata = getSpeechData(processedvideos, videoname)\n",
    "            if speechdata is None or \"segments\" not in speechdata:\n",
    "                logging.warning(f\"No speech data found for {videoname}\")\n",
    "                continue\n",
    "            \n",
    "            if not hasattr(sample, 'metadata') or sample.metadata is None or 'frame_rate' not in sample.metadata or 'total_frame_count' not in sample.metadata:\n",
    "                logging.warning(f\"Missing metadata for {videoname}\")\n",
    "                continue\n",
    "            \n",
    "            fps = sample.metadata[\"frame_rate\"]\n",
    "            max_frames = sample.metadata[\"total_frame_count\"]\n",
    "            \n",
    "            for segment in speechdata[\"segments\"]:\n",
    "                start_frame, end_frame = framerange_from_timestamps(\n",
    "                    [segment[\"start\"], segment[\"end\"]], fps, max_frames\n",
    "                )\n",
    "                for frame_number in range(start_frame, end_frame + 1):\n",
    "                    if frame_number in sample.frames:\n",
    "                        sample.frames[frame_number][\"Captions\"] = segment[\"text\"]\n",
    "            sample.save()\n",
    "            logging.info(f\"Added speech annotations to {videoname}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error adding speech to {sample.filepath}: {str(e)}\")\n",
    "            import traceback\n",
    "            logging.error(traceback.format_exc())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dominant_emotion(dataset, processedvideos):\n",
    "    \"\"\"Add dominant facial emotion to each frame, handling multiple faces\"\"\"\n",
    "    logging.info(\"Adding dominant facial emotions...\")\n",
    "    \n",
    "    for sample in dataset:\n",
    "        try:\n",
    "            videoname = os.path.basename(sample.filepath)\n",
    "            emotions = getFaceData(processedvideos, videoname)\n",
    "            if emotions is None or emotions.empty:\n",
    "                logging.warning(f\"No emotion data found for {videoname}\")\n",
    "                continue\n",
    "            \n",
    "            for frame_number, frame in sample.frames.items():\n",
    "                frame_emotions = emotions[emotions['frame'] == frame_number - 1]\n",
    "                if frame_emotions.empty:\n",
    "                    continue\n",
    "                \n",
    "                detections = []\n",
    "                for face_id in frame_emotions['face_id'].unique():\n",
    "                    face_emotions = frame_emotions[frame_emotions['face_id'] == face_id]\n",
    "                    if face_emotions.empty:\n",
    "                        continue\n",
    "                    \n",
    "                    # Find the dominant emotion for the face\n",
    "                    dominant_emotion = face_emotions['dominant_emotion'].iloc[0]\n",
    "                    \n",
    "                    # Get bounding box\n",
    "                    x = face_emotions['x'].iloc[0]\n",
    "                    y = face_emotions['y'].iloc[0]\n",
    "                    w = face_emotions['w'].iloc[0]\n",
    "                    h = face_emotions['h'].iloc[0]\n",
    "                    bbox = [x, y, w, h]\n",
    "                    \n",
    "                    # Create detection\n",
    "                    detection = fo.Detection(\n",
    "                        label=f\"{idx2person(face_id)}: {dominant_emotion}\",\n",
    "                        bounding_box=xyxy2ltwh( [x, y, x+w, y+h] )  # Convert to FiftyOne format\n",
    "                    )\n",
    "                    detections.append(detection)\n",
    "                \n",
    "                if detections:\n",
    "                    frame[\"People\"] = fo.Detections(detections=detections)\n",
    "            sample.save()\n",
    "            logging.info(f\"Added dominant emotions to {videoname}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error adding dominant emotions to {sample.filepath}: {str(e)}\")\n",
    "            import traceback\n",
    "            logging.error(traceback.format_exc())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.2 Create or load FiftyOne dataset\n",
    "\n",
    "Let's check if we have an existing dataset. If not, we'll create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our dataset name\n",
    "DATASET_NAME = \"BabyJokes\"\n",
    "\n",
    "# Check for existing datasets\n",
    "datasets = fo.list_datasets()\n",
    "print(f\"Available datasets: {datasets}\")\n",
    "\n",
    "# Do we want to delete existing datasets?\n",
    "delete_existing = True  # Set to True to recreate the dataset from scratch\n",
    "if delete_existing and DATASET_NAME in datasets:\n",
    "    print(f\"Deleting existing dataset: {DATASET_NAME}\")\n",
    "    fo.delete_dataset(DATASET_NAME)\n",
    "    datasets = fo.list_datasets()\n",
    "\n",
    "# Create or load our dataset\n",
    "dataset = create_dataset(videos_in, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.3 Add metadata and annotations to the dataset\n",
    "\n",
    "Now let's populate our dataset with all the metadata and annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply functions to add metadata and annotations\n",
    "dataset = add_metadata_to_samples(dataset, processedvideos)\n",
    "dataset = add_people_bounding_boxes(dataset, processedvideos)\n",
    "dataset = add_speech_annotations(dataset, processedvideos)\n",
    "dataset = add_dominant_emotion(dataset, processedvideos)\n",
    "\n",
    "# Print dataset information after applying functions\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.4 View dataset in FiftyOne App\n",
    "\n",
    "Now we can visualize our dataset with all annotations in the FiftyOne App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view that filters samples with temporal detections\n",
    "from fiftyone import F\n",
    "\n",
    "# Filter samples that have temporal detections\n",
    "temp_view = dataset.match(F.field(\"temporal_detections\").exists())\n",
    "print(f\"Found {len(temp_view)} samples with speech temporal detections\")\n",
    "\n",
    "# Filter samples that have emotion detections\n",
    "emotion_view = dataset.match(F.field(\"emotion_detections\").exists())\n",
    "print(f\"Found {len(emotion_view)} samples with emotion temporal detections\")\n",
    "\n",
    "# Update the session view to show only samples with temporal data\n",
    "combined_view = dataset.match(\n",
    "    F.field(\"temporal_detections\").exists() | F.field(\"emotion_detections\").exists()\n",
    ")\n",
    "print(f\"Found {len(combined_view)} samples with any temporal detections\")\n",
    "\n",
    "# To update the current session view, uncomment:\n",
    "# session.view = combined_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's launch the FiftyOne app to visualize our dataset\n",
    "try:\n",
    "    session = fo.launch_app(dataset)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error launching FiftyOne app: {e}\")\n",
    "    import traceback\n",
    "    logging.error(traceback.format_exc())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyjokes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
