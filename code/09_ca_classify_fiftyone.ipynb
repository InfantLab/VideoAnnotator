{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 Classify videos by joke type and visualise with FiftyOne\n",
    "\n",
    "Each video demonstrates a single joke type. We have this as meta-data. Can we train a classifier based on the movement data?\n",
    "\n",
    "Each video shows one joke from a set of five possibilities [Peekaboo,TearingPaper,NomNomNom,ThatsNotAHat,ThatsNotACat].\n",
    "\n",
    "We will use TensorFlow to train a classifier to predict the joke type from the movement data.\n",
    "In order to see what classifier is doing, we will use [FiftyOne]() to visualise the data and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# local imports\n",
    "import utils\n",
    "import display\n",
    "import calcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Load the data\n",
    "\n",
    "### Use either small demo \n",
    "Consists of 54 videos. From 4 families (parent and baby) demoing five jokes three times each. (Some missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_in = os.path.join(\"..\",\"LookitLaughter.test\")\n",
    "demo_data = os.path.join(\"..\",\"data\", \"demo\")\n",
    "temp_out = os.path.join(\"..\",\"data\",\"0_temp\")\n",
    "data_out = os.path.join(\"..\",\"data\",\"1_interim\")\n",
    "videos_out = os.path.join(\"..\",\"data\",\"2_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or the Full set\n",
    "\n",
    "Consists of 1425 videos. From 90 familes (parent and baby) demoing approximately five jokes three times each. Some repetitions and omissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_in = os.path.join(\"..\",\"..\",\"LookitLaughter.full.videos\")\n",
    "temp_out = os.path.join(\"..\",\"..\",\"LookitLaughter.full.data\",\"0_temp\")\n",
    "data_out = os.path.join(\"..\",\"..\",\"LookitLaughter.full.data\",\"1_interim\")\n",
    "videos_out = os.path.join(\"..\",\"..\",\"LookitLaughter.full.data\",\"2_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedVideos = utils.getProcessedVideos(data_out)\n",
    "minFrames = processedVideos['Frames'].min()\n",
    "maxFrames = processedVideos['Frames'].max()\n",
    "print(f\"We have {len(processedVideos)} processed videos.\")\n",
    "print(f\"Min Frames: {minFrames}\\nMax Frames: {maxFrames}\")\n",
    "processedVideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use matplotlib to draw histograam of number of frames\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(processedVideos['Frames'], bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.2 Use Voxel51 and PytorchVideo for examining videos\n",
    "\n",
    "Voxel51 seems to be a useful tool for looking at training data (and trained predictions).\n",
    "\n",
    "Let's start with the minimal implementation. Just viewing videos.\n",
    "\n",
    "https://docs.voxel51.com/user_guide/dataset_creation/index.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.1 Is dataset already created?\n",
    "\n",
    "FiftyOne may aleady have a dataset created. Let's check. And reload it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = fo.list_datasets()\n",
    "if len(datasets) == 0:\n",
    "    print(\"No datasets found. Load in step 7.1.2\")\n",
    "else:\n",
    "    print(\"Loading saved datasets: \", datasets[0])\n",
    "    dataset = fo.load_dataset(datasets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 Populate a FiftyOne dataset with our videos and labels.\n",
    "\n",
    "Either there is no existing dataset or we want to rebuild it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: delete datasets\n",
    "fo.delete_datasets(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from a directory of videos\n",
    "dataset = fo.Dataset.from_videos_dir(videos_in)\n",
    "dataset.ensure_frames()\n",
    "\n",
    "dataset.name = 'LookitLaughter.full'\n",
    "\n",
    "dataset.add_sample_field(\"JokeType\", fo.StringField, description=\"What joke is being told?\")\n",
    "dataset.add_sample_field(\"HowFunny\", fo.StringField, description=\"How funny is the joke?\")\n",
    "dataset.add_sample_field(\"LaughYesNo\",  fo.BooleanField, description=\"Did the child laugh?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can add our metadata classifications. Recalling that each video demos one joke type `[Peekaboo,TearingPaper,NomNomNom,ThatsNotAHat,ThatsNotACat]` and has rating of how funny the baby found it `[Not Funny, Slightly Funny, Funny, Extremely Funny]` and whether they laughed `[Yes, No]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the joke type, how funny and laugh yes/no for each sample in the dataset\n",
    "for sample in dataset:\n",
    "    #split the filepath to get the video name, system independent\n",
    "    videoname = os.path.basename(sample.filepath)\n",
    "    row = processedVideos[processedVideos[\"VideoID\"]==videoname]\n",
    "    if len(row) == 0:\n",
    "        print(f\"Video {videoname} not found in processed videos.\")\n",
    "        continue\n",
    "    sample[\"VideoID\"]  = row[\"VideoID\"].values[0]\n",
    "    sample[\"JokeType\"]  = row[\"JokeType\"].values[0]\n",
    "    sample[\"HowFunny\"]  = row[\"HowFunny\"].values[0]\n",
    "    sample[\"LaughYesNo\"]  = (row[\"LaughYesNo\"].values[0] == \"Yes\")\n",
    "#    sample[\"JokeType\"]  = fo.Classification(label = row[\"JokeType\"].values[0])\n",
    "#    sample[\"HowFunny\"]  = fo.Classification(label = row[\"HowFunny\"].values[0])\n",
    "#    sample[\"LaughYesNo\"]  = fo.Classification(label = (row[\"LaughYesNo\"].values[0] == \"Yes\"))\n",
    "    \n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the frame by frame annotations directly onto the videos inside fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start with people bounding boxes\n",
    "\n",
    "for sample in dataset:\n",
    "    #retrieve people bounding boxes from the keypoints file\n",
    "    try:\n",
    "        keypoints = utils.readKeyPointsFromCSV(processedVideos,sample.filepath,normed= True)    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error reading keypoints for {sample.filepath}\")\n",
    "        continue\n",
    "    \n",
    "    for index, row in keypoints.iterrows():\n",
    "        framenumber = row[\"frame\"] + 1\n",
    "        person = row[\"person\"]\n",
    "        bbox = [row[\"bbox.x1\"], row[\"bbox.y1\"], row[\"bbox.x2\"], row[\"bbox.y2\"]]\n",
    "        bbox51 = calcs.xyxy2ltwh(bbox)\n",
    "        if sample.frames[framenumber]:\n",
    "            frame = sample.frames[framenumber]\n",
    "        else:\n",
    "            frame = fo.Frame()\n",
    "        frame[person] = fo.Detection(label=person, bounding_box=bbox51)\n",
    "        sample.frames[framenumber] = frame\n",
    "        #TODO fiftyone add keypoints not well documented and i can't get it to work. \n",
    "        #frame[person + \"KeyPoints\"] =  \n",
    "        #fo.KeypointSkeleton()\n",
    "        #TODO can't see how to add timesynced captions either!\n",
    "\n",
    "        sample.save()\n",
    "        \n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)\n",
    "# in docker launch fiftiy needs port\n",
    "# session = fo.launch_app(dataset, address=\"0.0.0.0\", port=5151)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Load and preprocess the data\n",
    "\n",
    "1. Load normed movement data. \n",
    "2. Exclude videos shorter than a min length\n",
    "3. Pad all sequences to the same max length. \n",
    "4. Interpolate missing values (up to last frame of real data).\n",
    "5. Replace final missing values with zeros.\n",
    "6. Add to tf.data.Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMovementDataset(processedVideos, minFrames = 0, maxFrames = None, ragged = False):\n",
    "    \"\"\"\n",
    "    Creates a movement dataset from processed videos.\n",
    "\n",
    "    Args:\n",
    "        processedVideos (pandas.DataFrame): A DataFrame containing processed video data.\n",
    "        minFrames (int, optional): Rows with less than minframes are excluded. Defaults to 0 (include all frames.)\n",
    "        maxFrames (int, optional): Data padded or truncated to have maxFrames. Defaults to max of all videos.\n",
    "        ragged (bool, optional): Whether to create a ragged tensor. Defaults to False (TODO: not implemented yet)\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: A TensorFlow Dataset containing features and labels.\n",
    "    \"\"\"\n",
    "    if maxFrames is None:\n",
    "        maxFrames = processedVideos[\"Frames\"].max()\n",
    "    if ragged:\n",
    "        raise NotImplementedError(\"Ragged tensors not implemented yet.\")    \n",
    "    dataset = []\n",
    "    labels = []\n",
    "    # for each row of processedVideos, we add one timeseries to the dataset\n",
    "    for index, r in processedVideos.iterrows():\n",
    "        df = pd.read_csv(r['Keypoints.normed'])\n",
    "        if r[\"Frames\"] < minFrames:\n",
    "            continue\n",
    "        df = utils.padMovementData(df, maxFrames)\n",
    "        df = utils.interpolateMovementData(df)\n",
    "        df = df.replace(np.nan, 0)\n",
    "        df = utils.flattenMovementDataset(df)\n",
    "        \n",
    "        features = tf.convert_to_tensor(df.values, dtype=tf.float32)\n",
    "        label = r[\"Joke.Label\"]\n",
    "        dataset.append(features)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return tf.data.Dataset.from_tensor_slices((dataset, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMinFrames = 100\n",
    "trainMaxFrames = 1000   \n",
    "tfdataset = createMovementDataset(processedVideos,trainMinFrames,trainMaxFrames)\n",
    "\n",
    "train, test = tf.keras.utils.split_dataset(tfdataset, left_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get first element of dataset so we can grab its dimensions\n",
    "keyPoints = next(iter(train))[0]\n",
    "\n",
    "#let's build a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(keyPoints.shape[0], keyPoints.shape[1])),\n",
    "    layers.LSTM(8),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "              sample_weight_mode='temporal',\n",
    "              optimizer=tf.optimizers.Adam(),\n",
    "              metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train.batch(32), epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's evaluate the model\n",
    "model.evaluate(test.batch(32))\n",
    "\n",
    "#table of predictions\n",
    "predictions = model.predict(test.batch(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
