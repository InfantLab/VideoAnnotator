{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Code Demonstrations\n",
    "\n",
    "This notebook demonstrates our feature extraction codebase with simple explanations of how each component works. It covers the full pipeline from video input to processed data.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#environment-setup)\n",
    "   - [Setup Python Environment](#setup-python-environment)\n",
    "   - [Check Module Imports](#check-module-imports)\n",
    "   - [Torch and CUDA Check](#torch-and-cuda-check)\n",
    "   - [Demo Data Overview](#demo-data-overview)\n",
    "2. [Keypoint Processing](#keypoint-processing)\n",
    "   - [YOLOv8 Pose Estimation](#yolov8-pose-estimation)\n",
    "   - [Video to Keypoints Dataframe](#video-to-keypoints-dataframe)\n",
    "   - [Filtering and Normalization](#filtering-and-normalization)\n",
    "   - [Full Keypoint Processing Pipeline](#full-keypoint-processing-pipeline)\n",
    "3. [Audio Processing](#audio-processing)\n",
    "   - [Extracting Audio from Video](#extracting-audio-from-video)\n",
    "   - [Speech Recognition](#speech-recognition)\n",
    "   - [Speaker Diarization](#speaker-diarization)\n",
    "   - [Additional Audio Analysis](#additional-audio-analysis)\n",
    "   - [Complete Audio Processing Pipeline](#complete-audio-processing-pipeline)\n",
    "4. [Visualization and Integration](#visualization-and-integration)\n",
    "   - [Visualizing Keypoint Data](#visualizing-keypoint-data)\n",
    "   - [Adding Annotations to Videos](#adding-annotations-to-videos)\n",
    "   - [Creating Complete Annotated Videos](#creating-complete-annotated-videos)\n",
    "\n",
    "Let's begin our exploration of these methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "## Setup Python Environment\n",
    "\n",
    "Follow the instructions in [README.md](README.md) to setup the python environment.\n",
    "\n",
    "## Check Module Imports\n",
    "\n",
    "Run the next cell to check that all modules are correctly imported. ModuleNotFound errors are _usually_  fixed by installing the missing libraries typing `pip install <library_name>` in terminal. For example:\n",
    "\n",
    "```bash\n",
    "ModuleNotFoundError: No module named 'ultralytics'\n",
    "\n",
    "> pip install ultralytics\n",
    "```\n",
    "\n",
    "There are a few exceptions:  \n",
    "For `pyannote` > `pip install pyannote.audio`  \n",
    "For `dotenv` > `pip install python-dotenv`  \n",
    "\n",
    "Then restart the kernel and run the cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0.3 Torch and CUDA?\n",
    "\n",
    "If you have a GPU and want to use it, make sure that torch is installed with CUDA support. You can check it by running the next cell.  \n",
    "If `True` you are good to go.  \n",
    "If `False` then might be best to start over in a new environment. Torch install instructions https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import our custom modules and check that they are working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# first, add project to path so we can import the modules\n",
    "project_root = os.path.join(\"..\")\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# impoty the functions for video processing\n",
    "import src.utils as utils\n",
    "\n",
    "import src.processors.keypoint_processor as kp_processor\n",
    "import src.processors.audio_processor as audio_processor\n",
    "import src.processors.video_processor as video_processor\n",
    "import src.processors.face_processor as face_processor\n",
    "import src.processors.object_processor as object_processor\n",
    "import src.processors.video_understanding as video_understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Demonstrating some of the function calls we use during feature extraction \n",
    "\n",
    "A useful set of simple examples to show how to call the models and parse the data they return.\n",
    "\n",
    "\n",
    "### 0.1.1 Demo data\n",
    "\n",
    "Where will we find videos, images and audio for our examples? Two videos, the associate audio files and a set of images are available in the `data\\demo` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "demo_data = os.path.join(\"..\",\"data\", \"demo\")\n",
    "\n",
    "# a couple of videos for testing\n",
    "VIDEO_FILE = os.path.join(demo_data, \"2UWdXP.joke1.rep2.take1.Peekaboo_h265.mp4\")\n",
    "VIDEO_FILE2 = os.path.join(demo_data, \"2UWdXP.joke2.rep1.take1.NomNomNom_h265.mp4\")\n",
    "\n",
    "AUDIO_FILE = os.path.join(demo_data, \"2UWdXP.joke1.rep2.take1.Peekaboo.mp3\")\n",
    "AUDIO_FILE2 = os.path.join(demo_data, \"2UWdXP.joke2.rep1.take1.NomNomNom.mp3\")\n",
    "\n",
    "IMAGE1 = os.path.join(demo_data, \"mother-and-baby.jpg\")\n",
    "IMAGE2 = os.path.join(demo_data, \"peekaboo.png\")\n",
    "IMAGE3 = os.path.join(demo_data, \"twopeople.jpg\")\n",
    "\n",
    "videoset = [VIDEO_FILE, VIDEO_FILE2]\n",
    "audioset = [AUDIO_FILE, AUDIO_FILE2]\n",
    "photoset = [IMAGE1, IMAGE2, IMAGE3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 YOLOv8\n",
    "\n",
    "Go to [docs.ultralytics.com](https://docs.ultralytics.com/) for detailed documentation and lots of examples. We just demo a few here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Load YOLOv8 model with pose estimation capability\n",
    "# The 'n' in yolov8n-pose.pt stands for 'nano' - the smallest and fastest model variant\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# Run inference on an image\n",
    "results = model(IMAGE3)\n",
    "print(f\"Results type: {type(results)}\")\n",
    "print(f\"Number of results: {len(results)}\")\n",
    "print(f\"Fields in first result: {dir(results[0])[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Display the image with keypoints, skeleton, and bounding boxes\n",
    "labelledimage = results[0].plot()\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(labelledimage)\n",
    "plt.title(\"YOLOv8 Pose Estimation Result\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Extract keypoints as numpy arrays\n",
    "keypoints = results[0].keypoints.cpu().numpy()\n",
    "print(f\"Keypoints shape: {keypoints.xy.shape} - (persons, keypoints, xy)\")\n",
    "print(\"\\nKeypoint coordinates (x,y):\")\n",
    "print(keypoints.xy)\n",
    "print(\"\\nKeypoint confidence scores:\")\n",
    "print(keypoints.conf)\n",
    "print(\"\\nFull keypoint data (x,y,confidence):\")\n",
    "print(keypoints.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# YOLOv8 returns keypoints as a 3D tensor with x, y, confidence values\n",
    "# For storage in dataframes, we typically flatten it to a 1D list\n",
    "if len(keypoints.data) > 0:  # Check if any people were detected\n",
    "    xyc = keypoints.data[0].flatten().tolist()  # Flatten first person's keypoints\n",
    "    print(f\"Flattened keypoint data (length: {len(xyc)}):\\n{xyc}\")\n",
    "    \n",
    "    # Explain the structure\n",
    "    print(\"\\nStructure: Each keypoint has 3 values - [x, y, confidence]\")\n",
    "    print(\"Example of the first few keypoints:\")\n",
    "    for i in range(0, 15, 3):  # Show first 5 keypoints\n",
    "        keypoint_idx = i // 3\n",
    "        print(f\"Keypoint {keypoint_idx}: x={xyc[i]:.2f}, y={xyc[i+1]:.2f}, confidence={xyc[i+2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video to Keypoints Dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoints Dataframe Structure\n",
    "\n",
    "Our keypoints dataframe has the following structure:\n",
    "\n",
    "![keypoints dataframe](../docs/keypointsdf.png)\n",
    "\n",
    "For each video `frame`:\n",
    "- Each detected person gets a row with `person` label and `index`\n",
    "- Bounding box coordinates: `x`, `y`, `w`, `h` (center x, center y, width, height)\n",
    "- Detection confidence: `conf`\n",
    "- 17 COCO keypoints: Each has 3 values (x, y, confidence)\n",
    "  - Keypoints include: nose, eyes, ears, shoulders, elbows, wrists, hips, knees, ankles\n",
    "\n",
    "Now, let's use the convenience function to process a complete video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Process a full video with a single function call\n",
    "# We limit to 60 frames for demonstration purposes\n",
    "df = src.utils.videotokeypoints(model, VIDEO_FILE, track=False, max_frames=60)\n",
    "\n",
    "print(f\"Processed {df['frame'].max()+1} frames and extracted {len(df)} rows of pose data\")\n",
    "\n",
    "# Save to CSV for later use\n",
    "stemname = os.path.splitext(VIDEO_FILE)[0]\n",
    "csvpath = os.path.join(data_out, os.path.basename(stemname) + \".csv\")\n",
    "df.to_csv(csvpath, index=False)\n",
    "print(f\"Saved keypoint data to {csvpath}\")\n",
    "\n",
    "# Display a sample\n",
    "df_read = pd.read_csv(csvpath, index_col=None)\n",
    "display(df_read.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1.2 model.track()\n",
    "\n",
    "YoloV8 also comes with a `model.track` method. This aims to keep track of all identified people (and other objects?) over the course of a video. \n",
    "\n",
    "This is pretty easy instead of calling \n",
    "`results = model(video_path, stream=True)`\n",
    "\n",
    "we can call\n",
    "`results = model.track(video_path, stream=True)`\n",
    "\n",
    "https://docs.ultralytics.com/modes/track/#persisting-tracks-loop\n",
    "\n",
    "Here's an inline example of it working.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get video dimensions for normalization\n",
    "video = cv2.VideoCapture(VIDEO_FILE)\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "video.release()\n",
    "print(f\"Video dimensions: {width}Ã—{height}\")\n",
    "\n",
    "# Let's examine the keypoint processing functions\n",
    "print(\"\\nKeypoint processor functions:\")\n",
    "print(\"1. Filter by confidence:\")\n",
    "print(keypoint_processor.filter_keypoints_by_confidence.__doc__)\n",
    "print(\"\\n2. Normalize coordinates:\")\n",
    "print(keypoint_processor.normalize_keypoints.__doc__)\n",
    "print(\"\\n3. Interpolate missing data:\")\n",
    "print(keypoint_processor.interpolate_missing_keypoints.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of filtering by confidence\n",
    "# First, let's count how many valid keypoints we have in the original data\n",
    "conf_columns = [col for col in df_read.columns if col.endswith('_c')]\n",
    "print(\"Original data:\")\n",
    "print(f\"Total keypoints: {len(df_read) * len(conf_columns)}\")\n",
    "\n",
    "# Apply filtering with different thresholds\n",
    "for threshold in [0.0, 0.3, 0.5, 0.7]:\n",
    "    # Using the function from keypoint_processor\n",
    "    filtered_df = keypoint_processor.filter_keypoints_by_confidence(\n",
    "        df_read.copy(), confidence_threshold=threshold\n",
    "    )\n",
    "    \n",
    "    # Count keypoints below threshold\n",
    "    below_threshold = 0\n",
    "    for col in conf_columns:\n",
    "        below_threshold += sum(filtered_df[col] < threshold)\n",
    "    \n",
    "    print(f\"Threshold {threshold}: {below_threshold} keypoints filtered out\")\n",
    "\n",
    "# Demonstrate normalization\n",
    "# Apply filtering first with threshold 0.5\n",
    "filtered_df = keypoint_processor.filter_keypoints_by_confidence(df_read.copy(), 0.5)\n",
    "\n",
    "# Then normalize coordinates\n",
    "normalized_df = keypoint_processor.normalize_keypoints(filtered_df, height, width)\n",
    "\n",
    "# Compare original and normalized coordinates for one keypoint\n",
    "print(\"\\nOriginal vs. Normalized coordinates (first row, keypoint 0):\")\n",
    "orig_x = filtered_df['k0_x'].iloc[0]\n",
    "orig_y = filtered_df['k0_y'].iloc[0]\n",
    "norm_x = normalized_df['k0_x'].iloc[0]\n",
    "norm_y = normalized_df['k0_y'].iloc[0]\n",
    "print(f\"Original: ({orig_x:.2f}, {orig_y:.2f}) pixels\")\n",
    "print(f\"Normalized: ({norm_x:.4f}, {norm_y:.4f}) [0-1 range]\")\n",
    "print(f\"Verification: {norm_x:.4f} = {orig_x:.2f}/{width}, {norm_y:.4f} = {orig_y:.2f}/{height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Keypoint Processing Pipeline\n",
    "\n",
    "The `keypoint_processor.py` module provides a comprehensive function `process_keypoints_for_modeling()` that applies all processing steps in sequence. This is the recommended way to prepare keypoint data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the full processing pipeline function\n",
    "print(\"Full keypoint processing pipeline:\")\n",
    "print(keypoint_processor.process_keypoints_for_modeling.__doc__)\n",
    "\n",
    "# For demonstration purposes, create a simple configuration\n",
    "# In practice, this would be imported from src.config\n",
    "keypoint_processor.KEYPOINT_CONFIG = {\n",
    "    'confidence_threshold': 0.5,\n",
    "    'interpolate_missing': True\n",
    "}\n",
    "\n",
    "# Apply the full processing pipeline\n",
    "processed_df = keypoint_processor.process_keypoints_for_modeling(df_read, height, width)\n",
    "\n",
    "print(f\"\\nProcessed {len(processed_df)} rows of keypoint data\")\n",
    "print(\"\\nProcessed dataframe sample:\")\n",
    "display(processed_df[['frame', 'person', 'index', 'k0_x', 'k0_y', 'k0_c']].head(3))\n",
    "\n",
    "# Verify the range of normalized coordinates\n",
    "x_cols = [col for col in processed_df.columns if col.endswith('_x')]\n",
    "y_cols = [col for col in processed_df.columns if col.endswith('_y')]\n",
    "\n",
    "print(f\"\\nX coordinate range: [{processed_df[x_cols].min().min():.3f}, {processed_df[x_cols].max().max():.3f}]\")\n",
    "print(f\"Y coordinate range: [{processed_df[y_cols].min().min():.3f}, {processed_df[y_cols].max().max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Processing\n",
    "\n",
    "Our `audio_processor.py` module contains a suite of functions for extracting and analyzing audio from videos. Let's explore these functions step by step.\n",
    "\n",
    "## Extracting Audio from Video\n",
    "\n",
    "First, we need to extract the audio track from a video file using the `extract_audio()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the audio extraction function\n",
    "print(\"Audio extraction function:\")\n",
    "print(audio_processor.extract_audio.__doc__)\n",
    "\n",
    "# Extract audio from video\n",
    "audio_path = audio_processor.extract_audio(VIDEO_FILE, temp_out, output_ext=\"wav\")\n",
    "print(f\"\\nExtracted audio saved to: {audio_path}\")\n",
    "\n",
    "# Play the extracted audio\n",
    "display(Audio(audio_path))\n",
    "\n",
    "# Show audio information\n",
    "try:\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "    \n",
    "    print(f\"\\nAudio information:\")\n",
    "    print(f\"  Sample rate: {sr} Hz\")\n",
    "    print(f\"  Duration: {duration:.2f} seconds\")\n",
    "    print(f\"  Number of samples: {len(y)}\")\n",
    "    \n",
    "    # Plot waveform\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(np.linspace(0, duration, len(y)), y)\n",
    "    plt.title(\"Audio Waveform\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing audio: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition\n",
    "\n",
    "Next, we can transcribe the speech in the audio using OpenAI's Whisper model. Our `transcribe_audio()` function handles this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the transcription function\n",
    "print(\"Audio transcription function:\")\n",
    "print(audio_processor.transcribe_audio.__doc__)\n",
    "\n",
    "try:\n",
    "    # Transcribe the extracted audio\n",
    "    print(\"\\nTranscribing audio (this may take a moment)...\")\n",
    "    transcript_path, transcript_data = audio_processor.transcribe_audio(\n",
    "        audio_path, temp_out, model_name=\"base\"\n",
    "    )\n",
    "    \n",
    "    if transcript_path and transcript_data:\n",
    "        print(f\"Transcript saved to: {transcript_path}\")\n",
    "        \n",
    "        print(\"\\nFull transcript:\")\n",
    "        print(transcript_data['text'])\n",
    "        \n",
    "        # Show the segments with timestamps\n",
    "        print(\"\\nTranscript segments:\")\n",
    "        for i, segment in enumerate(transcript_data['segments'][:3]):  # Show first 3 segments\n",
    "            print(f\"Segment {i+1}: {segment['start']:.2f}s - {segment['end']:.2f}s\")\n",
    "            print(f\"  '{segment['text']}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during transcription: {e}\")\n",
    "    print(\"This may be due to Whisper not being installed or other dependencies missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check captions for this video at\n",
    "with open(SPEECH_FILE) as f:\n",
    "    speechjson = json.load(f)\n",
    "caption = display.WhisperExtractCurrentCaption(speechjson, framenum, 15)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.3 Facial Emotion Recognition\n",
    "\n",
    "we use DeepFace wrapped in a fucntion to store the resutls to a dataframe, indexed by person detected and video frame number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "demo_data = os.path.join(\"..\",\"data\", \"demo\")\n",
    "\n",
    "# a couple of videos for testing\n",
    "VIDEO_FILE = os.path.join(demo_data, \"2UWdXP.joke1.rep2.take1.Peekaboo_h265.mp4\")\n",
    "VIDEO_FILE2 = os.path.join(demo_data, \"2UWdXP.joke2.rep1.take1.NomNomNom_h265.mp4\")\n",
    "\n",
    "AUDIO_FILE = os.path.join(demo_data, \"2UWdXP.joke1.rep2.take1.Peekaboo.mp3\")\n",
    "AUDIO_FILE2 = os.path.join(demo_data, \"2UWdXP.joke2.rep1.take1.NomNomNom.mp3\")\n",
    "\n",
    "IMAGE1 = os.path.join(demo_data, \"mother-and-baby.jpg\")\n",
    "IMAGE2 = os.path.join(demo_data, \"peekaboo.png\")\n",
    "IMAGE3 = os.path.join(demo_data, \"twopeople.jpg\")\n",
    "\n",
    "videoset = [VIDEO_FILE, VIDEO_FILE2]\n",
    "audioset = [AUDIO_FILE, AUDIO_FILE2]\n",
    "photoset = [IMAGE1, IMAGE2, IMAGE3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#let's check the face detectiion works\n",
    "features = (\"emotion\", \"age\", \"gender\")\n",
    "\n",
    "backend = backends[1] # opencv\n",
    "for backend in backends:    \n",
    "    print(f\"Using backend: {backend}\")\n",
    "    for photo in photoset:\n",
    "        print(photo)\n",
    "        img = cv2.imread(photo)\n",
    "        faces = extract_faces_from_image(img, backend=backend, features=features, precision=5, debug=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.9 Diarization with pyannote\n",
    "\n",
    "We can use pyannote to diarize the audio and then use the results to extract the speech from the audio.\n",
    "\n",
    "The code is in our utils.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "diarization = utils.diarize_audio(AUDIO_FILE)\n",
    "\n",
    "with open(\"output.rttm\", \"w\") as rttm:\n",
    "    diarization.write_rttm(rttm)\n",
    "\n",
    "print(diarization)  # diarizaation as values\n",
    "\n",
    "diarization # as visual timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Audio Analysis\n",
    "\n",
    "Our `audio_processor.py` module also provides functions for extracting additional audio features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the fundamental frequency extraction function\n",
    "print(\"Fundamental frequency extraction function:\")\n",
    "print(audio_processor.extract_fundamental_frequency.__doc__)\n",
    "\n",
    "try:\n",
    "    print(\"\\nExtracting fundamental frequency (this may take a moment)...\")\n",
    "    f0_path = audio_processor.extract_fundamental_frequency(audio_path, temp_out)\n",
    "    \n",
    "    if f0_path:\n",
    "        print(f\"F0 data saved to {f0_path}\")\n",
    "        \n",
    "        # Load and analyze the F0 data\n",
    "        f0_data = np.load(f0_path)\n",
    "        f0 = f0_data['f0']\n",
    "        voiced_flag = f0_data['voiced_flag']\n",
    "        \n",
    "        # Basic statistics\n",
    "        n_voiced = np.sum(voiced_flag)\n",
    "        voiced_f0 = f0[voiced_flag]\n",
    "        \n",
    "        print(f\"Total frames: {len(f0)}\")\n",
    "        print(f\"Voiced frames: {n_voiced} ({n_voiced/len(f0)*100:.1f}%)\")\n",
    "        \n",
    "        if len(voiced_f0) > 0:\n",
    "            print(f\"\\nF0 statistics (Hz):\")\n",
    "            print(f\"  Mean: {np.mean(voiced_f0):.1f}\")\n",
    "            print(f\"  Median: {np.median(voiced_f0):.1f}\")\n",
    "            print(f\"  Min: {np.min(voiced_f0):.1f}\")\n",
    "            print(f\"  Max: {np.max(voiced_f0):.1f}\")\n",
    "            \n",
    "            # Plot the F0 contour\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            \n",
    "            # Create time axis\n",
    "            y, sr = librosa.load(audio_path)\n",
    "            duration = librosa.get_duration(y=y, sr=sr)\n",
    "            time_axis = np.linspace(0, duration, len(f0))\n",
    "            \n",
    "            # Plot only voiced frames\n",
    "            plt.plot(time_axis[voiced_flag], f0[voiced_flag], 'b-')\n",
    "            plt.title('Fundamental Frequency (F0) Contour')\n",
    "            plt.xlabel('Time (seconds)')\n",
    "            plt.ylabel('Frequency (Hz)')\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting fundamental frequency: {e}\")\n",
    "\n",
    "# Look at the laughter detection function\n",
    "print(\"\\nLaughter detection function:\")\n",
    "print(audio_processor.detect_laughter.__doc__)\n",
    "print(\"\\nNote: Laughter detection requires additional installation steps.\")\n",
    "print(\"We'll show sample output for demonstration purposes.\")\n",
    "\n",
    "# Sample laughter detection output\n",
    "sample_laughter = [\n",
    "    {\"start\": 3.2, \"end\": 5.7, \"prob\": 0.89},\n",
    "    {\"start\": 12.5, \"end\": 14.3, \"prob\": 0.76}\n",
    "]\n",
    "\n",
    "print(\"\\nSample laughter detection output:\")\n",
    "for i, segment in enumerate(sample_laughter):\n",
    "    print(f\"Laughter {i+1}: {segment['start']:.1f}s - {segment['end']:.1f}s (probability: {segment['prob']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Audio Processing Pipeline\n",
    "\n",
    "The `audio_processor.py` module provides a comprehensive function `process_audio()` that combines all audio processing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the complete audio processing function\n",
    "print(\"Complete audio processing pipeline:\")\n",
    "print(audio_processor.process_audio.__doc__)\n",
    "\n",
    "# For demonstration, we'll create a short clip to process\n",
    "try:\n",
    "    # Create a short clip for faster processing\n",
    "    short_clip_path = os.path.join(temp_out, \"demo_short.mp4\")\n",
    "    video_clip = mp.VideoFileClip(VIDEO_FILE).subclip(0, 5)  # First 5 seconds\n",
    "    video_clip.write_videofile(short_clip_path, codec=\"libx264\", audio_codec=\"aac\", logger=None)\n",
    "    video_clip.close()\n",
    "    \n",
    "    print(f\"\\nProcessing short video clip: {os.path.basename(short_clip_path)}\")\n",
    "    print(\"This will demonstrate the full audio processing pipeline...\")\n",
    "    \n",
    "    # Process with most features enabled, but disable diarization if no token\n",
    "    results = audio_processor.process_audio(\n",
    "        short_clip_path, \n",
    "        temp_out, \n",
    "        enable_whisper=True,\n",
    "        enable_diarization=(hf_token is not None),\n",
    "        enable_f0=True,\n",
    "        enable_laughter=False,  # Disable for demo as it requires additional setup\n",
    "        force_process=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\nProcessing complete! Results:\")\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, str):\n",
    "            print(f\"  {key}: {os.path.basename(value)}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(value)}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError in audio processing pipeline: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.7 visualising data over time\n",
    "\n",
    "some of the calculations to help us visualise the movement of participants over time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that calculates the average x and y coordinates of a set of keypoints (where confidence score is above a threshold)\n",
    "xycs = np.array(\n",
    "    [\n",
    "        [1, 2, 0.9],\n",
    "        [2, 3, 0.8],\n",
    "        [3, 4, 0.7],\n",
    "        [4, 5, 0.6],\n",
    "        [5, 6, 0.5],\n",
    "        [6, 7, 0.4],\n",
    "        [7, 8, 0.3],\n",
    "        [8, 9, 0.2],\n",
    "        [9, 10, 0.1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "avgx, avgy = calcs.avgxys(xycs, threshold=0.5)\n",
    "\n",
    "print(avgx, avgy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framenumber = 34\n",
    "bboxlabels, bboxes, xycs = utils.getFrameKpts(df, framenumber)\n",
    "\n",
    "print(bboxlabels)\n",
    "print(bboxes)\n",
    "print(xycs)\n",
    "\n",
    "video = cv2.VideoCapture(VIDEO_FILE)\n",
    "video.set(cv2.CAP_PROP_POS_FRAMES, framenumber)\n",
    "success, image = video.read()\n",
    "video.release()\n",
    "\n",
    "image = display.drawOneFrame(image, bboxlabels, bboxes, xycs)\n",
    "\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.8 Adding annotations to the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedvideos = utils.getProcessedVideos(data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's grab a single frame from the video\n",
    "\n",
    "framenum = 60\n",
    "video = cv2.VideoCapture(VIDEO_FILE)\n",
    "video.set(cv2.CAP_PROP_POS_FRAMES, framenum)\n",
    "ret, frame = video.read()\n",
    "video.release()\n",
    "\n",
    "if ret:\n",
    "    plt.imshow(frame)\n",
    "    plt.show()\n",
    "\n",
    "videoname = os.path.basename(VIDEO_FILE)\n",
    "\n",
    "kpts = utils.getKeyPoints(processedvideos, videoname)\n",
    "kpts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, the extract movement algorithm has mislabelled the adult and the child (the labels get applied at random). We need to swap the labels around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts = utils.relabelPersonIndex(\n",
    "    kpts, person=\"child\", index=0, newPerson=\"temp\", newIndex=100\n",
    ")\n",
    "kpts = utils.relabelPersonIndex(\n",
    "    kpts, person=\"adult\", index=1, newPerson=\"child\", newIndex=0\n",
    ")\n",
    "kpts = utils.relabelPersonIndex(\n",
    "    kpts, person=\"temp\", index=100, newPerson=\"adult\", newIndex=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.8.7 Add annotations onto a video.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedVideo = display.createAnnotatedVideo(\n",
    "    VIDEO_FILE, kpts, facedata, speechdata, temp_out, True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyjokes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
