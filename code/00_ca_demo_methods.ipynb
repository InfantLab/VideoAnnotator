{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Demonstrating some of the function calls we use during feature extraction \n",
    "\n",
    "A useful set of simple examples to show how to call the models and parse the data they return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Demo data\n",
    "\n",
    "Where will we find videos, images and audio for our examples? Two videos, the associate audio files and a set of images are available in the `data\\demo` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "demo_data = os.path.join(\"..\",\"data\", \"demo\")\n",
    "\n",
    "# a couple of videos for testing\n",
    "VIDEO_FILE = os.path.join(demo_data, \"2UWdXP.joke1.rep2.take1.Peekaboo.mp4\")\n",
    "VIDEO_FILE2 = os.path.join(demo_data, \"2UWdXP.joke2.rep1.take1.NomNomNom.mp4\")\n",
    "\n",
    "AUDIO_FILE = os.path.join(demo_data, \"2UWdXP.joke1.rep2.take1.Peekaboo.mp3\")\n",
    "AUDIO_FILE2 = os.path.join(demo_data, \"2UWdXP.joke2.rep1.take1.NomNomNom.mp3\")\n",
    "\n",
    "IMAGE1 = os.path.join(demo_data, \"mother-and-baby.jpg\")\n",
    "IMAGE2 = os.path.join(demo_data, \"peekaboo.png\")\n",
    "IMAGE3 = os.path.join(demo_data, \"twopeople.jpg\")\n",
    "\n",
    "videoset = [VIDEO_FILE, VIDEO_FILE2]\n",
    "audioset = [AUDIO_FILE, AUDIO_FILE2]\n",
    "photoset = [IMAGE1, IMAGE2, IMAGE3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 YOLOv8\n",
    "\n",
    "Go to [docs.ultralytics.com](https://docs.ultralytics.com/) for detailed documentation and lots of examples. We just demo a few here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import calcs  # local calcs.py contains some helper functions\n",
    "import utils  # local utils.py contains some helper functions\n",
    "import display  # local display.py contains display helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.1 Pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a yolo model with pose estimation\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "# the results will contain object detection and pose estimation data.\n",
    "results = model(IMAGE3)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically display image overlayed with keypoints, skeleton and bounding boxes\n",
    "labelledimage = results[0].plot()\n",
    "plt.imshow(labelledimage)\n",
    "plt.show()\n",
    "\n",
    "# get the keypoints as a numpy arrays of x,y coordinates each with a confidence score.\n",
    "# note yolo returns tensors so we have to convert to numpy\n",
    "keypoints = results[0].keypoints.cpu().numpy()\n",
    "print(keypoints.xy)\n",
    "print(keypoints.conf)\n",
    "print(keypoints.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo returns keypoints as a 3 x 17 tensor of x,y,confidence, we typically flatten it to a 51 element list to store in dataframes\n",
    "xyc = keypoints.data[0].flatten().tolist()\n",
    "print(xyc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1.2 YOLOv8 video -> keypoints dataframe\n",
    "\n",
    "If we pass model a video rather than image, the results object must be iterated over to get the results for each frame.\n",
    "\n",
    "We extract movement and save it to dataframe using our own helper functions: \n",
    "\n",
    "* `utils.createKeypointsDF` - initialise an empty keypoints dataframe\n",
    "* `utils.addKeypointsToDF` - adds keypoints to dataframe\n",
    "* `utils.videotodf` - extracts keypoints from video and saves to dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(VIDEO_FILE, stream=True)\n",
    "df = utils.createKeypointsDF()\n",
    "frame = 0\n",
    "for r in results:\n",
    "    # print(torch.flatten(r.keypoints.xy[0]).tolist())\n",
    "    df = utils.addKeypointsToDF(df, frame, r.boxes.xywh, r.boxes.conf, r.keypoints.data)\n",
    "    frame += 1\n",
    "\n",
    "print(f\"Video {VIDEO_FILE} has {frame} frames and {len(df)} rows of data\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our keypoints dataframe has the following structure\n",
    "\n",
    "![keypoints dataframe](../docs/keypointsdf.png)\n",
    "\n",
    "For each video `frame`, we have one row person `person` and `index`. The next five columns describe the bounding box for that person marked by it's top left `(x1,y1)` and bottom right `(x2,y2)` corners. This is followed 51 columns representing 17 COCO pose points each labelled with `(x,y)` coordinate and a confidence `c` between (0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.videotokeypoints(model, VIDEO_FILE, track=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemname = os.path.splitext(VIDEO_FILE)[0]\n",
    "csvpath = stemname + \".csv\"\n",
    "df.to_csv(csvpath, index=False)\n",
    "\n",
    "df = pd.read_csv(csvpath, index_col=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying data \n",
    "\n",
    "Two functions help display keypoint and other data overlayed on frame. \n",
    "\n",
    "* `utils.getFrameKpts` takes keypoints dataframe and framenumber and returns list of all bounding boxes, their labels and corresponding keypoints.\n",
    "* `display.drawOneFrame` takes thes outputs and draws them on the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framenumber = 34\n",
    "bboxlabels, bboxes, xycs = utils.getFrameKpts(df, framenumber)\n",
    "\n",
    "print(bboxlabels)\n",
    "print(bboxes)\n",
    "print(xycs)\n",
    "\n",
    "video = cv2.VideoCapture(VIDEO_FILE)\n",
    "video.set(cv2.CAP_PROP_POS_FRAMES, framenumber)\n",
    "success, image = video.read()\n",
    "video.release()\n",
    "\n",
    "image = display.drawOneFrame(image, bboxlabels, bboxes, xycs)\n",
    "\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1.2 model.track()\n",
    "\n",
    "YoloV8 also comes with a `model.track` method. This aims to keep track of all identified people (and other objects?) over the course of a video. \n",
    "\n",
    "This is pretty easy instead of calling \n",
    "`results = model(video_path, stream=True)`\n",
    "\n",
    "we can call\n",
    "`results = model.track(video_path, stream=True)`\n",
    "\n",
    "https://docs.ultralytics.com/modes/track/#persisting-tracks-loop\n",
    "\n",
    "Here's an inline example of it working.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video file\n",
    "video_path = VIDEO_FILE\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        results = model.track(frame, persist=True)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Extracting Speech\n",
    "\n",
    "We extract the audio and then use off the shelf speech recognition to extract the text.\n",
    "\n",
    "### 0.2.1 Extracting audio with moviepy\n",
    "\n",
    "MoviePy is basic movie editing tool that wraps ffmpeg and allows us to extract audio from video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mp\n",
    "\n",
    "video_path = VIDEO_FILE\n",
    "output_ext = \"mp3\"\n",
    "output_ext = \"wav\"\n",
    "\n",
    "filename = os.path.splitext(video_path)[0]\n",
    "clip = mp.VideoFileClip(video_path)\n",
    "audio_file = os.path.join(f\"{filename}.{output_ext}\")\n",
    "clip.audio.write_audiofile(audio_file)\n",
    "clip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playback the audio file\n",
    "from IPython.display import Audio\n",
    "\n",
    "Audio(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.7 visualising data over time\n",
    "\n",
    "some of the calculations to help us visualise the movement of participants over time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that calculates the average x and y coordinates of a set of keypoints (where confidence score is above a threshold)\n",
    "xycs = np.array(\n",
    "    [\n",
    "        [1, 2, 0.9],\n",
    "        [2, 3, 0.8],\n",
    "        [3, 4, 0.7],\n",
    "        [4, 5, 0.6],\n",
    "        [5, 6, 0.5],\n",
    "        [6, 7, 0.4],\n",
    "        [7, 8, 0.3],\n",
    "        [8, 9, 0.2],\n",
    "        [9, 10, 0.1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "avgx, avgy = calcs.avgxys(xycs, threshold=0.5)\n",
    "\n",
    "print(avgx, avgy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.8 Adding annotations to the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_in = r\"..\\LookitLaughter.test\"\n",
    "metadata_file = \"_LookitLaughter.xlsx\"\n",
    "data_out = r\"..\\data\\1_interim\"\n",
    "videos_out = r\"..\\data\\2_final\"\n",
    "temp_out = r\"..\\data\\0_temp\"\n",
    "\n",
    "# a couple of files for testing\n",
    "VIDEO_FILE = os.path.join(videos_in, \"2UWdXP.joke1.rep2.take1.Peekaboo.mp4\")\n",
    "VIDEO_FILE2 = os.path.join(videos_in, \"2UWdXP.joke2.rep1.take1.NomNomNom.mp4\")\n",
    "AUDIO_FILE = os.path.join(data_out, \"2UWdXP.joke1.rep2.take1.Peekaboo.wav\")\n",
    "AUDIO_FILE2 = os.path.join(data_out, \"2UWdXP.joke2.rep1.take1.NomNomNom.wav\")\n",
    "SPEECH_FILE = os.path.join(data_out, \"2UWdXP.joke1.rep2.take1.Peekaboo.json\")\n",
    "SPEECH_FILE2 = os.path.join(data_out, \"2UWdXP.joke2.rep1.take1.NomNomNom.json\")\n",
    "\n",
    "testset = [VIDEO_FILE, VIDEO_FILE2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedvideos = utils.getProcessedVideos(data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's grab a single frame from the video\n",
    "\n",
    "framenum = 60\n",
    "video = cv2.VideoCapture(VIDEO_FILE)\n",
    "video.set(cv2.CAP_PROP_POS_FRAMES, framenum)\n",
    "ret, frame = video.read()\n",
    "video.release()\n",
    "\n",
    "if ret:\n",
    "    plt.imshow(frame)\n",
    "    plt.show()\n",
    "\n",
    "videoname = os.path.basename(VIDEO_FILE)\n",
    "\n",
    "kpts = utils.getKeyPoints(processedvideos, videoname)\n",
    "kpts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.8.7 Add annotations onto a video.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's grab a single frame from the video\n",
    "bboxlabels, bboxes, xycs = utils.getFrameKpts(kpts, framenum)\n",
    "print(bboxlabels)\n",
    "print(bboxes)\n",
    "print(xycs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = frame.shape[:2]\n",
    "print(h, w)\n",
    "\n",
    "frame = display.drawOneFrame(frame, bboxlabels, bboxes, xycs, \"Peek-a-boo\")\n",
    "\n",
    "plt.imshow(frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check captions for this video at\n",
    "with open(SPEECH_FILE) as f:\n",
    "    speechjson = json.load(f)\n",
    "caption = display.WhisperExtractCurrentCaption(speechjson, framenum, 15)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get all the annotations for this video\n",
    "kpts = utils.getKeyPoints(processedvideos, videoname)\n",
    "facedata = utils.getFaceData(processedvideos, videoname)\n",
    "speechdata = utils.getSpeechData(processedvideos, videoname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, the extract movement algorithm has mislabelled the adult and the child (the labels get applied at random). We need to swap the labels around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts = utils.relabelPersonIndex(\n",
    "    kpts, person=\"child\", index=0, newPerson=\"temp\", newIndex=100\n",
    ")\n",
    "kpts = utils.relabelPersonIndex(\n",
    "    kpts, person=\"adult\", index=1, newPerson=\"child\", newIndex=0\n",
    ")\n",
    "kpts = utils.relabelPersonIndex(\n",
    "    kpts, person=\"temp\", index=100, newPerson=\"adult\", newIndex=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedVideo = display.createAnnotatedVideo(\n",
    "    VIDEO_FILE, kpts, facedata, speechdata, temp_out, True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the audio file back onto the annotated video\n",
    "# annotatedVideo = r\"..\\data\\2_final\\2UWdXP.joke1.rep2.take1.Peekaboo_annotated.mp4\"\n",
    "\n",
    "from moviepy.editor import *\n",
    "\n",
    "videoclip = VideoFileClip(annotatedVideo)\n",
    "audioclip = AudioFileClip(AUDIO_FILE)\n",
    "\n",
    "videoclip = videoclip.set_audio(audioclip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioclip.ipython_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoclip.ipython_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note need to save with a different name as the original video is still open\n",
    "videoclip.write_videofile(\n",
    "    r\"..\\data\\0_temp\\2UWdXP.joke1.rep2.take1.Peekaboo_annotated_audio.mp4\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyjokes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
