{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Extract speech and laughter from audio files\n",
    "\n",
    "For speech recognition we try the [SpeechBrain](https://github.com/speechbrain/speechbrain) project and OpenAI's [Whisper](https://github.com/openai/whisper) model.\n",
    "\n",
    "We also try identifying laughter with [Laughter Detection model](https://github.com/jrgillick/laughter-detection) by jrgillick. \n",
    "\n",
    "This code here is based on prototypes developed at Sage IDEMS hackathon in 2023 \n",
    "https://github.com/chilledgeek/ethical_ai_hackathon_2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_in = os.path.join(\"..\",\"LookitLaughter.test\")\n",
    "data_out = os.path.join(\"..\", \"data\", \"1_interim\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_data = os.path.join(\"..\",\"data\", \"demo\")\n",
    "AUDIO_FILE = os.path.join(demo_data, \"2UWdXP.joke1.rep2.take1.Peekaboo.mp3\")\n",
    "AUDIO_FILE2 = os.path.join(demo_data, \"2UWdXP.joke2.rep1.take1.NomNomNom.mp3\")\n",
    "testset = [AUDIO_FILE, AUDIO_FILE2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedvideos = utils.getProcessedVideos(data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Audio extraction with moviepy\n",
    "\n",
    "The first step is simple. We extract the audio from each video and save it as `mp3` or `wav`. We will use the `moviepy` library to do this. \n",
    "This will be helpful for later analysis and regenerating labeled videos with audio.\n",
    "\n",
    "Note that `moviepy` is a wrapper around `ffmpeg` and `ffmpeg` needs to be installed separately. \n",
    "\n",
    "`conda install ffmpeg moviepy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forceaudio = False\n",
    "#output_ext=\"mp3\"\n",
    "output_ext=\"wav\"\n",
    "\n",
    "for index, r in processedvideos.iterrows():\n",
    "    if forceaudio or pd.isnull(r[\"Audio.file\"]):\n",
    "        audiopath = utils.convert_video_to_audio_moviepy(videos_in,r[\"VideoID\"], data_out, output_ext=output_ext)\n",
    "        r[\"Audio.file\"] = audiopath\n",
    "        r[\"Audio.when\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "        #update this row in processedvideos dataframe\n",
    "        processedvideos.loc[index] = r\n",
    "    else:\n",
    "        print(\"Audio already extracted for video: \", r[\"VideoID\"])\n",
    "        \n",
    "\n",
    "utils.saveProcessedVideos(processedvideos, data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Speech-to-text \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 How not do to it  \n",
    "\n",
    "We tried [SpeechBrain]((https://github.com/speechbrain/speechbrain), [Speech Recognition](https://github.com/Uberi/speech_recognition) and [Sphinx](https://github.com/cmusphinx/pocketsphinx). None of them worked out of the box. Speechbrain was fiddly and not very accurate (with default settings). Rather than trying to improve it. Let's try the OpenAI Whisper model instead.\n",
    "\n",
    "See our [version 0.5alpha](https://github.com/InfantLab/babyjokes/releases/tag/v0.5alpha-sage) for the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Speech-to-text using OpenAI Whisper \n",
    "\n",
    "There is a free version of the [OpenAI Whisper](https://github.com/openai/whisper) model. It is multilingual (xx languages) and comes in a range of different sizes (and accuracies). We'll try the `base` model. \n",
    "\n",
    "Simple tutorial: https://analyzingalpha.com/openai-whisper-python-tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whisper_transcribe(audio_file, save_path, saveJSON = True):\n",
    "    result = model.transcribe(audio_file, verbose = True)\n",
    "    if saveJSON:\n",
    "        basename = os.path.basename(audio_file)\n",
    "        filename, ext = os.path.splitext(basename)\n",
    "        jsonfile = os.path.join(save_path,filename,\".json\")\n",
    "        with open(jsonfile, \"w\") as f:\n",
    "            json.dump(result, f)\n",
    "        return jsonfile, result\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedvideos = utils.getProcessedVideos(data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, r in processedvideos.iterrows():\n",
    "    if pd.isnull(r[\"Speech.file\"]) and not pd.isnull(r[\"Audio.file\"]):\n",
    "        speechpath, result = whisper_transcribe(r[\"Audio.file\"],save_path=data_out)\n",
    "        r[\"Speech.file\"] = speechpath\n",
    "        r[\"Speech.when\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "        #update this row in processedvideos dataframe\n",
    "        processedvideos.loc[index] = r\n",
    "        \n",
    "utils.saveProcessedVideos(processedvideos, data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Speaker annotation with PyAnnote\n",
    "\n",
    "We'll use the [PyAnnote](https://github.com/pyannote/pyannote-audio) library to annotate the speakers in the audio. This library listens to the audio and tries to identify the individual speakers. It provides a 'diarization' of the audio. Identifying the start and end of each utterance and assigning them to particular speakers. It can handle multiple speakers and overlapping speech. \n",
    "\n",
    "`conda install pyannote.audio -c pyannote`\n",
    "\n",
    "or \n",
    "\n",
    "`pip install pyannote.audio`\n",
    "\n",
    "To access the pretrained models you need a HuggingFace API token. Store this in .env file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#may want to add two new columns to the processedvideos dataframe\n",
    "#Diary.file: the path to the diary file\n",
    "#Diary.when: the time when the diary file was created\n",
    "\n",
    "if (\"Diary.file\" not in processedvideos.columns):\n",
    "    processedvideos[\"Diary.file\"] = None\n",
    "if (\"Diary.when\" not in processedvideos.columns):\n",
    "    processedvideos[\"Diary.when\"] = None    \n",
    "\n",
    "utils.saveProcessedVideos(processedvideos, data_out)\n",
    "processedvideos.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forceDiary = True\n",
    "\n",
    "for index, r in processedvideos.iterrows():\n",
    "    if (forceDiary or pd.isnull(r[\"Diary.file\"])) and not pd.isnull(r[\"Audio.file\"]):\n",
    "        diary = diarize_audio(r[\"Audio.file\"])\n",
    "        #save the diaryjson to a file\n",
    "        basename = os.path.basename(r[\"Audio.file\"])\n",
    "        filename, ext = os.path.splitext(basename)\n",
    "        diarypath = os.path.join(data_out, filename + \".diary.rttm\")\n",
    "        with open(diarypath, \"w\") as f:\n",
    "            diary.write_rttm(f)\n",
    "        r[\"Diary.file\"] = diarypath\n",
    "        r[\"Diary.when\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "        #update this row in processedvideos dataframe\n",
    "        processedvideos.loc[index] = r\n",
    "\n",
    "utils.saveProcessedVideos(processedvideos, data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use load_rttm to reload the rttm file\n",
    "\n",
    "# Load the diarization output from the RTTM file\n",
    "_, diary = load_rttm(\"output.rttm\").popitem()\n",
    "\n",
    "diary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Voice fundamental frequency (F0) estimation\n",
    "\n",
    "A simvple estimate is easy to get from `librosa`. So let's include it. Will use diarization to do it per utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_f0(audio_file):\n",
    "    \"\"\"\n",
    "    Extract the fundamental frequency (F0) from an audio file.\n",
    "    Args:\n",
    "        audio_file (str): The path to the audio file.\n",
    "    Returns:\n",
    "        np.array: The fundamental frequency values.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    # Extract the fundamental frequency\n",
    "    f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
    "    return f0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 #TODO - Laughter detection\n",
    "\n",
    "We would like to process videos to identifying laughter with [Laughter Detection model](https://github.com/jrgillick/laughter-detection) by jrgillick. However, want to find a simple way to call that from remote project rather than incorporating code into our own project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laughter_segmenter\n",
    "\n",
    "\n",
    "def segment_laughter(wav_filename):\n",
    "        #results[file_prefix][\"laughs\"] = segment_laughter(wav_filename)\n",
    "\n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
