{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Extract speech and laughter from audio files\n",
    "\n",
    "For speech recognition we try the [SpeechBrain](https://github.com/speechbrain/speechbrain) project and OpenAI's [Whisper](https://github.com/openai/whisper) model.\n",
    "\n",
    "We also try identifying laughter with [Laughter Detection model](https://github.com/jrgillick/laughter-detection) by jrgillick. \n",
    "\n",
    "This code here is based on prototypes developed at Sage IDEMS hackathon in 2023 \n",
    "https://github.com/chilledgeek/ethical_ai_hackathon_2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_in = \"..\\\\LookitLaughter.test\\\\\"\n",
    "data_dir = \"..\\\\data\\\\1_interim\\\\\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedvideos = utils.getprocessedvideos(data_dir)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Audio extraction with moviepy\n",
    "\n",
    "The first step is simple. We extract the audio from each video and save it as `mp3` or `wav`. We will use the `moviepy` library to do this. \n",
    "This will be helpful for later analysis and regenerating labeled videos with audio.\n",
    "\n",
    "Note that `moviepy` is a wrapper around `ffmpeg` and `ffmpeg` needs to be installed separately. \n",
    "\n",
    "`conda install ffmpeg moviepy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forceaudio = True\n",
    "#output_ext=\"mp3\"\n",
    "output_ext=\"wav\"\n",
    "\n",
    "for index, r in processedvideos.iterrows():\n",
    "    if forceaudio or pd.isnull(r[\"Audio.file\"]):\n",
    "        audiopath = utils.convert_video_to_audio_moviepy(videos_in,r[\"VideoID\"], data_dir, output_ext=output_ext)\n",
    "        r[\"Audio.file\"] = audiopath\n",
    "        r[\"Audio.when\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "        #update this row in processedvideos dataframe\n",
    "        processedvideos.loc[index] = r\n",
    "        \n",
    "\n",
    "utils.saveprocessedvidoes(processedvideos, data_dir)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Speech-to-text - SpeechBrain\n",
    "\n",
    "Let's try [SpeechBrain](https://github.com/speechbrain/speechbrain). It's not on Anaconda so we'll have to install it with pip.\n",
    "\n",
    "`pip install speechbrain`\n",
    "\n",
    "It depends on pytorch and torchaudio. So we'll install them with conda. Note that we need to specify the cuda version. And install a sound processing backend libary. On windows this is `soundfile` on mac\\linux it is `sox`. \n",
    "\n",
    "```\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n",
    "conda install -c conda-forge pysoundfile\n",
    "conda install -c conda-forge ffmpeg\n",
    "```\n",
    "\n",
    "Windows users: If you encounter `Backend not found.` or similiar errors try restarting the PC.   \n",
    "Windows users: If you encounter `: UserWarning: huggingface_hub cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in ` You could try running VSCode as Administrator (Right click icon in start menu look under More >).  See [speechbrain issue 1155](https://github.com/speechbrain/speechbrain/issues/1155) \n",
    "\n",
    "\n",
    "*Initially we tried with Google Cloud Speech to text. But it's a closed model and kept crashing my ipykernel. Then we tried the [Speech Recognition](https://github.com/Uberi/speech_recognition) project to try and access the [Sphinx](https://github.com/cmusphinx/pocketsphinx) speech model. But that pocketsphinx is not maintained on Anaconda any more and compiling from source is a bit beyond me :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import EncoderDecoderASR\n",
    "\n",
    "source=\"speechbrain/asr-crdnn-rnnlm-librispeech\" \n",
    "savedir=\"pretrained_models/asr-crdnn-rnnlm-librispeech\"\n",
    "\n",
    "asr_model = EncoderDecoderASR.from_hparams(\n",
    "    source=source, \n",
    "    savedir=savedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio_file in testset:\n",
    "    results = asr_model.transcribe_file(audio_file)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.convert_audio_to_wav_moviepy(AUDIO_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Speech-to-text using OpenAI Whisper \n",
    "\n",
    "\n",
    "\n",
    "https://analyzingalpha.com/openai-whisper-python-tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whisper_transcribe(audio_file, save_path, saveJSON = True):\n",
    "    result = model.transcribe(audio_file, verbose = True)\n",
    "    if saveJSON:\n",
    "        basename = os.path.basename(audio_file)\n",
    "        filename, ext = os.path.splitext(basename)\n",
    "        jsonfile = f\"{save_path}{filename}.json\"\n",
    "        with open(jsonfile, \"w\") as f:\n",
    "            json.dump(result, f)\n",
    "        return jsonfile, result\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedvideos = utils.getprocessedvideos(data_dir)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, r in processedvideos.iterrows():\n",
    "    if pd.isnull(r[\"Speech.file\"]) and not pd.isnull(r[\"Audio.file\"]):\n",
    "        speechpath, result = whisper_transcribe(r[\"Audio.file\"],save_path=data_dir)\n",
    "        r[\"Speech.file\"] = speechpath\n",
    "        r[\"Speech.when\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "        #update this row in processedvideos dataframe\n",
    "        processedvideos.loc[index] = r\n",
    "        \n",
    "\n",
    "utils.saveprocessedvidoes(processedvideos, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = processedvideos[\"Audio.file\"][0:3].tolist()\n",
    "#just filenames, no path\n",
    "testset = [x.split(\"\\\\\")[-1] for x in testset]\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 TODO - Laughter detection\n",
    "\n",
    "Might not do this here as it seems like we would need to import a lot of supporting code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laughter-detection import laughter_segmenter\n",
    "\n",
    "\n",
    "def segment_laughter(wav_filename):\n",
    "        #results[file_prefix][\"laughs\"] = segment_laughter(wav_filename)\n",
    "\n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
