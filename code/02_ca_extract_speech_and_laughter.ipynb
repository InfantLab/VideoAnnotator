{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Extract speech and laughter from audio files\n",
    "\n",
    "For speech recognition we try the [SpeechBrain](https://github.com/speechbrain/speechbrain) project and OpenAI's [Whisper](https://github.com/openai/whisper) model.\n",
    "\n",
    "We also try identifying laughter with [Laughter Detection model](https://github.com/jrgillick/laughter-detection) by jrgillick. \n",
    "\n",
    "This code here is based on prototypes developed at Sage IDEMS hackathon in 2023 \n",
    "https://github.com/chilledgeek/ethical_ai_hackathon_2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_in = os.path.join(\"..\",\"LookitLaughter.test\")\n",
    "data_out = os.path.join(\"..\", \"data\", \"1_interim\")\n",
    "\n",
    "\n",
    "#videos_in = r\"..\\..\\LookitLaughter.full\"\n",
    "#data_out = r\"..\\..\\LookitLaughter.full.data\\1_interim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedvideos = utils.getProcessedVideos(data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Audio extraction with moviepy\n",
    "\n",
    "The first step is simple. We extract the audio from each video and save it as `mp3` or `wav`. We will use the `moviepy` library to do this. \n",
    "This will be helpful for later analysis and regenerating labeled videos with audio.\n",
    "\n",
    "Note that `moviepy` is a wrapper around `ffmpeg` and `ffmpeg` needs to be installed separately. \n",
    "\n",
    "`conda install ffmpeg moviepy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forceaudio = False\n",
    "#output_ext=\"mp3\"\n",
    "output_ext=\"wav\"\n",
    "\n",
    "for index, r in processedvideos.iterrows():\n",
    "    if forceaudio or pd.isnull(r[\"Audio.file\"]):\n",
    "        audiopath = utils.convert_video_to_audio_moviepy(videos_in,r[\"VideoID\"], data_out, output_ext=output_ext)\n",
    "        r[\"Audio.file\"] = audiopath\n",
    "        r[\"Audio.when\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "        #update this row in processedvideos dataframe\n",
    "        processedvideos.loc[index] = r\n",
    "    else:\n",
    "        print(\"Audio already extracted for video: \", r[\"VideoID\"])\n",
    "        \n",
    "\n",
    "utils.saveProcessedVideos(processedvideos, data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Speech-to-text \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2.1 How not do to it - SpeechBrain Example \n",
    "\n",
    "Let's look at [SpeechBrain](https://github.com/speechbrain/speechbrain). It's not on Anaconda so we'll have to install it with pip.\n",
    "\n",
    "`pip install speechbrain`\n",
    "\n",
    "It depends on pytorch and torchaudio. So we'll install them with conda. Note that we need to specify the cuda version. And install a sound processing backend libary. On windows this is `soundfile` on mac\\linux it is `sox`. \n",
    "\n",
    "```\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n",
    "conda install -c conda-forge pysoundfile\n",
    "conda install -c conda-forge ffmpeg\n",
    "```\n",
    "\n",
    "Windows users: If you encounter `Backend not found.` or similiar errors try restarting the PC.   \n",
    "Windows users: If you encounter `: UserWarning: huggingface_hub cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in ` You could try running VSCode as Administrator (Right click icon in start menu look under More >).  See [speechbrain issue 1155](https://github.com/speechbrain/speechbrain/issues/1155) \n",
    "\n",
    "\n",
    "*Initially we tried with Google Cloud Speech to text. But it's a closed model and kept crashing my ipykernel. Then we tried the [Speech Recognition](https://github.com/Uberi/speech_recognition) project to try and access the [Sphinx](https://github.com/cmusphinx/pocketsphinx) speech model. But that pocketsphinx is not maintained on Anaconda any more and compiling from source is a bit beyond me :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from speechbrain.inference import EncoderDecoderASR\n",
    "import torchaudio\n",
    "    \n",
    "\n",
    "# source=\"speechbrain/asr-crdnn-rnnlm-librispeech\" \n",
    "# savedir=\"pretrained_models/asr-crdnn-rnnlm-librispeech\"\n",
    "source=\"speechbrain/asr-conformer-transformerlm-librispeech\"\n",
    "savedir=\"pretrained_models/asr-transformer-transformerlm-librispeech\"\n",
    "\n",
    "\n",
    "asr_model = EncoderDecoderASR.from_hparams( source=source, savedir=savedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_data = os.path.join(\"..\",\"data\", \"demo\")\n",
    "AUDIO_FILE = os.path.join(demo_data, \"2UWdXP.joke1.rep2.take1.Peekaboo.mp3\")\n",
    "AUDIO_FILE2 = os.path.join(demo_data, \"2UWdXP.joke2.rep1.take1.NomNomNom.mp3\")\n",
    "testset = [AUDIO_FILE, AUDIO_FILE2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the audio file is in a supported format\n",
    "for audio_file in testset:\n",
    "    results = asr_model.transcribe_file(audio_file)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speechbrain not very accurate (with these default settings). Rather than trying to improve it. Let's try the OpenAI Whisper model instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Speech-to-text using OpenAI Whisper \n",
    "\n",
    "There is a free version of the [OpenAI Whisper](https://github.com/openai/whisper) model. It is multilingual (xx languages) and comes in a range of different sizes (and accuracies). We'll try the `base` model. \n",
    "\n",
    "Simple tutorial: https://analyzingalpha.com/openai-whisper-python-tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whisper_transcribe(audio_file, save_path, saveJSON = True):\n",
    "    result = model.transcribe(audio_file, verbose = True)\n",
    "    if saveJSON:\n",
    "        basename = os.path.basename(audio_file)\n",
    "        filename, ext = os.path.splitext(basename)\n",
    "        jsonfile = os.path.join(save_path,filename,\".json\")\n",
    "        with open(jsonfile, \"w\") as f:\n",
    "            json.dump(result, f)\n",
    "        return jsonfile, result\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedvideos = utils.getProcessedVideos(data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, r in processedvideos.iterrows():\n",
    "    if pd.isnull(r[\"Speech.file\"]) and not pd.isnull(r[\"Audio.file\"]):\n",
    "        speechpath, result = whisper_transcribe(r[\"Audio.file\"],save_path=data_out)\n",
    "        r[\"Speech.file\"] = speechpath\n",
    "        r[\"Speech.when\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "        #update this row in processedvideos dataframe\n",
    "        processedvideos.loc[index] = r\n",
    "        \n",
    "utils.saveProcessedVideos(processedvideos, data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 #TODO - Laughter detection\n",
    "\n",
    "We would like to process videos to identifying laughter with [Laughter Detection model](https://github.com/jrgillick/laughter-detection) by jrgillick. However, want to find a simple way to call that from remote project rather than incorporating code into our own project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laughter_segmenter\n",
    "\n",
    "\n",
    "def segment_laughter(wav_filename):\n",
    "        #results[file_prefix][\"laughs\"] = segment_laughter(wav_filename)\n",
    "\n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
