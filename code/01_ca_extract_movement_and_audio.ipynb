{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Extract movement and track positions over time.\n",
    "\n",
    "For each video we use YOLOv8 to extract movement data as a set of body keypoints and use its `model.track` method to track individuals over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Video pose estimation with YOLOv8\n",
    "\n",
    "[YOLOv8](https://github.com/ultralytics/ultralytics) is a commercially maintained version of the YOLO object recognition model. [Yolov7](https://github.com/WongKinYiu/yolov7) introduced pose estimation and v8 improves the models and makes everything much more user-friendly. It can be installed as a package\n",
    "\n",
    "* Pip : `pip install ultralytics`\n",
    "* Conda : `conda install -c conda-forge ultralytics`\n",
    "\n",
    "## 1.2 Object tracking \n",
    "\n",
    "YoloV8 also comes with a `model.track` method. This aims to keep track of all identified objects over the course of a video. Let's make use of that. \n",
    "\n",
    "This is pretty easy instead of calling \n",
    "`results = model(video_path, stream=True)`\n",
    "\n",
    "we can call\n",
    "`results = model.track(video_path, stream=True)`\n",
    "\n",
    "https://docs.ultralytics.com/modes/track/#persisting-tracks-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_in = r\"..\\LookitLaughter.test\"\n",
    "metadata_file = \"_LookitLaughter.xlsx\"\n",
    "data_out = r\"..\\data\\1_interim\"\n",
    "\n",
    "#a couple of files for testing\n",
    "VIDEO_FILE  = os.path.join(videos_in, \"2UWdXP.joke1.rep2.take1.Peekaboo.mp4\")\n",
    "VIDEO_FILE2 = os.path.join(videos_in, \"2UWdXP.joke2.rep1.take1.NomNomNom.mp4\")\n",
    "\n",
    "testset = [VIDEO_FILE, VIDEO_FILE2] \n",
    "\n",
    "#get metadata from excel file\n",
    "metadata = pd.read_excel(os.path.join(videos_in, metadata_file))\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get yolo model with pose estimation\n",
    "model = YOLO('yolov8n-pose.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedvideos = utils.getprocessedvideos(data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through each row of metadata and\n",
    "#process all related videos\n",
    "forcemetadata = False\n",
    "forceprocess = True\n",
    "tracking = True\n",
    "\n",
    "for index, mrow in metadata.iterrows():\n",
    "    #get VIDEOID from first column of metadata\n",
    "    videoname = mrow[\"VideoID\"]\n",
    "    stemname = os.path.splitext(videoname)[0]\n",
    "    print(f\"video:{videoname}\")\n",
    "\n",
    "    #check we want to refill metadata or this video is not already in processedvideos dataframe\n",
    "    if forcemetadata or videoname not in processedvideos[\"VideoID\"].values: \n",
    "        #use cv2 to get fps and other video info to add to dataframe\n",
    "        cap = cv2.VideoCapture(os.path.join(videos_in,videoname))    \n",
    "        if (cap.isOpened()== False):\n",
    "            print(\"Error opening video stream or file\")\n",
    "            continue\n",
    "        else:\n",
    "            #add row to processedvideos dataframe\n",
    "            row = {\"VideoID\":videoname,\n",
    "                \"ChildID\":mrow[\"ChildID\"],\n",
    "                \"JokeType\":mrow[\"JokeType\"],\n",
    "                \"JokeNum\":mrow[\"JokeNum\"],\n",
    "                \"JokeRep\":mrow[\"JokeRep\"],\n",
    "                \"JokeTake\":mrow[\"JokeTake\"],\n",
    "                \"HowFunny\":mrow[\"HowFunny\"],\n",
    "                \"LaughYesNo\":mrow[\"LaughYesNo\"],\n",
    "                \"Frames\":cap.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "                \"FPS\":cap.get(cv2.CAP_PROP_FPS) , \n",
    "                \"Width\":cap.get(cv2.CAP_PROP_FRAME_WIDTH), \n",
    "                \"Height\":cap.get(cv2.CAP_PROP_FRAME_HEIGHT), \n",
    "                \"Duration\":cap.get(cv2.CAP_PROP_FRAME_COUNT)/cap.get(cv2.CAP_PROP_FPS)\n",
    "                }\n",
    "            cap.release()\n",
    "            print(f\"Adding video info: {row}\")\n",
    "            newrow = pd.DataFrame(row, index=[0])\n",
    "            processedvideos = pd.concat([processedvideos,newrow], ignore_index=True)\n",
    "\n",
    "    #select the dataframe row for this video \n",
    "    row = processedvideos.loc[processedvideos[\"VideoID\"] == videoname]\n",
    "    #check if we have already processed this video is keypoints is not nan\n",
    "    if row.empty:\n",
    "        print(f\"error: no row for {videoname}\")\n",
    "        continue\n",
    "    elif not forceprocess and not pd.isnull(row[\"Keypoints.file\"].values[0]):\n",
    "        print(row[\"Keypoints.file\"].values[0]  )\n",
    "        print(f\"already processed {videoname}\")\n",
    "        continue\n",
    "    else:\n",
    "        #use ultralytics YOLO to get keypoints\n",
    "        keypointsdf =utils.videotokeypoints(model, os.path.join(videos_in,videoname) , track = True)\n",
    "        #save keypointsdf as csv    \n",
    "        keypointspath = data_out + \"\\\\\" + stemname + \".csv\"\n",
    "        keypointsdf.to_csv(keypointspath)\n",
    "        row[\"Keypoints.file\"] = keypointspath\n",
    "        row[\"Keypoints.when\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "        #update this row in processedvideos dataframe\n",
    "        processedvideos.loc[processedvideos[\"VideoID\"] == videoname] = row\n",
    "    \n",
    "\n",
    "    #update processedvideos excel file\n",
    "    processedvideos.to_excel(data_out + \"\\\\processedvideos.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's test it out first\n",
    "\n",
    "# Open the video file\n",
    "video_path = VIDEO_FILE\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        results = model.track(frame, persist=True)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's have a look at the results object for video \n",
    "\n",
    "results = model.track(VIDEO_FILE,stream=True)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Results' object has no attribute 'print'. See valid attributes below.\n\n    A class for storing and manipulating inference results.\n\n    Args:\n        orig_img (numpy.ndarray): The original image as a numpy array.\n        path (str): The path to the image file.\n        names (dict): A dictionary of class names.\n        boxes (torch.tensor, optional): A 2D tensor of bounding box coordinates for each detection.\n        masks (torch.tensor, optional): A 3D tensor of detection masks, where each mask is a binary image.\n        probs (torch.tensor, optional): A 1D tensor of probabilities of each class for classification task.\n        keypoints (List[List[float]], optional): A list of detected keypoints for each object.\n\n    Attributes:\n        orig_img (numpy.ndarray): The original image as a numpy array.\n        orig_shape (tuple): The original image shape in (height, width) format.\n        boxes (Boxes, optional): A Boxes object containing the detection bounding boxes.\n        masks (Masks, optional): A Masks object containing the detection masks.\n        probs (Probs, optional): A Probs object containing probabilities of each class for classification task.\n        keypoints (Keypoints, optional): A Keypoints object containing detected keypoints for each object.\n        speed (dict): A dictionary of preprocess, inference, and postprocess speeds in milliseconds per image.\n        names (dict): A dictionary of class names.\n        path (str): The path to the image file.\n        _keys (tuple): A tuple of attribute names for non-empty attributes.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cas\\OneDrive\\LegoGPI\\babyjokes\\notebooks\\01_ca_extract_movement_and_audio.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cas/OneDrive/LegoGPI/babyjokes/notebooks/01_ca_extract_movement_and_audio.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m frame \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cas/OneDrive/LegoGPI/babyjokes/notebooks/01_ca_extract_movement_and_audio.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m results:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cas/OneDrive/LegoGPI/babyjokes/notebooks/01_ca_extract_movement_and_audio.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     r\u001b[39m.\u001b[39mprint()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cas/OneDrive/LegoGPI/babyjokes/notebooks/01_ca_extract_movement_and_audio.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     frame \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cas/OneDrive/LegoGPI/babyjokes/notebooks/01_ca_extract_movement_and_audio.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m frame \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\cas\\Anaconda3\\envs\\babyjokes\\Lib\\site-packages\\ultralytics\\utils\\__init__.py:153\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[0;32m    152\u001b[0m name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m--> 153\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. See valid attributes below.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__doc__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Results' object has no attribute 'print'. See valid attributes below.\n\n    A class for storing and manipulating inference results.\n\n    Args:\n        orig_img (numpy.ndarray): The original image as a numpy array.\n        path (str): The path to the image file.\n        names (dict): A dictionary of class names.\n        boxes (torch.tensor, optional): A 2D tensor of bounding box coordinates for each detection.\n        masks (torch.tensor, optional): A 3D tensor of detection masks, where each mask is a binary image.\n        probs (torch.tensor, optional): A 1D tensor of probabilities of each class for classification task.\n        keypoints (List[List[float]], optional): A list of detected keypoints for each object.\n\n    Attributes:\n        orig_img (numpy.ndarray): The original image as a numpy array.\n        orig_shape (tuple): The original image shape in (height, width) format.\n        boxes (Boxes, optional): A Boxes object containing the detection bounding boxes.\n        masks (Masks, optional): A Masks object containing the detection masks.\n        probs (Probs, optional): A Probs object containing probabilities of each class for classification task.\n        keypoints (Keypoints, optional): A Keypoints object containing detected keypoints for each object.\n        speed (dict): A dictionary of preprocess, inference, and postprocess speeds in milliseconds per image.\n        names (dict): A dictionary of class names.\n        path (str): The path to the image file.\n        _keys (tuple): A tuple of attribute names for non-empty attributes.\n    "
     ]
    }
   ],
   "source": [
    "frame = 0\n",
    "for r in results:\n",
    "    r.\n",
    "    frame += 1\n",
    "    if frame == 5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyjokes-hnIM0ZSK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
