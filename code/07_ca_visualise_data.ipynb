{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Visualise activity in a video.\n",
    "\n",
    "We have extracted all the features we plan to use. Overlaying these on the video was useful.\n",
    "But watching annotated videos is inefficient and not always informative.. \n",
    "\n",
    "To help with understanding we build a few tools that let's see at a glance what happens over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Plot movements over time. \n",
    "\n",
    "In each frame let's find the `centre of gravity` for each person (the average of all the high-confidence marker points). This is handy for time series visualisation. For example plotting the cog.x for each person over time shows how they move closer and further from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import calcs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_in = r\"..\\LookitLaughter.test\"\n",
    "data_out = r\"..\\data\\1_interim\"\n",
    "videos_out = r\"..\\data\\2_final\"\n",
    "\n",
    "#a couple of files for testing\n",
    "VIDEO_FILE  = os.path.join(videos_in, \"2UWdXP.joke1.rep2.take1.Peekaboo.mp4\")\n",
    "VIDEO_FILE2 = os.path.join(videos_in, \"2UWdXP.joke2.rep1.take1.NomNomNom.mp4\")\n",
    "AUDIO_FILE = os.path.join(data_out, \"2UWdXP.joke1.rep2.take1.Peekaboo.wav\")\n",
    "SPEECH_FILE = os.path.join(data_out, \"2UWdXP.joke1.rep2.take1.Peekaboo.json\")\n",
    "\n",
    "testset = [VIDEO_FILE, VIDEO_FILE2] \n",
    "\n",
    "processedvideos = utils.getprocessedvideos(data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the keypoint data and calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = utils.readKeyPointsFromCSV(processedvideos,VIDEO_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this bit of pandas magic calculates average x and y for all the rows.\n",
    "\n",
    "keypoints[[\"cogx\",\"cogy\"]] = keypoints.apply(lambda row: calcs.rowcogs(row.iloc[8:59]), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child = keypoints[keypoints[\"person\"]==\"child\"]\n",
    "adult = keypoints[keypoints[\"person\"]==\"adult\"]\n",
    "\n",
    "#a plot of child's centre of gravity frame by frame\n",
    "childplot = plt.plot(child[\"frame\"], child[\"cogx\"], c=\"red\", alpha=0.5)\n",
    "## add line of adult's centre of gravity\n",
    "adultplot = plt.plot(adult[\"frame\"], adult[\"cogx\"], c=\"blue\", alpha=0.5)\n",
    "#add legend\n",
    "plt.legend(['child', 'adult'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out of curiousity, let's plot the location of the adult and child's nose\n",
    "\n",
    "childnoseplot = plt.plot(child[\"frame\"], child[\"nose.x\"], c=\"red\", alpha=0.5)\n",
    "adultnoseplot = plt.plot(adult[\"frame\"], adult[\"nose.x\"], c=\"blue\", alpha=0.5)\n",
    "plt.legend(['child', 'adult'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Use Voxel51 and PytorchVideo for examining videos\n",
    "\n",
    "Voxel51 seems to be a useful tool for looking at training data (and trained predictions).\n",
    "\n",
    "Let's start with the minimal implementation. Just viewing videos.\n",
    "\n",
    "https://docs.voxel51.com/user_guide/dataset_creation/index.html\n",
    "\n",
    "We will use our annotated videos from step 6.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.delete_datasets(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Create a dataset from a directory of videos\n",
    "dataset = fo.Dataset.from_videos_dir(\"../data/2_final\")\n",
    "dataset.ensure_frames()\n",
    "\n",
    "dataset.name = 'LookitLaughter.test'\n",
    "\n",
    "\n",
    "dataset.add_sample_field(\"JokeType\", fo.StringField, description=\"What joke is being told?\")\n",
    "dataset.add_sample_field(\"HowFunny\", fo.StringField, description=\"How funny is the joke?\")\n",
    "dataset.add_sample_field(\"LaughYesNo\",  fo.BooleanField, description=\"Did the child laugh?\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can add our metadata classifications. Recalling that each video demos one joke type `[Peekaboo,TearingPaper,NomNomNom,ThatsNotAHat,ThatsNotACat]` and has rating of how funny the baby found it `[Not Funny, Slightly Funny, Funny, Extremely Funny]` and whether they laughed `[Yes, No]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoIDfromannotated(filepath):\n",
    "    #strip off the path and the annotation and filetype and add back on filetype\n",
    "    #eg. \n",
    "    #\"C:\\Users\\caspar\\OneDrive\\LegoGPI\\babyjokes\\data\\2_final\\2UWdXP.joke1.rep2.take1.Peekaboo.annotated.mp4\"\n",
    "    #becomes\n",
    "    #2UWdXP.joke1.rep2.take1.Peekaboo.mp4\n",
    "    \n",
    "    #split the path and the filename\n",
    "    path, filename = os.path.split(filepath)    \n",
    "    #split the filename and the extension\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    #split the name and the annotation\n",
    "    name, annot = name.split(\".annotated\")\n",
    "    #put it all back together\n",
    "    return name + ext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the joke type, how funny and laugh yes/no for each sample in the dataset\n",
    "for sample in dataset:\n",
    "    videoname = videoIDfromannotated(sample.filepath)\n",
    "    row = processedvideos[processedvideos[\"VideoID\"]==videoname]\n",
    "    sample[\"VideoID\"]  = row[\"VideoID\"].values[0]\n",
    "    sample[\"JokeType\"]  = row[\"JokeType\"].values[0]\n",
    "    sample[\"HowFunny\"]  = row[\"HowFunny\"].values[0]\n",
    "    sample[\"LaughYesNo\"]  = (row[\"LaughYesNo\"].values[0] == \"Yes\")\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the frame by frame annotations directly onto the videos inside fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2.1 #TODO - Create annotations natively in Voxel51\n",
    "\n",
    "At present we start from our own annoated videos from step 6. But in theory we could start from scratch in Voxel51. However, it looks like the Voxel51 annotation isn't great at the moment. In particular it does not seeem not possible to annotate a video with a time series of keypoint data. (There are functions for keypoint data but they are not well documented and don't seem to work with video.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start with people bounding boxes\n",
    "\n",
    "\n",
    "for sample in dataset:\n",
    "    #retrieve people bounding boxes from the keypoints file\n",
    "    keypoints = utils.readKeyPointsFromCSV(processedvideos,sample.filepath,normed= True)    \n",
    "\n",
    "    for index, row in keypoints.iterrows():\n",
    "        framenumber = row[\"frame\"] + 1\n",
    "        person = row[\"person\"]\n",
    "        bbox = [row[\"bbox.x1\"], row[\"bbox.y1\"], row[\"bbox.x2\"], row[\"bbox.y2\"]]\n",
    "        bbox51 = calcs.xyxy2ltwh(bbox)\n",
    "        if sample.frames[framenumber]:\n",
    "            frame = sample.frames[framenumber]\n",
    "        else:\n",
    "            frame = fo.Frame()\n",
    "        frame[person] = fo.Detection(label=person, bounding_box=bbox51)\n",
    "        sample.frames[framenumber] = frame\n",
    "        #frame[person + \"KeyPoints\"] =  \n",
    "        #fo.KeypointSkeleton()\n",
    "        sample.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyjokes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
