{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Visualise activity in a video.\n",
    "\n",
    "We have extracted all the features we plan to use. Overlaying these on the video was useful.\n",
    "But watching annotated videos is inefficient and not always informative.. \n",
    "\n",
    "To help with understanding we build a few tools that let's see at a glance what happens over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Plot movements over time. \n",
    "\n",
    "In each frame let's find the `centre of gravity` for each person (the average of all the high-confidence marker points). This is handy for time series visualisation. For example plotting the cog.x for each person over time shows how they move closer and further from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import calcs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_in = r\"..\\LookitLaughter.test\"\n",
    "data_out = r\"..\\data\\1_interim\"\n",
    "videos_out = r\"..\\data\\2_final\"\n",
    "\n",
    "#a couple of files for testing\n",
    "VIDEO_FILE  = os.path.join(videos_in, \"2UWdXP.joke1.rep2.take1.Peekaboo.mp4\")\n",
    "VIDEO_FILE2 = os.path.join(videos_in, \"2UWdXP.joke2.rep1.take1.NomNomNom.mp4\")\n",
    "AUDIO_FILE = os.path.join(data_out, \"2UWdXP.joke1.rep2.take1.Peekaboo.wav\")\n",
    "SPEECH_FILE = os.path.join(data_out, \"2UWdXP.joke1.rep2.take1.Peekaboo.json\")\n",
    "\n",
    "testset = [VIDEO_FILE, VIDEO_FILE2] \n",
    "\n",
    "processedvideos = utils.getprocessedvideos(data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = utils.readKeyPointsFromCSV(processedvideos,VIDEO_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this bit of pandas magic calculates average x and y for all the rows.\n",
    "\n",
    "keypoints[[\"cogx\",\"cogy\"]] = keypoints.apply(lambda row: calcs.rowcogs(row.iloc[8:59]), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child = keypoints[keypoints[\"person\"]==\"child\"]\n",
    "adult = keypoints[keypoints[\"person\"]==\"adult\"]\n",
    "\n",
    "#a plot of child's centre of gravity frame by frame\n",
    "childplot = plt.plot(child[\"frame\"], child[\"cogx\"], c=\"red\", alpha=0.5)\n",
    "## add line of adult's centre of gravity\n",
    "adultplot = plt.plot(adult[\"frame\"], adult[\"cogx\"], c=\"blue\", alpha=0.5)\n",
    "#add legend\n",
    "plt.legend(['child', 'adult'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out of curiousity, let's plot the location of the adult and child's nose\n",
    "\n",
    "childnoseplot = plt.plot(child[\"frame\"], child[\"nose.x\"], c=\"red\", alpha=0.5)\n",
    "adultnoseplot = plt.plot(adult[\"frame\"], adult[\"nose.x\"], c=\"blue\", alpha=0.5)\n",
    "plt.legend(['child', 'adult'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 TODO Use Voxel51 and PytorchVideo for examining videos\n",
    "\n",
    "Voxel51 seems to be a useful tool for looking at training data (and trained predictions).\n",
    "\n",
    "Let's start with the minimal \n",
    "\n",
    "https://docs.voxel51.com/user_guide/dataset_creation/index.html\n",
    "\n",
    "\n",
    "[fiftyone-examples/examples/pytorchvideo_tutorial.ipynb at master Â· voxel51/fiftyone-examples (github.com)](https://github.com/voxel51/fiftyone-examples/blob/master/examples/pytorchvideo_tutorial.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "\n",
    "# Create a dataset from a directory of videos\n",
    "dataset = fo.Dataset.from_videos_dir(\"../LookitLaughter.test\")\n",
    "\n",
    "# Launch the App\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.3 Let's normalise all the coordinates\n",
    "\n",
    "Let's convert all coordinates to values between 0 and 1. But let's preserve aspect ratios. \n",
    "So divide all x and y coords by the max of (height, width). This also has the advantage of being easier to code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyjokes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
