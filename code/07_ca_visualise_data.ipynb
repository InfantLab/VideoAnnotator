{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Visualise activity in a video.\n",
    "\n",
    "We have extracted all the features we plan to use. Overlaying these on the video was useful.\n",
    "But watching annotated videos is inefficient and not always informative.. \n",
    "\n",
    "To help with understanding we build a few tools that let's see at a glance what happens over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import calcs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing processedvideos.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VideoID</th>\n",
       "      <th>ChildID</th>\n",
       "      <th>JokeType</th>\n",
       "      <th>JokeNum</th>\n",
       "      <th>JokeRep</th>\n",
       "      <th>JokeTake</th>\n",
       "      <th>HowFunny</th>\n",
       "      <th>LaughYesNo</th>\n",
       "      <th>Frames</th>\n",
       "      <th>FPS</th>\n",
       "      <th>...</th>\n",
       "      <th>Speech.file</th>\n",
       "      <th>Speech.when</th>\n",
       "      <th>Objects.file</th>\n",
       "      <th>Objects.when</th>\n",
       "      <th>Understand.file</th>\n",
       "      <th>Understand.when</th>\n",
       "      <th>Faces.normed</th>\n",
       "      <th>Keypoints.normed</th>\n",
       "      <th>annotatedVideo</th>\n",
       "      <th>annotated.when</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2UWdXP.joke1.rep2.take1.Peekaboo.mp4</td>\n",
       "      <td>2UWdXP</td>\n",
       "      <td>Peekaboo</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Slightly funny</td>\n",
       "      <td>No</td>\n",
       "      <td>217</td>\n",
       "      <td>14.298910</td>\n",
       "      <td>...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke1.rep2.take1.Peek...</td>\n",
       "      <td>2023-09-20 16:58:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke1.rep2.take1.Peek...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke1.rep2.take1.Peek...</td>\n",
       "      <td>../data/2_final/2UWdXP.joke1.rep2.take1.Peekab...</td>\n",
       "      <td>2024-02-16 11:03:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2UWdXP.joke1.rep3.take1.Peekaboo.mp4</td>\n",
       "      <td>2UWdXP</td>\n",
       "      <td>Peekaboo</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Slightly funny</td>\n",
       "      <td>No</td>\n",
       "      <td>152</td>\n",
       "      <td>14.359089</td>\n",
       "      <td>...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke1.rep3.take1.Peek...</td>\n",
       "      <td>2023-09-20 16:58:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke1.rep3.take1.Peek...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke1.rep3.take1.Peek...</td>\n",
       "      <td>../data/2_final/2UWdXP.joke1.rep3.take1.Peekab...</td>\n",
       "      <td>2024-02-16 11:03:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2UWdXP.joke2.rep1.take1.NomNomNom.mp4</td>\n",
       "      <td>2UWdXP</td>\n",
       "      <td>NomNomNom</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Funny</td>\n",
       "      <td>No</td>\n",
       "      <td>95</td>\n",
       "      <td>13.241315</td>\n",
       "      <td>...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep1.take1.NomN...</td>\n",
       "      <td>2023-09-20 16:58:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep1.take1.NomN...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep1.take1.NomN...</td>\n",
       "      <td>../data/2_final/2UWdXP.joke2.rep1.take1.NomNom...</td>\n",
       "      <td>2024-02-16 11:03:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2UWdXP.joke2.rep2.take1.NomNomNom.mp4</td>\n",
       "      <td>2UWdXP</td>\n",
       "      <td>NomNomNom</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Slightly funny</td>\n",
       "      <td>No</td>\n",
       "      <td>97</td>\n",
       "      <td>14.213813</td>\n",
       "      <td>...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep2.take1.NomN...</td>\n",
       "      <td>2023-09-20 16:58:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep2.take1.NomN...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep2.take1.NomN...</td>\n",
       "      <td>../data/2_final/2UWdXP.joke2.rep2.take1.NomNom...</td>\n",
       "      <td>2024-02-16 11:03:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2UWdXP.joke2.rep3.take1.NomNomNom.mp4</td>\n",
       "      <td>2UWdXP</td>\n",
       "      <td>NomNomNom</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Slightly funny</td>\n",
       "      <td>No</td>\n",
       "      <td>133</td>\n",
       "      <td>14.223092</td>\n",
       "      <td>...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep3.take1.NomN...</td>\n",
       "      <td>2023-09-20 16:58:48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep3.take1.NomN...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep3.take1.NomN...</td>\n",
       "      <td>../data/2_final/2UWdXP.joke2.rep3.take1.NomNom...</td>\n",
       "      <td>2024-02-16 11:03:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 VideoID ChildID   JokeType  JokeNum  JokeRep  \\\n",
       "0   2UWdXP.joke1.rep2.take1.Peekaboo.mp4  2UWdXP   Peekaboo        1        2   \n",
       "1   2UWdXP.joke1.rep3.take1.Peekaboo.mp4  2UWdXP   Peekaboo        1        3   \n",
       "2  2UWdXP.joke2.rep1.take1.NomNomNom.mp4  2UWdXP  NomNomNom        2        1   \n",
       "3  2UWdXP.joke2.rep2.take1.NomNomNom.mp4  2UWdXP  NomNomNom        2        2   \n",
       "4  2UWdXP.joke2.rep3.take1.NomNomNom.mp4  2UWdXP  NomNomNom        2        3   \n",
       "\n",
       "   JokeTake        HowFunny LaughYesNo  Frames        FPS  ...  \\\n",
       "0         1  Slightly funny         No     217  14.298910  ...   \n",
       "1         1  Slightly funny         No     152  14.359089  ...   \n",
       "2         1           Funny         No      95  13.241315  ...   \n",
       "3         1  Slightly funny         No      97  14.213813  ...   \n",
       "4         1  Slightly funny         No     133  14.223092  ...   \n",
       "\n",
       "                                         Speech.file          Speech.when  \\\n",
       "0  ../data/1_interim/2UWdXP.joke1.rep2.take1.Peek...  2023-09-20 16:58:38   \n",
       "1  ../data/1_interim/2UWdXP.joke1.rep3.take1.Peek...  2023-09-20 16:58:39   \n",
       "2  ../data/1_interim/2UWdXP.joke2.rep1.take1.NomN...  2023-09-20 16:58:40   \n",
       "3  ../data/1_interim/2UWdXP.joke2.rep2.take1.NomN...  2023-09-20 16:58:40   \n",
       "4  ../data/1_interim/2UWdXP.joke2.rep3.take1.NomN...  2023-09-20 16:58:48   \n",
       "\n",
       "   Objects.file Objects.when Understand.file Understand.when  \\\n",
       "0           NaN          NaN             NaN             NaN   \n",
       "1           NaN          NaN             NaN             NaN   \n",
       "2           NaN          NaN             NaN             NaN   \n",
       "3           NaN          NaN             NaN             NaN   \n",
       "4           NaN          NaN             NaN             NaN   \n",
       "\n",
       "                                        Faces.normed  \\\n",
       "0  ../data/1_interim/2UWdXP.joke1.rep2.take1.Peek...   \n",
       "1  ../data/1_interim/2UWdXP.joke1.rep3.take1.Peek...   \n",
       "2  ../data/1_interim/2UWdXP.joke2.rep1.take1.NomN...   \n",
       "3  ../data/1_interim/2UWdXP.joke2.rep2.take1.NomN...   \n",
       "4  ../data/1_interim/2UWdXP.joke2.rep3.take1.NomN...   \n",
       "\n",
       "                                    Keypoints.normed  \\\n",
       "0  ../data/1_interim/2UWdXP.joke1.rep2.take1.Peek...   \n",
       "1  ../data/1_interim/2UWdXP.joke1.rep3.take1.Peek...   \n",
       "2  ../data/1_interim/2UWdXP.joke2.rep1.take1.NomN...   \n",
       "3  ../data/1_interim/2UWdXP.joke2.rep2.take1.NomN...   \n",
       "4  ../data/1_interim/2UWdXP.joke2.rep3.take1.NomN...   \n",
       "\n",
       "                                      annotatedVideo       annotated.when  \n",
       "0  ../data/2_final/2UWdXP.joke1.rep2.take1.Peekab...  2024-02-16 11:03:50  \n",
       "1  ../data/2_final/2UWdXP.joke1.rep3.take1.Peekab...  2024-02-16 11:03:51  \n",
       "2  ../data/2_final/2UWdXP.joke2.rep1.take1.NomNom...  2024-02-16 11:03:52  \n",
       "3  ../data/2_final/2UWdXP.joke2.rep2.take1.NomNom...  2024-02-16 11:03:53  \n",
       "4  ../data/2_final/2UWdXP.joke2.rep3.take1.NomNom...  2024-02-16 11:03:54  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_in = os.path.join(\"..\",\"LookitLaughter.test\")\n",
    "demo_data = os.path.join(\"..\",\"data\", \"demo\")\n",
    "temp_out = os.path.join(\"..\",\"data\",\"0_temp\")\n",
    "data_out = os.path.join(\"..\",\"data\",\"1_interim\")\n",
    "videos_out = os.path.join(\"..\",\"data\",\"2_final\")\n",
    "\n",
    "metadata_file = \"_LookitLaughter.xlsx\"\n",
    "\n",
    "#a couple of files for testing\n",
    "VIDEO_FILE  = os.path.join(videos_in, \"2UWdXP.joke1.rep2.take1.Peekaboo.mp4\")\n",
    "VIDEO_FILE2 = os.path.join(videos_in, \"2UWdXP.joke2.rep1.take1.NomNomNom.mp4\")\n",
    "AUDIO_FILE = os.path.join(data_out, \"2UWdXP.joke1.rep2.take1.Peekaboo.wav\")\n",
    "SPEECH_FILE = os.path.join(data_out, \"2UWdXP.joke1.rep2.take1.Peekaboo.json\")\n",
    "\n",
    "testset = [VIDEO_FILE, VIDEO_FILE2] \n",
    "\n",
    "processedvideos = utils.getprocessedvideos(data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Use Voxel51 and PytorchVideo for examining videos\n",
    "\n",
    "Voxel51 seems to be a useful tool for looking at training data (and trained predictions).\n",
    "\n",
    "Let's start with the minimal implementation. Just viewing videos.\n",
    "\n",
    "https://docs.voxel51.com/user_guide/dataset_creation/index.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 Is dataset already created?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LookitLaughter.test']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo.list_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 Populate a FiftyOne dataset with our videos and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.delete_datasets(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 54/54 [44.9ms elapsed, 0s remaining, 1.2K samples/s]   \n",
      "Computing metadata...\n",
      " 100% |███████████████████| 54/54 [1.1s elapsed, 0s remaining, 48.5 samples/s]         \n"
     ]
    }
   ],
   "source": [
    "# Create a dataset from a directory of videos\n",
    "dataset = fo.Dataset.from_videos_dir(\"../LookitLaughter.test\")\n",
    "dataset.ensure_frames()\n",
    "\n",
    "dataset.name = 'LookitLaughter.test'\n",
    "\n",
    "\n",
    "dataset.add_sample_field(\"JokeType\", fo.StringField, description=\"What joke is being told?\")\n",
    "dataset.add_sample_field(\"HowFunny\", fo.StringField, description=\"How funny is the joke?\")\n",
    "dataset.add_sample_field(\"LaughYesNo\",  fo.BooleanField, description=\"Did the child laugh?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can add our metadata classifications. Recalling that each video demos one joke type `[Peekaboo,TearingPaper,NomNomNom,ThatsNotAHat,ThatsNotACat]` and has rating of how funny the baby found it `[Not Funny, Slightly Funny, Funny, Extremely Funny]` and whether they laughed `[Yes, No]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the joke type, how funny and laugh yes/no for each sample in the dataset\n",
    "for sample in dataset:\n",
    "    #split the filepath to get the video name, system independent\n",
    "    videoname = os.path.basename(sample.filepath)\n",
    "    row = processedvideos[processedvideos[\"VideoID\"]==videoname]\n",
    "    sample[\"VideoID\"]  = row[\"VideoID\"].values[0]\n",
    "    sample[\"JokeType\"]  = row[\"JokeType\"].values[0]\n",
    "    sample[\"HowFunny\"]  = row[\"HowFunny\"].values[0]\n",
    "    sample[\"LaughYesNo\"]  = (row[\"LaughYesNo\"].values[0] == \"Yes\")\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the frame by frame annotations directly onto the videos inside fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start with people bounding boxes\n",
    "\n",
    "for sample in dataset:\n",
    "    #retrieve people bounding boxes from the keypoints file\n",
    "    keypoints = utils.readKeyPointsFromCSV(processedvideos,sample.filepath,normed= True)    \n",
    "\n",
    "    for index, row in keypoints.iterrows():\n",
    "        framenumber = row[\"frame\"] + 1\n",
    "        person = row[\"person\"]\n",
    "        bbox = [row[\"bbox.x1\"], row[\"bbox.y1\"], row[\"bbox.x2\"], row[\"bbox.y2\"]]\n",
    "        bbox51 = calcs.xyxy2ltwh(bbox)\n",
    "        if sample.frames[framenumber]:\n",
    "            frame = sample.frames[framenumber]\n",
    "        else:\n",
    "            frame = fo.Frame()\n",
    "        frame[person] = fo.Detection(label=person, bounding_box=bbox51)\n",
    "        sample.frames[framenumber] = frame\n",
    "        #TODO fiftyone add keypoints not well documented and i can't get it to work. \n",
    "        #frame[person + \"KeyPoints\"] =  \n",
    "        #fo.KeypointSkeleton()\n",
    "        #TODO can't see how to add timesynced captions either!\n",
    "\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 View dataset in Voxel51 GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://0.0.0.0:5151/?notebook=True&subscription=26992910-81f9-42db-a9f3-6b9da1cd1477\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f26e7ff3f70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = fo.launch_app(dataset, address=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(session.selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No samples selected. Click the checkbox in the top left of each video to select it.\n"
     ]
    }
   ],
   "source": [
    "#session.selected contains the indices of the dataset samples clicked on in the UI.\n",
    "if len(session.selected) == 0:\n",
    "    print(\"No samples selected. Click the checkbox in the top left of each video to select it.\")\n",
    "else:\n",
    "    print(dataset[session.selected[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Draw annotated timeline for a select video \n",
    "\n",
    "A group of visualisations to see what happens in a video. \n",
    "\n",
    "In each frame let's find the `centre of gravity` for each person (the average of all the high-confidence marker points). This is handy for time series visualisation. For example plotting the cog.x for each person over time shows how they move closer and further from each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the keypoint data and calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = utils.getVideoProperty(processedvideos, VIDEO_FILE2, \"FPS\")\n",
    "xmax = keypoints[\"frame\"].max()\n",
    "\n",
    "emotionColors = {\"angry\":{\"color\":\"red\",\"arousal\":0.7,\"valence\":-0.7},\n",
    "                 \"afraid\":{\"color\":\"orange\",\"arousal\":0.8,\"valence\":-0.2},\n",
    "                 \"happy\":{\"color\":\"yellow\",\"arousal\":0.2,\"valence\":0.9},\n",
    "                 \"neutral\":{\"color\":\"gray\",\"arousal\":0,\"valence\":0},\n",
    "                 \"sad\":{\"color\":\"blue\",\"arousal\":-0.2,\"valence\":-0.9}}\n",
    "who = [\"child\", \"adult\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCoGrav = True\n",
    "plotSpeech = True\n",
    "plotEmotions = True\n",
    "\n",
    "#numerical sum of boolean flags\n",
    "subplots = sum([plotCoGrav, plotSpeech, plotEmotions])\n",
    "\n",
    "if len(session.selected) == 0:\n",
    "    print(\"No video selected\")\n",
    "    exit()\n",
    "\n",
    "VideoID = dataset[session.selected[0]][\"VideoID\"]\n",
    "keypoints = utils.readKeyPointsFromCSV(processedvideos,VideoID)\n",
    "\n",
    "#this bit of pandas magic calculates average x and y for all the rows.\n",
    "keypoints[[\"cogx\",\"cogy\"]] = keypoints.apply(lambda row: calcs.rowcogs(row.iloc[8:59]), axis=1, result_type='expand')\n",
    "\n",
    "#going to add a subplot foe each of the above flags\n",
    "plt.figure(figsize=(20, 5*subplots))\n",
    "plt.suptitle(\"Video Time Line Plots\")\n",
    "pltidx = 0\n",
    "if plotCoGrav:\n",
    "    ax = plt.subplot(subplots, 1, pltidx + 1)\n",
    "    pltidx += 1\n",
    "    ax.set_xlabel(\"Time (seconds)\")\n",
    "    ax.set_ylabel(\"Horizontal Position\")\n",
    "    ax.set_xlim(0, child[\"frame\"].max()/FPS)\n",
    "    child = keypoints[keypoints[\"person\"]==\"child\"]\n",
    "    adult = keypoints[keypoints[\"person\"]==\"adult\"]\n",
    "    #a plot of child's centre of gravity frame by frame\n",
    "    childplot = ax.plot(child[\"frame\"], child[\"cogx\"], c=\"red\", alpha=0.5)\n",
    "    ## add line of adult's centre of gravity\n",
    "    adultplot = ax.plot(adult[\"frame\"], adult[\"cogx\"], c=\"blue\", alpha=0.5)\n",
    "    #add legend\n",
    "    ax.legend(['child', 'adult'], loc='upper left')\n",
    "\n",
    "if plotSpeech:\n",
    "    ax2 = plt.subplot(subplots, 1, pltidx + 1)\n",
    "    pltidx += 1\n",
    "    ax2.set_xlabel(\"Time (seconds)\")\n",
    "    ax2.set_ylabel(\"Identified Speech\")\n",
    "    speechjson = utils.getSpeechData(processedvideos,VideoID)\n",
    "    if speechjson is not None:\n",
    "        nsegs = len(speechjson[\"segments\"])\n",
    "        ax2.set_xlim(0, child[\"frame\"].max()/FPS)\n",
    "        ax2.set_ylim(0, nsegs)\n",
    "        #let's plot the speech segments as boxes\n",
    "        #label each one with the text\n",
    "        for idx, seg in enumerate(speechjson[\"segments\"]):\n",
    "            # #rectangle with the start and end times as x coordinates and nsegs - idx as y coordinates\n",
    "            #fill the rectangle\n",
    "            ax2.fill([seg[\"start\"], seg[\"end\"], seg[\"end\"], seg[\"start\"]], [nsegs - idx - 1, nsegs - idx - 1, nsegs - idx, nsegs - idx], 'r', alpha=0.5)\n",
    "            ax2.text(seg[\"start\"], nsegs- idx -.5 , seg[\"text\"])\n",
    "\n",
    "if plotEmotions:\n",
    "    pltidx += 1\n",
    "    ax3.set_xlabel(\"Time (seconds)\")\n",
    "    ax3.set_xlin(0, child[\"frame\"].max()/FPS)  \n",
    "    emotions = utils.getFaceData(processedvideos,VideoID)\n",
    "    emotions[\"ticker\"] = 1\n",
    "\n",
    "fig, ax = plt.subplots(2,1)\n",
    "plt.xlim(0,xmax)\n",
    "\n",
    "for index in range(2):\n",
    "    ems = emotions[emotions[\"index\"]==index]\n",
    "    #who is the person we are plotting\n",
    "    # key gives the emotion name, data gives the actual values (also labels)\n",
    "    for key, data in ems.groupby('emotion'):\n",
    "        #plot scatter plot of emotion occurances\n",
    "        ax[index].scatter(data[\"frame\"], data[\"ticker\"], label=key, c=emotionColors[key], alpha=0.5, s=100)\n",
    "        ax[index].set_title(who[index] + \" emotions\")\n",
    "#show legend with emotion colours\n",
    "plt.legend(loc='best')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out of curiousity, let's plot the location of the adult and child's nose\n",
    "\n",
    "childnoseplot = plt.plot(child[\"frame\"], child[\"nose.x\"], c=\"red\", alpha=0.5)\n",
    "adultnoseplot = plt.plot(adult[\"frame\"], adult[\"nose.x\"], c=\"blue\", alpha=0.5)\n",
    "plt.legend(['child', 'adult'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#suppress non plot outputs\n",
    "\n",
    "childnoseplot = plt.plot(child[\"frame\"], child[\"nose.x\"], c=\"red\", alpha=0.5)\n",
    "adultnoseplot = plt.plot(adult[\"frame\"], adult[\"nose.x\"], c=\"blue\", alpha=0.5)\n",
    "plt.legend(['child', 'adult'], loc='upper left')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('X position of nose (horizontal pixels)')\n",
    "#add xticks in seconds (one second is every frame number/FPS)\n",
    "plt.xticks(np.arange(0, child[\"frame\"].max(), FPS), np.arange(0, child[\"frame\"].max()/FPS, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the captions.\n",
    "Go through the speechjson. For each speech segment add a horizotal line with the text. Start and End times from the speechjson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a timeline for the emotions of the participants.\n",
    "We'll experiment to find best visualisation. \n",
    "Note this assumes that faces are correctly assigned to correct indviduals. \n",
    "TODO - Code that uses bounding boxes to assign faces to individuals.\n",
    "\n",
    "First we will try a 'scatter' graph. Color coded for each emotion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoname = os.path.basename(VIDEO_FILE2)\n",
    "\n",
    "emotions = utils.getFaceData(processedvideos,videoname)\n",
    "emotions[\"ticker\"] = 1\n",
    "\n",
    "fig, ax = plt.subplots(2,1)\n",
    "plt.xlim(0,xmax)\n",
    "\n",
    "for index in range(2):\n",
    "    ems = emotions[emotions[\"index\"]==index]\n",
    "    #who is the person we are plotting\n",
    "    # key gives the emotion name, data gives the actual values (also labels)\n",
    "    for key, data in ems.groupby('emotion'):\n",
    "        #plot scatter plot of emotion occurances\n",
    "        ax[index].scatter(data[\"frame\"], data[\"ticker\"], label=key, c=emotionColors[key], alpha=0.5, s=100)\n",
    "        ax[index].set_title(who[index] + \" emotions\")\n",
    "#show legend with emotion colours\n",
    "plt.legend(loc='best')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a playback of the video. with a slider to move through the frames and a play/pause button to play the video.\n",
    "#make sure we show frame counter\n",
    "#use ipywidgets for this\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "annotatedVideo = utils.getVideoProperty(processedvideos,VIDEO_FILE2,\"annotatedVideo\")\n",
    "\n",
    "if annotatedVideo is not None:\n",
    "    #get the video\n",
    "    #video = ultralytics.Video(annotatedVideo)\n",
    "    #get the number of frames\n",
    "    nframes = 230\n",
    "    #create a slider\n",
    "    slider = widgets.IntSlider(min=0, max=nframes, step=1, value=0)\n",
    "    #create a play button\n",
    "    play = widgets.Play(min=0, max=nframes, step=1, interval=1000)\n",
    "    #link the slider and play button\n",
    "    widgets.jslink((play, 'value'), (slider, 'value'))\n",
    "    #create a video player using cv2\n",
    "        \n",
    "    #create a VBox to hold the video player, slider and play button\n",
    "    widgets.VBox([videoPlayer, widgets.HBox([play, slider])])\n",
    "\n",
    "# need to control videoplayer with slider and play button\n",
    "# Create a callback function to update the video player\n",
    "def update_video(value):\n",
    "    videoPlayer.seek(value)\n",
    "\n",
    "# Link the slider and play button to the callback function\n",
    "slider.observe(update_video, 'value')\n",
    "play.observe(update_video, 'value')\n",
    "\n",
    "# create a VBox to hold the video player, slider, and play button\n",
    "widget_box = widgets.VBox([videoPlayer, widgets.HBox([play, slider])])\n",
    "\n",
    "# display the widget box\n",
    "display(widget_box)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedVideo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import ipywidgets as wd\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Read the video (replace 'myvideo.mp4' with your video file)\n",
    "video = cv2.VideoCapture(VIDEO_FILE2)\n",
    "\n",
    "\n",
    "# Initialize the widget for displaying the video frame\n",
    "image_widget = wd.Image(format='jpeg')\n",
    "\n",
    "# Function to update the displayed frame\n",
    "def update_frame(change):\n",
    "    frame_index = change.new\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "    _, frame = video.read()\n",
    "    image_widget.value = cv2.imencode('.jpg', frame)[1].tobytes()\n",
    "\n",
    "# Create an IntSlider for frame selection\n",
    "frame_slider = wd.IntSlider(min=0, max=int(video.get(cv2.CAP_PROP_FRAME_COUNT)) - 1, value=0)\n",
    "frame_slider.observe(update_frame, names='value')\n",
    "\n",
    "# Display the initial frame\n",
    "update_frame(wd.observe(frame_slider, 'value'))\n",
    "\n",
    "# Show the slider and video frame\n",
    "display(wd.VBox([frame_slider, image_widget]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyjokes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
