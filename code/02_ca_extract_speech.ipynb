{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2 Extract speech and conversation from audio files\n",
                "\n",
                "For speech recognition we use OpenAI's [Whisper](https://github.com/openai/whisper) model.\n",
                "And pyAnnote to identify different speakers in the audio file.\n",
                "\n",
                "\n",
                "\n",
                "This code here is based on prototypes developed at Sage IDEMS hackathon in 2023 \n",
                "https://github.com/chilledgeek/ethical_ai_hackathon_2023\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "project_root = os.path.join(\"..\")\n",
                "sys.path.append(project_root)\n",
                "\n",
                "import time\n",
                "import json\n",
                "import pandas as pd\n",
                "from src.utils import *\n",
                "# Import specific modules for better readability\n",
                "from src.utils.io_utils import getProcessedVideos, saveProcessedVideos\n",
                "from src.utils.audio import convert_video_to_audio_moviepy, find_f0\n",
                "from src.utils.speech import whisper_transcribe\n",
                "from src.utils.diarization import diarize_audio, load_rttm\n",
                "from src.config import PATH_CONFIG\n",
                "from src.utils.notebook_utils import display_config_info, ensure_dir_exists"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get paths from config - override here if needed\n",
                "videos_in = PATH_CONFIG['videos_in']\n",
                "data_out = PATH_CONFIG['data_out']\n",
                "\n",
                "# Ensure output directory exists\n",
                "if ensure_dir_exists(data_out):\n",
                "    print(f\"Created output directory: {data_out}\")\n",
                "\n",
                "# Display configuration information\n",
                "display_config_info(videos_in, data_out, \"Audio Processing Configuration\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "demo_data = PATH_CONFIG['demo_data']\n",
                "AUDIO_FILE = os.path.join(demo_data, \"2UWdXP.joke1.rep2.take1.Peekaboo.mp3\")\n",
                "AUDIO_FILE2 = os.path.join(demo_data, \"2UWdXP.joke2.rep1.take1.NomNomNom.mp3\")\n",
                "testset = [AUDIO_FILE, AUDIO_FILE2]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "processedvideos = getProcessedVideos(data_out)\n",
                "processedvideos.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.1 Audio extraction with moviepy\n",
                "\n",
                "The first step is simple. We extract the audio from each video and save it as `mp3` or `wav`. We will use the `moviepy` library to do this. \n",
                "This will be helpful for later analysis and regenerating labeled videos with audio.\n",
                "\n",
                "Note that `moviepy` is a wrapper around `ffmpeg` and `ffmpeg` needs to be installed separately. \n",
                "\n",
                "`conda install ffmpeg moviepy`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "forceaudio = False\n",
                "#output_ext=\"mp3\"\n",
                "output_ext=\"wav\"\n",
                "\n",
                "for index, r in processedvideos.iterrows():\n",
                "    if forceaudio or pd.isnull(r[\"Audio.file\"]):\n",
                "        audiopath = convert_video_to_audio_moviepy(videos_in,r[\"VideoID\"], data_out, output_ext=output_ext)\n",
                "        r[\"Audio.file\"] = audiopath\n",
                "        r[\"Audio.when\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
                "        #update this row in processedvideos dataframe\n",
                "        processedvideos.loc[index] = r\n",
                "    else:\n",
                "        print(\"Audio already extracted for video: \", r[\"VideoID\"])\n",
                "        \n",
                "\n",
                "saveProcessedVideos(processedvideos, data_out)\n",
                "processedvideos.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 Speech-to-text \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2.1 How not do to it  \n",
                "\n",
                "We tried [SpeechBrain]((https://github.com/speechbrain/speechbrain), [Speech Recognition](https://github.com/Uberi/speech_recognition) and [Sphinx](https://github.com/cmusphinx/pocketsphinx). None of them worked out of the box. Speechbrain was fiddly and not very accurate (with default settings). Rather than trying to improve it. Let's try the OpenAI Whisper model instead.\n",
                "\n",
                "See our [version 0.5alpha](https://github.com/InfantLab/babyjokes/releases/tag/v0.5alpha-sage) for the code."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2.2 Speech-to-text using OpenAI Whisper \n",
                "\n",
                "There is a free version of the [OpenAI Whisper](https://github.com/openai/whisper) model. It is multilingual (xx languages) and comes in a range of different sizes (and accuracies). We'll try the `base` model. \n",
                "\n",
                "Simple tutorial: https://analyzingalpha.com/openai-whisper-python-tutorial "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import whisper\n",
                "model = whisper.load_model(\"base\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We can use the imported function directly from our new module\n",
                "# No need to redefine it here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for index, r in processedvideos.iterrows():\n",
                "    if pd.isnull(r[\"Speech.file\"]) and not pd.isnull(r[\"Audio.file\"]):\n",
                "        speechpath, result = whisper_transcribe(r[\"Audio.file\"], save_path=data_out)\n",
                "        r[\"Speech.file\"] = speechpath\n",
                "        r[\"Speech.when\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
                "        #update this row in processedvideos dataframe\n",
                "        processedvideos.loc[index] = r\n",
                "        \n",
                "saveProcessedVideos(processedvideos, data_out)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "processedvideos.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.3 Speaker annotation with PyAnnote\n",
                "\n",
                "We'll use the [PyAnnote](https://github.com/pyannote/pyannote-audio) library to annotate the speakers in the audio. This library listens to the audio and tries to identify the individual speakers. It provides a 'diarization' of the audio. Identifying the start and end of each utterance and assigning them to particular speakers. It can handle multiple speakers and overlapping speech. \n",
                "\n",
                "`conda install pyannote.audio -c pyannote`\n",
                "\n",
                "or \n",
                "\n",
                "`pip install pyannote.audio`\n",
                "\n",
                "To access the pretrained models you need a HuggingFace API token. Store this in .env file. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#may want to add two new columns to the processedvideos dataframe\n",
                "#Diary.file: the path to the diary file\n",
                "#Diary.when: the time when the diary file was created\n",
                "\n",
                "if (\"Diary.file\" not in processedvideos.columns):\n",
                "    processedvideos[\"Diary.file\"] = None\n",
                "if (\"Diary.when\" not in processedvideos.columns):\n",
                "    processedvideos[\"Diary.when\"] = None    \n",
                "\n",
                "saveProcessedVideos(processedvideos, data_out)\n",
                "processedvideos.head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We can use the imported functions directly from our new module\n",
                "# No need to redefine them here\n",
                "\n",
                "forceDiary = True\n",
                "\n",
                "for index, r in processedvideos.iterrows():\n",
                "    if (forceDiary or pd.isnull(r[\"Diary.file\"])) and not pd.isnull(r[\"Audio.file\"]):\n",
                "        diary = diarize_audio(r[\"Audio.file\"])\n",
                "        #save the diaryjson to a file\n",
                "        basename = os.path.basename(r[\"Audio.file\"])\n",
                "        filename, ext = os.path.splitext(basename)\n",
                "        diarypath = os.path.join(data_out, filename + \".diary.rttm\")\n",
                "        with open(diarypath, \"w\") as f:\n",
                "            diary.write_rttm(f)\n",
                "        r[\"Diary.file\"] = diarypath\n",
                "        r[\"Diary.when\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
                "        #update this row in processedvideos dataframe\n",
                "        processedvideos.loc[index] = r\n",
                "\n",
                "saveProcessedVideos(processedvideos, data_out)\n",
                "processedvideos.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# we can use load_rttm to reload the rttm file\n",
                "\n",
                "# Load the diarization output from the RTTM file\n",
                "_, diary = load_rttm(r\"C:\\Users\\caspar\\OneDrive\\LegoGPI\\babyjokes\\data\\1_interim\\2UWdXP.joke1.rep2.take1.Peekaboo_h265.diary.rttm\").popitem()\n",
                "\n",
                "diary"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2.4 Voice fundamental frequency (F0) estimation\n",
                "\n",
                "A simvple estimate is easy to get from `librosa`. So let's include it. Will use diarization to do it per utterance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add F0 columns to processedvideos if they don't exist\n",
                "if \"F0.file\" not in processedvideos.columns:\n",
                "    processedvideos[\"F0.file\"] = None\n",
                "if \"F0.when\" not in processedvideos.columns:\n",
                "    processedvideos[\"F0.when\"] = None\n",
                "\n",
                "# Process F0 for each video\n",
                "forceF0 = False\n",
                "for index, r in processedvideos.iterrows():\n",
                "    if (forceF0 or pd.isnull(r[\"F0.file\"])) and not pd.isnull(r[\"Audio.file\"]):\n",
                "        f0, voiced_flag, voiced_probs = find_f0(r[\"Audio.file\"])\n",
                "        \n",
                "        # Save F0 data\n",
                "        basename = os.path.basename(r[\"Audio.file\"])\n",
                "        filename, ext = os.path.splitext(basename)\n",
                "        f0path = os.path.join(data_out, filename + \".f0.npz\")\n",
                "        np.savez(f0path, f0=f0, voiced_flag=voiced_flag, voiced_probs=voiced_probs)\n",
                "        \n",
                "        # Update record\n",
                "        r[\"F0.file\"] = f0path\n",
                "        r[\"F0.when\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
                "        processedvideos.loc[index] = r\n",
                "\n",
                "saveProcessedVideos(processedvideos, data_out)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "babyjokes",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.5"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
