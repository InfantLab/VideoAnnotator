{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 Classify videos by joke type.\n",
    "\n",
    "Each video demonstrates a single joke type. We have this as meta-data. Can we train a classifier based on the movement data?\n",
    "\n",
    "Each video shows one joke from a set of five possibilities [Peekaboo,TearingPaper,NomNomNom,ThatsNotAHat,ThatsNotACat].\n",
    "\n",
    "We will use TensorFlow to train a classifier to predict the joke type from the movement data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\caspar\\Anaconda3\\envs\\babyjokes\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# local imports\n",
    "import utils\n",
    "import display\n",
    "import calcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Load the data\n",
    "\n",
    "### Use either small demo \n",
    "Consists of 54 videos. From 4 families (parent and baby) demoing five jokes three times each. (Some missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_in = os.path.join(\"..\",\"LookitLaughter.test\")\n",
    "demo_data = os.path.join(\"..\",\"data\", \"demo\")\n",
    "temp_out = os.path.join(\"..\",\"data\",\"0_temp\")\n",
    "data_out = os.path.join(\"..\",\"data\",\"1_interim\")\n",
    "videos_out = os.path.join(\"..\",\"data\",\"2_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or the Full set\n",
    "\n",
    "Consists of 1425 videos. From 90 familes (parent and baby) demoing approximately five jokes three times each. Some repetitions and omissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_in = os.path.join(\"..\",\"..\",\"LookitLaughter.full.videos\")\n",
    "temp_out = os.path.join(\"..\",\"..\",\"LookitLaughter.full.data\",\"0_temp\")\n",
    "data_out = os.path.join(\"..\",\"..\",\"LookitLaughter.full.data\",\"1_interim\")\n",
    "videos_out = os.path.join(\"..\",\"..\",\"LookitLaughter.full.data\",\"2_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing processedvideos.xlsx\n",
      "We have 54 processed videos.\n",
      "Min Frames: 47\n",
      "Max Frames: 586\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VideoID</th>\n",
       "      <th>ChildID</th>\n",
       "      <th>JokeType</th>\n",
       "      <th>Joke.Label</th>\n",
       "      <th>JokeNum</th>\n",
       "      <th>JokeRep</th>\n",
       "      <th>JokeTake</th>\n",
       "      <th>HowFunny</th>\n",
       "      <th>LaughYesNo</th>\n",
       "      <th>Frames</th>\n",
       "      <th>...</th>\n",
       "      <th>Speech.file</th>\n",
       "      <th>Speech.when</th>\n",
       "      <th>Objects.file</th>\n",
       "      <th>Objects.when</th>\n",
       "      <th>Understand.file</th>\n",
       "      <th>Understand.when</th>\n",
       "      <th>Faces.normed</th>\n",
       "      <th>Keypoints.normed</th>\n",
       "      <th>annotatedVideo</th>\n",
       "      <th>annotated.when</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2UWdXP.joke1.rep2.take1.Peekaboo.mp4</td>\n",
       "      <td>2UWdXP</td>\n",
       "      <td>Peekaboo</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Slightly funny</td>\n",
       "      <td>No</td>\n",
       "      <td>217</td>\n",
       "      <td>...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke1.rep2.take1.Peek...</td>\n",
       "      <td>2023-09-20 16:58:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke1.rep2.take1.Peek...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke1.rep2.take1.Peek...</td>\n",
       "      <td>../data/2_final/2UWdXP.joke1.rep2.take1.Peekab...</td>\n",
       "      <td>2024-02-16 11:03:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2UWdXP.joke1.rep3.take1.Peekaboo.mp4</td>\n",
       "      <td>2UWdXP</td>\n",
       "      <td>Peekaboo</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Slightly funny</td>\n",
       "      <td>No</td>\n",
       "      <td>152</td>\n",
       "      <td>...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke1.rep3.take1.Peek...</td>\n",
       "      <td>2023-09-20 16:58:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke1.rep3.take1.Peek...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke1.rep3.take1.Peek...</td>\n",
       "      <td>../data/2_final/2UWdXP.joke1.rep3.take1.Peekab...</td>\n",
       "      <td>2024-02-16 11:03:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2UWdXP.joke2.rep1.take1.NomNomNom.mp4</td>\n",
       "      <td>2UWdXP</td>\n",
       "      <td>NomNomNom</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Funny</td>\n",
       "      <td>No</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep1.take1.NomN...</td>\n",
       "      <td>2023-09-20 16:58:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep1.take1.NomN...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep1.take1.NomN...</td>\n",
       "      <td>../data/2_final/2UWdXP.joke2.rep1.take1.NomNom...</td>\n",
       "      <td>2024-02-16 11:03:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2UWdXP.joke2.rep2.take1.NomNomNom.mp4</td>\n",
       "      <td>2UWdXP</td>\n",
       "      <td>NomNomNom</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Slightly funny</td>\n",
       "      <td>No</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep2.take1.NomN...</td>\n",
       "      <td>2023-09-20 16:58:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep2.take1.NomN...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep2.take1.NomN...</td>\n",
       "      <td>../data/2_final/2UWdXP.joke2.rep2.take1.NomNom...</td>\n",
       "      <td>2024-02-16 11:03:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2UWdXP.joke2.rep3.take1.NomNomNom.mp4</td>\n",
       "      <td>2UWdXP</td>\n",
       "      <td>NomNomNom</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Slightly funny</td>\n",
       "      <td>No</td>\n",
       "      <td>133</td>\n",
       "      <td>...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep3.take1.NomN...</td>\n",
       "      <td>2023-09-20 16:58:48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep3.take1.NomN...</td>\n",
       "      <td>../data/1_interim/2UWdXP.joke2.rep3.take1.NomN...</td>\n",
       "      <td>../data/2_final/2UWdXP.joke2.rep3.take1.NomNom...</td>\n",
       "      <td>2024-02-16 11:03:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 VideoID ChildID   JokeType  Joke.Label  \\\n",
       "0   2UWdXP.joke1.rep2.take1.Peekaboo.mp4  2UWdXP   Peekaboo           2   \n",
       "1   2UWdXP.joke1.rep3.take1.Peekaboo.mp4  2UWdXP   Peekaboo           2   \n",
       "2  2UWdXP.joke2.rep1.take1.NomNomNom.mp4  2UWdXP  NomNomNom           1   \n",
       "3  2UWdXP.joke2.rep2.take1.NomNomNom.mp4  2UWdXP  NomNomNom           1   \n",
       "4  2UWdXP.joke2.rep3.take1.NomNomNom.mp4  2UWdXP  NomNomNom           1   \n",
       "\n",
       "   JokeNum  JokeRep  JokeTake        HowFunny LaughYesNo  Frames  ...  \\\n",
       "0        1        2         1  Slightly funny         No     217  ...   \n",
       "1        1        3         1  Slightly funny         No     152  ...   \n",
       "2        2        1         1           Funny         No      95  ...   \n",
       "3        2        2         1  Slightly funny         No      97  ...   \n",
       "4        2        3         1  Slightly funny         No     133  ...   \n",
       "\n",
       "                                         Speech.file          Speech.when  \\\n",
       "0  ../data/1_interim/2UWdXP.joke1.rep2.take1.Peek...  2023-09-20 16:58:38   \n",
       "1  ../data/1_interim/2UWdXP.joke1.rep3.take1.Peek...  2023-09-20 16:58:39   \n",
       "2  ../data/1_interim/2UWdXP.joke2.rep1.take1.NomN...  2023-09-20 16:58:40   \n",
       "3  ../data/1_interim/2UWdXP.joke2.rep2.take1.NomN...  2023-09-20 16:58:40   \n",
       "4  ../data/1_interim/2UWdXP.joke2.rep3.take1.NomN...  2023-09-20 16:58:48   \n",
       "\n",
       "   Objects.file  Objects.when Understand.file Understand.when  \\\n",
       "0           NaN           NaN             NaN             NaN   \n",
       "1           NaN           NaN             NaN             NaN   \n",
       "2           NaN           NaN             NaN             NaN   \n",
       "3           NaN           NaN             NaN             NaN   \n",
       "4           NaN           NaN             NaN             NaN   \n",
       "\n",
       "                                        Faces.normed  \\\n",
       "0  ../data/1_interim/2UWdXP.joke1.rep2.take1.Peek...   \n",
       "1  ../data/1_interim/2UWdXP.joke1.rep3.take1.Peek...   \n",
       "2  ../data/1_interim/2UWdXP.joke2.rep1.take1.NomN...   \n",
       "3  ../data/1_interim/2UWdXP.joke2.rep2.take1.NomN...   \n",
       "4  ../data/1_interim/2UWdXP.joke2.rep3.take1.NomN...   \n",
       "\n",
       "                                    Keypoints.normed  \\\n",
       "0  ../data/1_interim/2UWdXP.joke1.rep2.take1.Peek...   \n",
       "1  ../data/1_interim/2UWdXP.joke1.rep3.take1.Peek...   \n",
       "2  ../data/1_interim/2UWdXP.joke2.rep1.take1.NomN...   \n",
       "3  ../data/1_interim/2UWdXP.joke2.rep2.take1.NomN...   \n",
       "4  ../data/1_interim/2UWdXP.joke2.rep3.take1.NomN...   \n",
       "\n",
       "                                      annotatedVideo       annotated.when  \n",
       "0  ../data/2_final/2UWdXP.joke1.rep2.take1.Peekab...  2024-02-16 11:03:50  \n",
       "1  ../data/2_final/2UWdXP.joke1.rep3.take1.Peekab...  2024-02-16 11:03:51  \n",
       "2  ../data/2_final/2UWdXP.joke2.rep1.take1.NomNom...  2024-02-16 11:03:52  \n",
       "3  ../data/2_final/2UWdXP.joke2.rep2.take1.NomNom...  2024-02-16 11:03:53  \n",
       "4  ../data/2_final/2UWdXP.joke2.rep3.take1.NomNom...  2024-02-16 11:03:54  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processedvideos = utils.getprocessedvideos(data_out)\n",
    "minFrames = processedvideos['Frames'].min()\n",
    "maxFrames = processedvideos['Frames'].max()\n",
    "print(f\"We have {len(processedvideos)} processed videos.\")\n",
    "print(f\"Min Frames: {minFrames}\\nMax Frames: {maxFrames}\")\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Load and preprocess the data\n",
    "\n",
    "1. Load normed movement data. \n",
    "2. Pad all sequences to the same length. \n",
    "3. Interpolate missing values (up to last frame of real data).\n",
    "4. Replace final missing values with zeros.\n",
    "5. Add to tf.data.Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMovementDataset(processedVideos, maxFrames = None, ragged = False):\n",
    "    \"\"\"\n",
    "    Creates a movement dataset from processed videos.\n",
    "\n",
    "    Args:\n",
    "        processedVideos (pandas.DataFrame): A DataFrame containing processed video data.\n",
    "        maxFrames (int, optional): The maximum number of frames to include in the dataset. Defaults to max of all videos.\n",
    "        ragged (bool, optional): Whether to create a ragged tensor. Defaults to False (TODO: not implemented yet)\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: A TensorFlow Dataset containing features and labels.\n",
    "    \"\"\"\n",
    "    if maxFrames is None:\n",
    "        maxFrames = processedVideos[\"Frames\"].max()\n",
    "    if ragged:\n",
    "        raise NotImplementedError(\"Ragged tensors not implemented yet.\")    \n",
    "    dataset = []\n",
    "    labels = []\n",
    "    # for each row of processedVideos, we add one timeseries to the dataset\n",
    "    for index, r in processedvideos.iterrows():\n",
    "        df = pd.read_csv(r['Keypoints.normed'])\n",
    "        df = utils.padMovementData(df, maxFrames)\n",
    "        df = utils.interpolateMovementData(df)\n",
    "        df = df.replace(np.nan, 0)\n",
    "        df = utils.flattenMovementDataset(df)\n",
    "        \n",
    "        features = tf.convert_to_tensor(df.values, dtype=tf.float32)\n",
    "        label = r[\"Joke.Label\"]\n",
    "        dataset.append(features)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return tf.data.Dataset.from_tensor_slices((dataset, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Pack_N_54_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [317,115] != values[28].shape = [585,115] [Op:Pack] name: component_0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m((maxFrames \u001b[38;5;241m+\u001b[39m minFrames) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m tfdataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreateMovementDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessedvideos\u001b[49m\u001b[43m,\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m train, test \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39msplit_dataset(tfdataset, left_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m, in \u001b[0;36mcreateMovementDataset\u001b[1;34m(processedVideos, maxFrames, ragged)\u001b[0m\n\u001b[0;32m     29\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[0;32m     30\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caspar\\Anaconda3\\envs\\babyjokes\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:825\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[1;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caspar\\Anaconda3\\envs\\babyjokes\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caspar\\Anaconda3\\envs\\babyjokes\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m     35\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[1;32mc:\\Users\\caspar\\Anaconda3\\envs\\babyjokes\\Lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:134\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    131\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    133\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 134\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[1;32mc:\\Users\\caspar\\Anaconda3\\envs\\babyjokes\\Lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caspar\\Anaconda3\\envs\\babyjokes\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:696\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[0;32m    695\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caspar\\Anaconda3\\envs\\babyjokes\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m           _add_error_prefix(\n\u001b[0;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\caspar\\Anaconda3\\envs\\babyjokes\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1585\u001b[0m, in \u001b[0;36m_autopacking_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m   1583\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m inferred_dtype:\n\u001b[0;32m   1584\u001b[0m   v \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(_cast_nested_seqs_to_dtype(dtype), v)\n\u001b[1;32m-> 1585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_autopacking_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpacked\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\caspar\\Anaconda3\\envs\\babyjokes\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1492\u001b[0m, in \u001b[0;36m_autopacking_helper\u001b[1;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m   1489\u001b[0m   \u001b[38;5;66;03m# NOTE: Fast path when all the items are tensors, this doesn't do any type\u001b[39;00m\n\u001b[0;32m   1490\u001b[0m   \u001b[38;5;66;03m# checking.\u001b[39;00m\n\u001b[0;32m   1491\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(elem, core\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m list_or_tuple):\n\u001b[1;32m-> 1492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_or_tuple\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m must_pack \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1494\u001b[0m converted_elems \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\caspar\\Anaconda3\\envs\\babyjokes\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:6719\u001b[0m, in \u001b[0;36mpack\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   6717\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   6718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 6719\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6720\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   6721\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\caspar\\Anaconda3\\envs\\babyjokes\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Pack_N_54_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [317,115] != values[28].shape = [585,115] [Op:Pack] name: component_0"
     ]
    }
   ],
   "source": [
    "frames = round((maxFrames + minFrames) / 2)\n",
    "tfdataset = createMovementDataset(processedvideos,frames)\n",
    "\n",
    "train, test = tf.keras.utils.split_dataset(tfdataset, left_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get first element of dataset so we can grab its dimensions\n",
    "keyPoints = next(iter(train))[0]\n",
    "\n",
    "#let's build a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(keyPoints.shape[0], keyPoints.shape[1])),\n",
    "    layers.LSTM(8),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "              sample_weight_mode='temporal',\n",
    "              optimizer=tf.optimizers.Adam(),\n",
    "              metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train.batch(32), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's evaluate the model\n",
    "model.evaluate(test.batch(32))\n",
    "\n",
    "#table of predictions\n",
    "predictions = model.predict(test.batch(32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
