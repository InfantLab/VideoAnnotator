{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Visualise labelled data and remove artifacts\n",
    "\n",
    "Code that let us overlay each frame of video with outputs from the models. And create time series plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ultralytics.utils as ultrautils\n",
    "import utils\n",
    "import display\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these to your imports\n",
    "from src.config import PATH_CONFIG\n",
    "from src.utils.notebook_utils import display_config_info, ensure_dir_exists\n",
    "\n",
    "# Get paths from config\n",
    "videos_in = PATH_CONFIG['videos_in']\n",
    "data_out = PATH_CONFIG['data_out']\n",
    "\n",
    "# Ensure output directory exists\n",
    "if ensure_dir_exists(data_out):\n",
    "    print(f\"Created output directory: {data_out}\")\n",
    "\n",
    "# Display configuration information\n",
    "display_config_info(videos_in, data_out, \"Processing Configuration\")\n",
    "metadata_file = \"_LookitLaughter.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_in = os.path.join(\"..\",\"..\",\"LookitLaughter.full.videos\")\n",
    "temp_out = os.path.join(\"..\",\"..\",\"LookitLaughter.full.data\",\"0_temp\")\n",
    "data_out = os.path.join(\"..\",\"..\",\"LookitLaughter.full.data\",\"1_interim\")\n",
    "videos_out = os.path.join(\"..\",\"..\",\"LookitLaughter.full.data\",\"2_final\")\n",
    "\n",
    "metadata_file = \"_LookitLaughter.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m processedvideos \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mgetProcessedVideos(data_out)\n\u001b[0;32m      2\u001b[0m processedvideos\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "processedvideos = utils.getProcessedVideos(data_out)\n",
    "processedvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Add annotations to all vidoes.\n",
    "\n",
    "Generate annotated videos for all videos in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forceAnnotation = False\n",
    "\n",
    "for index, r in processedvideos.iterrows():\n",
    "\n",
    "    videopath = os.path.join(videos_in,r[\"VideoID\"])\n",
    "    videoname = os.path.basename(r[\"VideoID\"])\n",
    "    try: \n",
    "        #let's get all the annotations for this video\n",
    "        kpts = utils.getKeyPoints(processedvideos,videoname)\n",
    "        facedata = utils.getFaceData(processedvideos,videoname)\n",
    "        speechdata = utils.getSpeechData(processedvideos,videoname)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Data error for {videoname}\\n\" + \"Error: \" + str(e))\n",
    "        continue\n",
    "    if forceAnnotation or pd.isnull(r[\"annotatedVideo\"]) or not os.path.exists(r[\"annotatedVideo\"]):\n",
    "        print(f\"Creating annotated video for {videoname}\")\n",
    "        annotatedVideo = display.createAnnotatedVideo(videopath, kpts, facedata, speechdata, temp_out, False)\n",
    "        vidwithaudio = display.addSoundtoVideo(annotatedVideo, r[\"Audio.file\"], videos_out)\n",
    "        r[\"annotatedVideo\"] = vidwithaudio\n",
    "        r[\"annotated.when\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "        #update this row in processedvideos dataframe\n",
    "        processedvideos.loc[index] = r\n",
    "    else:\n",
    "        print(f\"Already processed {r['VideoID']}\")\n",
    "\n",
    "#save the processedvideos dataframe\n",
    "utils.saveProcessedVideos(processedvideos, data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#facedata = utils.getFaceData(processedvideos,videoname)\n",
    "videodata = processedvideos[processedvideos[\"VideoID\"] == videoname]\n",
    "if videodata.shape[0] <= 0:\n",
    "    raise FileNotFoundError(f\"No face data file found for {videoname}\")\n",
    "print(f\"We have a face data file for {videoname}\")\n",
    "facesfile = videodata[\"Faces.file\"].values[0]\n",
    "print(facesfile)\n",
    "pd.read_csv(facesfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2  Correct person labels in all videos.\n",
    "\n",
    "Swap parent and child if these are wrong. Ignore other people in video.\n",
    "\n",
    "In metadata, we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def keyPointsStdev(df, frames=[], people=\"all\", bodypart=\"whole\"):\n",
    "    \"\"\"find standard deviation of x and y values of keypoints        \n",
    "\n",
    "    args:   df - dataframe of keypoints\n",
    "            frames - list of frames to include\n",
    "            people - list of people to include\n",
    "            bodypart - which bodypart to use, default is \"whole\" for all keypoints\n",
    "    returns:\n",
    "            dataframe of average positions\n",
    "    \"\"\"\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        frames = df.frame.unique()\n",
    "\n",
    "    if people == \"all\":\n",
    "        people = df.person.unique()\n",
    "\n",
    "    if bodypart != \"whole\":\n",
    "        raise NotImplementedError(\"Only whole body implemented for now\")\n",
    "\n",
    "    threshold = 0.5\n",
    "\n",
    "    # create new columns for the centre of gravity\n",
    "    df[\"std.x\"] = np.nan\n",
    "    df[\"std.y\"] = np.nan\n",
    "\n",
    "    for frame in frames:\n",
    "        for person in people:\n",
    "            # get the keypoints for this person in this frame\n",
    "            kpts = df[(df[\"frame\"] == frame) & (df[\"person\"] == person)]\n",
    "\n",
    "            if not kpts.empty:\n",
    "                # get the average position of the bodypart\n",
    "                if bodypart == \"whole\":\n",
    "                    xyc = kpts.iloc[:, 8:59].to_numpy()  # just keypoints\n",
    "                    xyc = xyc.reshape(-1, 3)  # reshape to n x 3 array (x,y,conf\n",
    "                    avgx, avgy = stdevxys(xyc, threshold)\n",
    "\n",
    "                df.loc[\n",
    "                    (df[\"frame\"] == frame) & (df[\"person\"] == person), \"cog.x\"\n",
    "                ] = avgx\n",
    "                df.loc[\n",
    "                    (df[\"frame\"] == frame) & (df[\"person\"] == person), \"cog.y\"\n",
    "                ] = avgy\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_timeline(sample):\n",
    "    \"\"\"Create a custom timeline visualization for a sample\"\"\"\n",
    "    if not hasattr(sample, \"metadata\") or \"duration\" not in sample.metadata:\n",
    "        print(f\"No duration metadata for {sample.id}\")\n",
    "        return\n",
    "        \n",
    "    videoname = os.path.basename(sample.filepath)\n",
    "    duration = sample.metadata[\"duration\"]\n",
    "    \n",
    "    # Create figure with multiple subplots\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(12, 8), sharex=True)\n",
    "    fig.suptitle(f\"Timeline for {videoname}\", fontsize=16)\n",
    "    \n",
    "    # Set x-axis limits\n",
    "    for ax in axs:\n",
    "        ax.set_xlim(0, duration)\n",
    "    \n",
    "    # Plot speech segments\n",
    "    ax_speech = axs[0]\n",
    "    ax_speech.set_title(\"Speech\")\n",
    "    ax_speech.set_yticks([])\n",
    "    \n",
    "    if hasattr(sample, \"temporal_detections\"):\n",
    "        detections = sample.temporal_detections.detections\n",
    "        for det in detections:\n",
    "            start, end = det.support\n",
    "            text = det.attrs.get(\"text\", \"\")\n",
    "            # Truncate long text\n",
    "            if len(text) > 40:\n",
    "                text = text[:37] + \"...\"\n",
    "            ax_speech.axvspan(start, end, alpha=0.3, color=\"blue\")\n",
    "            ax_speech.text(start, 0.5, text, fontsize=8, verticalalignment=\"center\")\n",
    "    \n",
    "    # Plot emotions\n",
    "    ax_emotion = axs[1]\n",
    "    ax_emotion.set_title(\"Emotions\")\n",
    "    \n",
    "    if hasattr(sample, \"emotion_detections\"):\n",
    "        detections = sample.emotion_detections.detections\n",
    "        \n",
    "        # Group by person\n",
    "        person_emotions = {\"Child\": [], \"Adult\": [], \"Unknown\": []}\n",
    "        for det in detections:\n",
    "            person = det.attrs.get(\"person\", \"Unknown\")\n",
    "            if person in person_emotions:\n",
    "                person_emotions[person].append(det)\n",
    "        \n",
    "        # Define y-positions for each person\n",
    "        y_positions = {\"Child\": 0.7, \"Adult\": 0.3, \"Unknown\": 0.5}\n",
    "        \n",
    "        # Plot each person's emotions\n",
    "        for person, detections in person_emotions.items():\n",
    "            y_pos = y_positions[person]\n",
    "            \n",
    "            for det in detections:\n",
    "                start, end = det.support\n",
    "                emotion = det.attrs.get(\"emotion\", \"unknown\")\n",
    "                color = emotionColors.get(emotion, {}).get(\"color\", \"gray\")\n",
    "                ax_emotion.axvspan(start, end, ymin=y_pos-0.15, ymax=y_pos+0.15, alpha=0.3, color=color)\n",
    "                ax_emotion.text(start, y_pos, emotion, fontsize=8, verticalalignment=\"center\")\n",
    "        \n",
    "        # Add legend for persons\n",
    "        for person, y_pos in y_positions.items():\n",
    "            ax_emotion.axhline(y=y_pos, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "            ax_emotion.text(duration, y_pos, person, fontsize=10, verticalalignment=\"center\",\n",
    "                           horizontalalignment=\"right\")\n",
    "    \n",
    "    # Plot joke type and ratings\n",
    "    ax_meta = axs[2]\n",
    "    ax_meta.set_title(\"Metadata\")\n",
    "    ax_meta.set_yticks([])\n",
    "    ax_meta.set_xlabel(\"Time (seconds)\")\n",
    "    \n",
    "    # Create colored bar based on laugh yes/no\n",
    "    laugh = sample.LaughYesNo if hasattr(sample, \"LaughYesNo\") else None\n",
    "    if laugh is not None:\n",
    "        color = \"green\" if laugh else \"red\"\n",
    "        ax_meta.axhspan(0.4, 0.6, xmin=0, xmax=1, alpha=0.3, color=color)\n",
    "        ax_meta.text(duration/2, 0.5, f\"Laugh: {'Yes' if laugh else 'No'}\", \n",
    "                    fontsize=12, horizontalalignment=\"center\", verticalalignment=\"center\")\n",
    "    \n",
    "    # Add joke type and rating as text\n",
    "    joke_type = sample.JokeType if hasattr(sample, \"JokeType\") else \"Unknown\"\n",
    "    how_funny = sample.HowFunny if hasattr(sample, \"HowFunny\") else \"Unknown\"\n",
    "    txt = f\"Joke: {joke_type}\\nRating: {how_funny}\"\n",
    "    ax_meta.text(0.02, 0.8, txt, transform=ax_meta.transAxes, fontsize=10,\n",
    "                verticalalignment=\"top\", bbox=dict(boxstyle=\"round\", alpha=0.1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Draw annotated timeline for a select video \n",
    "\n",
    "A group of visualisations to see what happens in a video. \n",
    "\n",
    "In each frame let's find the `centre of gravity` for each person (the average of all the high-confidence marker points). This is handy for time series visualisation. For example plotting the cog.x for each person over time shows how they move closer and further from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotionColors = {\"angry\":{\"color\":\"red\",\"arousal\":0.9,\"valence\":-0.2},\n",
    "                 \"fear\":{\"color\":\"orange\",\"arousal\":0.2,\"valence\":-0.9},\n",
    "                 \"happy\":{\"color\":\"yellow\",\"arousal\":0.2,\"valence\":0.9},\n",
    "                 \"neutral\":{\"color\":\"grey\",\"arousal\":0,\"valence\":0},\n",
    "                 \"sad\":{\"color\":\"blue\",\"arousal\":-0.2,\"valence\":-0.9},\n",
    "                 \"surprise\":{\"color\":\"green\",\"arousal\":0.9,\"valence\":0.2},\n",
    "                 \"disgust\":{\"color\":\"purple\",\"arousal\":-0.7,\"valence\":-0.7}}\n",
    "who = [\"child\", \"adult\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCoGrav = True\n",
    "plotStDev = True\n",
    "plotSpeech = True\n",
    "plotEmotions = True\n",
    "\n",
    "#numerical sum of boolean flags\n",
    "subplots = sum([plotCoGrav, plotStDev, plotSpeech, plotEmotions])\n",
    "\n",
    "if len(session.selected) == 0:\n",
    "    print(\"No video selected\")\n",
    "    exit()\n",
    "\n",
    "VideoID = dataset[session.selected[0]][\"VideoID\"]\n",
    "keypoints = utils.readKeyPointsFromCSV(processedvideos,VideoID)\n",
    "FPS = utils.getVideoProperty(processedvideos, VideoID, \"FPS\")\n",
    "xmax = keypoints[\"frame\"].max()\n",
    "#this bit of pandas magic calculates average x and y for all the rows.\n",
    "keypoints[[\"cogx\",\"cogy\"]] = keypoints.apply(lambda row: calcs.rowcogs(row.iloc[8:59]), axis=1, result_type='expand')\n",
    "keypoints[[\"stdx\",\"stdy\"]] = keypoints.apply(lambda row: calcs.rowstds(row.iloc[8:59]), axis=1, result_type='expand')\n",
    "\n",
    "#going to add a subplot foe each of the above flags\n",
    "plt.figure(figsize=(20, 5*subplots))\n",
    "plt.suptitle(\"Video Time Line Plots\")\n",
    "pltidx = 0\n",
    "if plotCoGrav:\n",
    "    ax = plt.subplot(subplots, 1, pltidx + 1)\n",
    "    pltidx += 1\n",
    "    ax.set_xlabel(\"Time (seconds)\")\n",
    "    ax.set_ylabel(\"Horizontal Position\")\n",
    "    ax.set_xlim(0, xmax/FPS)\n",
    "    child = keypoints[keypoints[\"person\"]==\"child\"]\n",
    "    adult = keypoints[keypoints[\"person\"]==\"adult\"]\n",
    "    #a plot of child's centre of gravity frame by frame\n",
    "    childplot = ax.plot(child[\"frame\"], child[\"cogx\"], c=\"red\", alpha=0.5)\n",
    "    ## add line of adult's centre of gravity\n",
    "    adultplot = ax.plot(adult[\"frame\"], adult[\"cogx\"], c=\"blue\", alpha=0.5)\n",
    "    #add legend\n",
    "    ax.legend(['child', 'adult'], loc='upper left')\n",
    "\n",
    "if plotStDev:\n",
    "    ax = plt.subplot(subplots, 1, pltidx + 1)\n",
    "    pltidx += 1\n",
    "    ax.set_xlabel(\"Time (seconds)\")\n",
    "    ax.set_ylabel(\"Horizontal Position\")\n",
    "    ax.set_xlim(0, xmax/FPS)\n",
    "    child = keypoints[keypoints[\"person\"]==\"child\"]\n",
    "    adult = keypoints[keypoints[\"person\"]==\"adult\"]\n",
    "    #a plot of child's centre of gravity frame by frame\n",
    "    childplot = ax.plot(child[\"frame\"], child[\"stdx\"], c=\"red\", alpha=0.5)\n",
    "    ## add line of adult's centre of gravity\n",
    "    adultplot = ax.plot(adult[\"frame\"], adult[\"stdx\"], c=\"blue\", alpha=0.5)\n",
    "    #add legend\n",
    "    ax.legend(['child', 'adult'], loc='upper left')\n",
    "\n",
    "if plotSpeech:\n",
    "    ax2 = plt.subplot(subplots, 1, pltidx + 1)\n",
    "    pltidx += 1\n",
    "    ax2.set_xlabel(\"Time (seconds)\")\n",
    "    ax2.set_ylabel(\"Identified Speech\")\n",
    "    speechjson = utils.getSpeechData(processedvideos,VideoID)\n",
    "    if speechjson is not None:\n",
    "        nsegs = len(speechjson[\"segments\"])\n",
    "        ax2.set_xlim(0, xmax/FPS)\n",
    "        ax2.set_ylim(0, nsegs)\n",
    "        #let's plot the speech segments as boxes\n",
    "        #label each one with the text\n",
    "        for idx, seg in enumerate(speechjson[\"segments\"]):\n",
    "            # #rectangle with the start and end times as x coordinates and nsegs - idx as y coordinates\n",
    "            #fill the rectangle\n",
    "            ax2.fill([seg[\"start\"], seg[\"end\"], seg[\"end\"], seg[\"start\"]], [nsegs - idx - 1, nsegs - idx - 1, nsegs - idx, nsegs - idx], 'r', alpha=0.5)\n",
    "            ax2.text(seg[\"start\"], nsegs- idx -.5 , seg[\"text\"])\n",
    "\n",
    "if plotEmotions:\n",
    "    ax3 = plt.subplot(subplots, 1, pltidx + 1)\n",
    "    pltidx += 1\n",
    "    ax3.set_xlabel(\"Time (seconds)\")\n",
    "    ax3.set_ylim(0, 2)\n",
    "    ax3.set_xlim(0, xmax/FPS)  \n",
    "    emotions = utils.getFaceData(processedvideos,VideoID)\n",
    "    emotions[\"ticker\"] = 1\n",
    "    for index in range(2):\n",
    "        ems = emotions[emotions[\"index\"]==index]\n",
    "        #who is the person we are plotting\n",
    "        # key gives the emotion name, data gives the actual values (also labels)\n",
    "        for key, data in ems.groupby('emotion'):\n",
    "            #plot scatter plot of emotion occurances\n",
    "            ax3.scatter(data[\"frame\"], data[\"ticker\"] + index, label=key, c=emotionColors[key][\"color\"], alpha=0.5, s=100)\n",
    "\n",
    "        \n",
    "    #show legend with emotion colours\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyjokes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
