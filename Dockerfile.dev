# VideoAnnotator Development Docker Image - Simple Approach
# This image includes GPU support AND your local model cache for instant testing
# Copies your existing models/ and weights/ directories
#
# Usage:
#   docker build -f dockerfile.dev -t videoannotator:dev .
#   docker run --gpus all --rm -p 8000:8000 -v ${PWD}/data:/app/data videoannotator:dev

FROM nvidia/cuda:12.6.0-runtime-ubuntu24.04

# Use bash with pipefail so RUN commands that use a pipe fail when any stage does
SHELL ["/bin/bash","-o","pipefail","-lc"]

# Prevent interactive prompts during package installation
ARG DEBIAN_FRONTEND=noninteractive

# Install base packages and locales in a single RUN to reduce layers and avoid
# pulling recommended packages unnecessarily (DL3015, DL3059)
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl python3 python3-venv python3-dev git git-lfs ffmpeg \
    libgl1-mesa-dri libglib2.0-0 libsm6 libxext6 libxrender1 libgomp1 locales \
    && locale-gen en_US.UTF-8 \
    && update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 \
    && rm -rf /var/lib/apt/lists/*

# Export UTF-8 locale for all processes
ENV LANG=en_US.UTF-8
ENV LC_ALL=en_US.UTF-8
ENV UV_LINK_MODE=copy

# Initialize Git LFS, install uv and set PATH, copy project files, and install
# dependencies in grouped RUNs to reduce layers. The uv installer is a piped
# command; using SHELL with pipefail above ensures failures are detected (DL4006).
RUN git lfs install \
    && curl -LsSf https://astral.sh/uv/install.sh | sh \
    && export PATH="/root/.local/bin:${PATH}" \
    && mkdir -p /app

ENV PATH="/root/.local/bin:${PATH}"

WORKDIR /app

# Copy project files explicitly so production images can exclude models/weights,
# while the dev image intentionally includes them.
COPY pyproject.toml uv.lock ./
COPY api_server.py ./
COPY src/ ./src/
COPY configs/ ./configs/
COPY scripts/ ./scripts/

# Convenience commands (avoid typing `uv run videoannotator ...`)
COPY scripts/container/bin/server /usr/local/bin/server
COPY scripts/container/bin/newtoken /usr/local/bin/newtoken
COPY scripts/container/bin/va /usr/local/bin/va
COPY scripts/container/bin/vatest /usr/local/bin/vatest
COPY scripts/container/bin/setupdb /usr/local/bin/setupdb
RUN chmod +x /usr/local/bin/server /usr/local/bin/newtoken /usr/local/bin/va /usr/local/bin/vatest /usr/local/bin/setupdb

# Copy your local models and weights for fast offline-ish iteration
COPY models/ /app/models/
COPY weights/ /app/weights/

# Install dependencies (torch/torchvision/torchaudio come from the configured
# `pytorch-cu124` uv index in pyproject.toml).
RUN uv sync --frozen --no-editable \
    && HADOLINT_DEST_DIR=/usr/local/bin bash scripts/install_hadolint.sh \
    && uv tool install specify-cli --from git+https://github.com/github/spec-kit.git

# Verify GPU access and model cache (no model downloading needed!)
RUN uv run python3 -c "import torch; from pathlib import Path; print(f'[DEV BUILD] CUDA available: {torch.cuda.is_available()}'); models_count = len(list(Path('/app/models').rglob('*'))) if Path('/app/models').exists() else 0; weights_count = len(list(Path('/app/weights').rglob('*'))) if Path('/app/weights').exists() else 0; print(f'[DEV BUILD] Models directory: {models_count} files'); print(f'[DEV BUILD] Weights directory: {weights_count} files'); print('[DEV BUILD] Development image ready with local model cache!');"

# Set environment for development
ENV PYTHONUNBUFFERED=1
ENV CUDA_VISIBLE_DEVICES=0

# Create directories for mounted volumes
RUN mkdir -p /app/data /app/output /app/logs

EXPOSE 18011

CMD ["uv", "run", "python3", "api_server.py", "--log-level", "info", "--port", "18011"]
