{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "vZVb_drCaApt",
        "outputId": "f3280661-5d49-422c-df34-9112c0c92bd8"
      },
      "outputs": [],
      "source": [
        "# @title Videos with Top 3 Emotion Predictions (Code to make such Videos below in Cell 5)\n",
        "# Cell 0: YouTube Video Previews\n",
        "\n",
        "# --- How to Use This Cell ---\n",
        "# 1. YouTube Video Links or IDs:\n",
        "#    - Modify the `youtube_video_sources` list below.\n",
        "#    - You can provide full YouTube video URLs (e.g., \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\")\n",
        "#    - Or just the YouTube Video ID (e.g., \"dQw4w9WgXcQ\")\n",
        "#    - Or \"youtu.be\" short links (e.g., \"https://youtu.be/dQw4w9WgXcQ\")\n",
        "#    - You can also include timestamps in the URL (e.g., \"https://youtu.be/BUnfuiwE_IM?t=90\"),\n",
        "#      though the thumbnail will still be for the video itself, not the specific timestamp.\n",
        "#\n",
        "# 2. Thumbnail Quality (Optional):\n",
        "#    - `THUMBNAIL_QUALITY` can be one of:\n",
        "#      - \"default\": Standard quality (120x90)\n",
        "#      - \"mqdefault\": Medium quality (320x180)\n",
        "#      - \"hqdefault\": High quality (480x360)\n",
        "#      - \"sddefault\": Standard definition (640x480) - may not exist for all videos\n",
        "#      - \"maxresdefault\": Maximum resolution (1280x720 or 1920x1080) - may not exist for all videos\n",
        "#\n",
        "# 3. Run the Cell:\n",
        "#    - Execute this cell to display the video thumbnails.\n",
        "# --- End of How to Use ---\n",
        "\n",
        "import re\n",
        "from IPython.display import HTML, display\n",
        "from typing import Optional, List # <<<<<<<<<<<< ADDED IMPORT HERE\n",
        "\n",
        "# --- Configuration ---\n",
        "youtube_video_sources: List[str] = [ # Changed to List[str] for consistency\n",
        "    \"https://youtu.be/TsTVKCmqHhk\",\n",
        "    \"https://www.youtube.com/watch?v=sErqFgL4vA8\",\n",
        "    \"BUnfuiwE_IM\", # Just the ID\n",
        "    \"https://youtu.be/BUnfuiwE_IM?t=90\" # With timestamp\n",
        "    \"https://www.youtube.com/watch?v=dDrmjcUq8W4?t=74\"\n",
        "]\n",
        "\n",
        "THUMBNAIL_QUALITY: str = \"hqdefault\" # Options: default, mqdefault, hqdefault, sddefault, maxresdefault\n",
        "# THUMBNAILS_PER_ROW is implicitly handled by flexbox wrap\n",
        "\n",
        "# --- Helper Function to Extract Video ID ---\n",
        "def extract_youtube_id(url_or_id: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Extracts the YouTube video ID from various URL formats or if an ID is given directly.\n",
        "    \"\"\"\n",
        "    if not url_or_id:\n",
        "        return None\n",
        "\n",
        "    # Check if it's likely already an ID (11 characters, no typical URL parts)\n",
        "    if re.fullmatch(r\"[a-zA-Z0-9_-]{11}\", url_or_id):\n",
        "        return url_or_id\n",
        "\n",
        "    # Regex patterns for different YouTube URL formats\n",
        "    patterns = [\n",
        "        r\"(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/watch\\?v=([a-zA-Z0-9_-]{11})\",  # Standard watch URL\n",
        "        r\"(?:https?:\\/\\/)?youtu\\.be\\/([a-zA-Z0-9_-]{11})\",  # Shortened youtu.be URL\n",
        "        r\"(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/embed\\/([a-zA-Z0-9_-]{11})\", # Embed URL\n",
        "        r\"(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/v\\/([a-zA-Z0-9_-]{11})\", # /v/ URL\n",
        "        r\"(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/shorts\\/([a-zA-Z0-9_-]{11})\" # Shorts URL\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, url_or_id)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "\n",
        "    print(f\"Warning: Could not extract a valid YouTube ID from '{url_or_id}'\")\n",
        "    return None\n",
        "\n",
        "# --- Generate HTML for Thumbnails ---\n",
        "def generate_thumbnail_html(video_sources: List[str], quality: str) -> str: # Removed per_row as it's handled by flex\n",
        "    if not video_sources:\n",
        "        return \"<p>No video sources provided.</p>\"\n",
        "\n",
        "    html_parts = [\n",
        "        \"<div style='display: flex; flex-wrap: wrap; justify-content: flex-start; gap: 15px;'>\"\n",
        "    ]\n",
        "\n",
        "    valid_ids_count = 0\n",
        "    for source in video_sources:\n",
        "        video_id = extract_youtube_id(source)\n",
        "        if video_id:\n",
        "            valid_ids_count +=1\n",
        "            thumbnail_url = f\"https://img.youtube.com/vi/{video_id}/{quality}.jpg\"\n",
        "            video_watch_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "\n",
        "            # Dynamic width based on quality for better layout\n",
        "            width_style = \"max-width: 300px;\" # default max\n",
        "            if quality == \"default\": width_style = \"width: 120px;\"\n",
        "            elif quality == \"mqdefault\": width_style = \"width: 320px;\"\n",
        "            elif quality == \"hqdefault\": width_style = \"width: 480px; max-width: 480px;\"\n",
        "            elif quality == \"sddefault\": width_style = \"width: 640px; max-width: 640px;\"\n",
        "            elif quality == \"maxresdefault\": width_style = \"width: 100%; max-width: 720px;\" # Maxres can be large\n",
        "\n",
        "            item_style = f\"flex: 0 1 auto; margin-bottom: 15px; text-align: center; {width_style}\"\n",
        "\n",
        "            html_parts.append(f\"\"\"\n",
        "            <div style='{item_style}'>\n",
        "                <a href='{video_watch_url}' target='_blank' title='Watch video {video_id}'>\n",
        "                    <img src='{thumbnail_url}' alt='YouTube Thumbnail for {video_id}' style='width: 100%; height: auto; border: 1px solid #ccc; border-radius: 4px; display: block;'>\n",
        "                </a>\n",
        "                <p style='font-size: 0.8em; margin-top: 5px; word-break: break-all;'>ID: {video_id}</p>\n",
        "            </div>\n",
        "            \"\"\")\n",
        "\n",
        "    if valid_ids_count == 0:\n",
        "        html_parts.append(\"<p>No valid YouTube video IDs could be extracted from the provided sources.</p>\")\n",
        "\n",
        "    html_parts.append(\"</div>\")\n",
        "    return \"\".join(html_parts)\n",
        "\n",
        "# --- Display the HTML ---\n",
        "if not youtube_video_sources:\n",
        "    print(\"The 'youtube_video_sources' list is empty. Please add YouTube video URLs or IDs.\")\n",
        "    html_output = \"<p>Please configure the <code>youtube_video_sources</code> list in this cell.</p>\"\n",
        "else:\n",
        "    print(f\"Generating YouTube thumbnails for {len(youtube_video_sources)} source(s)...\")\n",
        "    html_output = generate_thumbnail_html(youtube_video_sources, THUMBNAIL_QUALITY)\n",
        "\n",
        "display(HTML(html_output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZllQalsJBft1",
        "outputId": "2d6c58fb-7788-4f8f-f54b-9bc9a858a120"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Setup and Dependencies\n",
        "!pip install transformers torch torchaudio torchvision librosa huggingface_hub numpy pydub ipython --quiet\n",
        "print(\"Dependencies installed.\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import gc\n",
        "import logging\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Any, Optional, Set\n",
        "import base64\n",
        "import io\n",
        "import shutil # For cleaning up downloaded models if needed\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display # For plotting waveform in HTML\n",
        "import matplotlib.pyplot as plt # For plotting waveform\n",
        "\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from huggingface_hub import snapshot_download, hf_hub_download\n",
        "from IPython.display import HTML, display, Audio as IPythonAudio\n",
        "\n",
        "try:\n",
        "    from pydub import AudioSegment\n",
        "    from pydub.exceptions import CouldntDecodeError, CouldntEncodeError\n",
        "    PYDUB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PYDUB_AVAILABLE = False\n",
        "    print(\"WARNING: pydub library not found. Audio player in HTML report (Cell 3) will not have playable audio. Install with: !pip install pydub\")\n",
        "except Exception as e:\n",
        "    PYDUB_AVAILABLE = False\n",
        "    print(f\"WARNING: Error initializing pydub (likely ffmpeg/avconv issue): {e}. Audio player in HTML report (Cell 3) will not have playable audio.\")\n",
        "\n",
        "# Setup basic logging\n",
        "def setup_notebook_logging(log_level=logging.INFO):\n",
        "    for handler in logging.root.handlers[:]:\n",
        "        logging.root.removeHandler(handler)\n",
        "        handler.close()\n",
        "    logging.basicConfig(\n",
        "        level=log_level,\n",
        "        format='%(asctime)s [%(levelname)-7s] %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S',\n",
        "        handlers=[logging.StreamHandler(sys.stdout)]\n",
        "    )\n",
        "setup_notebook_logging()\n",
        "\n",
        "# --- Global Configuration Toggles (User can modify these in Cell 2) ---\n",
        "# These will be properly defined and used in Cell 2.\n",
        "# This is just a placeholder comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7nFGFerCF8t",
        "outputId": "70d18785-8bc4-48d2-fac0-79a7f4110605"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Configuration, Demo File Download, Model Definitions, and MLP Path Discovery\n",
        "# This demo is not speed optimized and therefore pretty slow.\n",
        "# Atm it is loading every MLP model at a time,\n",
        "# because the RAM and the VRAM of the GPU here in Google Colab are not big enough to keep all.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import gc\n",
        "import logging\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Any, Optional\n",
        "import shutil\n",
        "import requests # For downloading demo files\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import librosa\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from huggingface_hub import snapshot_download, hf_hub_download\n",
        "\n",
        "# --- User-Modifiable Configuration ---\n",
        "# For MLP model execution\n",
        "USE_CPU_OFFLOADING_FOR_MLPS = True\n",
        "USE_HALF_PRECISION_FOR_MLPS = True\n",
        "USE_TORCH_COMPILE_FOR_MLPS = True\n",
        "\n",
        "WHISPER_MODEL_ID = \"mkrausio/EmoWhisper-AnS-Small-v0.1\"\n",
        "HF_MLP_REPO_ID = \"laion/Empathic-Insight-Voice-Small\"\n",
        "LOCAL_MLP_MODELS_DOWNLOAD_DIR = Path(\"./empathic_insight_voice_small_models_downloaded\")\n",
        "\n",
        "# --- Paths for Batch Processing ---\n",
        "# Create these folders in your Colab environment if they don't exist\n",
        "# !mkdir -p /content/batch_audio_input /content/batch_annotations_output_html /content/batch_annotations_output_json\n",
        "BATCH_INPUT_AUDIO_FOLDER = Path(\"/content/batch_audio_input\")\n",
        "BATCH_OUTPUT_HTML_REPORT_FILE = Path(\"/content/batch_annotations_output_html/batch_emotion_report.html\")\n",
        "BATCH_OUTPUT_JSON_FOLDER = Path(\"/content/batch_annotations_output_json\")\n",
        "\n",
        "DEMO_AUDIO_FILES_TO_DOWNLOAD = {\n",
        "    \"1.mp3\": \"https://huggingface.co/laion/Empathic-Insight-Voice-Small/resolve/main/1.mp3\",\n",
        "    \"2.mp3\": \"https://huggingface.co/laion/Empathic-Insight-Voice-Small/resolve/main/2.mp3\",\n",
        "    \"3.mp3\": \"https://huggingface.co/laion/Empathic-Insight-Voice-Small/resolve/main/3.mp3\",\n",
        "    \"4.mp3\": \"https://huggingface.co/laion/Empathic-Insight-Voice-Small/resolve/main/4.mp3\",\n",
        "}\n",
        "\n",
        "# --- Core Model & Audio Configuration ---\n",
        "SAMPLING_RATE = 16000\n",
        "MAX_AUDIO_SECONDS = 30.0\n",
        "WHISPER_SEQ_LEN: int = 1500\n",
        "WHISPER_EMBED_DIM: int = 768\n",
        "PROJECTION_DIM_FOR_FULL_EMBED: int = 64\n",
        "MLP_HIDDEN_DIMS: List[int] = [64, 32, 16]\n",
        "MLP_DROPOUTS: List[float] = [0.0, 0.1, 0.1, 0.1]\n",
        "\n",
        "SUPPORTED_AUDIO_EXTENSIONS = ['.mp3', '.wav', '.flac', '.m4a', '.ogg', '.aac']\n",
        "\n",
        "TARGET_EMOTION_KEYS_FOR_REPORT: List[str] = [\n",
        "    \"Amusement\", \"Elation\", \"Pleasure/Ecstasy\", \"Contentment\", \"Thankfulness/Gratitude\",\n",
        "    \"Affection\", \"Infatuation\", \"Hope/Enthusiasm/Optimism\", \"Triumph\", \"Pride\",\n",
        "    \"Interest\", \"Awe\", \"Astonishment/Surprise\", \"Concentration\", \"Contemplation\",\n",
        "    \"Relief\", \"Longing\", \"Teasing\", \"Impatience and Irritability\",\n",
        "    \"Sexual Lust\", \"Doubt\", \"Fear\", \"Distress\", \"Confusion\", \"Embarrassment\", \"Shame\",\n",
        "    \"Disappointment\", \"Sadness\", \"Bitterness\", \"Contempt\", \"Disgust\", \"Anger\",\n",
        "    \"Malevolence/Malice\", \"Sourness\", \"Pain\", \"Helplessness\", \"Fatigue/Exhaustion\",\n",
        "    \"Emotional Numbness\", \"Intoxication/Altered States of Consciousness\", \"Jealousy / Envy\"\n",
        "]\n",
        "assert len(TARGET_EMOTION_KEYS_FOR_REPORT) == 40\n",
        "\n",
        "FILENAME_PART_TO_TARGET_KEY_MAP: Dict[str, str] = {\n",
        "    \"Affection\": \"Affection\", \"Age\": \"Age\", \"Amusement\": \"Amusement\", \"Anger\": \"Anger\",\n",
        "    \"Arousal\": \"Arousal\", \"Astonishment_Surprise\": \"Astonishment/Surprise\",\n",
        "    \"Authenticity\": \"Authenticity\", \"Awe\": \"Awe\", \"Background_Noise\": \"Background_Noise\",\n",
        "    \"Bitterness\": \"Bitterness\", \"Concentration\": \"Concentration\",\n",
        "    \"Confident_vs._Hesitant\": \"Confident_vs._Hesitant\", \"Confusion\": \"Confusion\",\n",
        "    \"Contemplation\": \"Contemplation\", \"Contempt\": \"Contempt\", \"Contentment\": \"Contentment\",\n",
        "    \"Disappointment\": \"Disappointment\", \"Disgust\": \"Disgust\", \"Distress\": \"Distress\",\n",
        "    \"Doubt\": \"Doubt\", \"Elation\": \"Elation\", \"Embarrassment\": \"Embarrassment\",\n",
        "    \"Emotional_Numbness\": \"Emotional Numbness\", \"Fatigue_Exhaustion\": \"Fatigue/Exhaustion\",\n",
        "    \"Fear\": \"Fear\", \"Gender\": \"Gender\", \"Helplessness\": \"Helplessness\",\n",
        "    \"High-Pitched_vs._Low-Pitched\": \"High-Pitched_vs._Low-Pitched\",\n",
        "    \"Hope_Enthusiasm_Optimism\": \"Hope/Enthusiasm/Optimism\",\n",
        "    \"Impatience_and_Irritability\": \"Impatience and Irritability\",\n",
        "    \"Infatuation\": \"Infatuation\", \"Interest\": \"Interest\",\n",
        "    \"Intoxication_Altered_States_of_Consciousness\": \"Intoxication/Altered States of Consciousness\",\n",
        "    \"Jealousy_&_Envy\": \"Jealousy / Envy\", \"Longing\": \"Longing\",\n",
        "    \"Malevolence_Malice\": \"Malevolence/Malice\",\n",
        "    \"Monotone_vs._Expressive\": \"Monotone_vs._Expressive\", \"Pain\": \"Pain\",\n",
        "    \"Pleasure_Ecstasy\": \"Pleasure/Ecstasy\", \"Pride\": \"Pride\",\n",
        "    \"Recording_Quality\": \"Recording_Quality\", \"Relief\": \"Relief\", \"Sadness\": \"Sadness\",\n",
        "    \"Serious_vs._Humorous\": \"Serious_vs._Humorous\", \"Sexual_Lust\": \"Sexual Lust\",\n",
        "    \"Shame\": \"Shame\", \"Soft_vs._Harsh\": \"Soft_vs._Harsh\", \"Sourness\": \"Sourness\",\n",
        "    \"Submissive_vs._Dominant\": \"Submissive_vs._Dominant\", \"Teasing\": \"Teasing\",\n",
        "    \"Thankfulness_Gratitude\": \"Thankfulness/Gratitude\", \"Triumph\": \"Triumph\",\n",
        "    \"Valence\": \"Valence\",\n",
        "    \"Vulnerable_vs._Emotionally_Detached\": \"Vulnerable_vs._Emotionally_Detached\",\n",
        "    \"Warm_vs._Cold\": \"Warm_vs._Cold\"\n",
        "}\n",
        "\n",
        "# --- MLP Model Definition ---\n",
        "class FullEmbeddingMLP(nn.Module): # (Same as before)\n",
        "    def __init__(self,\n",
        "                 seq_len: int,\n",
        "                 embed_dim: int,\n",
        "                 projection_dim: int,\n",
        "                 mlp_hidden_dims: List[int],\n",
        "                 mlp_dropout_rates: List[float]):\n",
        "        super().__init__()\n",
        "        if len(mlp_dropout_rates) != len(mlp_hidden_dims) + 1:\n",
        "            raise ValueError(f\"Dropout rates length error. Expected {len(mlp_hidden_dims) + 1}, got {len(mlp_dropout_rates)}\")\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.proj = nn.Linear(seq_len * embed_dim, projection_dim)\n",
        "        layers = [nn.ReLU(), nn.Dropout(mlp_dropout_rates[0])]\n",
        "        current_dim = projection_dim\n",
        "        for i, h_dim in enumerate(mlp_hidden_dims):\n",
        "            layers.extend([\n",
        "                nn.Linear(current_dim, h_dim), nn.ReLU(), nn.Dropout(mlp_dropout_rates[i+1])\n",
        "            ])\n",
        "            current_dim = h_dim\n",
        "        layers.append(nn.Linear(current_dim, 1))\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.ndim == 4 and x.shape[1] == 1: x = x.squeeze(1)\n",
        "        return self.mlp(self.proj(self.flatten(x)))\n",
        "\n",
        "# --- Helper Functions (Adapted) ---\n",
        "def download_hf_mlp_checkpoints(repo_id: str, local_dir: Path, force_redownload: bool = False) -> Path:\n",
        "    if local_dir.exists() and force_redownload:\n",
        "        logging.info(f\"Force redownload: Removing existing directory {local_dir}\")\n",
        "        shutil.rmtree(local_dir)\n",
        "    if not local_dir.exists() or not any(local_dir.glob(\"*.pth\")):\n",
        "        logging.info(f\"Downloading MLP checkpoints from {repo_id} to {local_dir}...\")\n",
        "        local_dir.mkdir(parents=True, exist_ok=True)\n",
        "        try:\n",
        "            snapshot_download(repo_id=repo_id, local_dir=local_dir, local_dir_use_symlinks=False, allow_patterns=[\"*.pth\"], repo_type=\"model\")\n",
        "            logging.info(f\"MLP checkpoints downloaded to {local_dir}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to download MLP checkpoints from {repo_id}: {e}\", exc_info=True)\n",
        "            raise RuntimeError(f\"Could not download MLP checkpoints from {repo_id}.\")\n",
        "    else:\n",
        "        logging.info(f\"MLP checkpoints found in local cache: {local_dir}\")\n",
        "    return local_dir\n",
        "\n",
        "def download_demo_audio_files(target_dir: Path, files_to_download: Dict[str, str]):\n",
        "    target_dir.mkdir(parents=True, exist_ok=True)\n",
        "    logging.info(f\"Downloading demo audio files to {target_dir}...\")\n",
        "    for filename, url in files_to_download.items():\n",
        "        filepath = target_dir / filename\n",
        "        if filepath.exists():\n",
        "            logging.info(f\"Demo file {filename} already exists. Skipping download.\")\n",
        "            continue\n",
        "        try:\n",
        "            response = requests.get(url, stream=True)\n",
        "            response.raise_for_status()\n",
        "            with open(filepath, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            logging.info(f\"Downloaded {filename} to {filepath}\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logging.error(f\"Error downloading {filename} from {url}: {e}\")\n",
        "        except IOError as e:\n",
        "            logging.error(f\"Error writing {filename} to {filepath}: {e}\")\n",
        "\n",
        "def get_mlp_model_paths_map(mlp_checkpoints_dir: Path, filename_map: Dict[str, str]) -> Dict[str, Path]:\n",
        "    all_mapped_model_paths: Dict[str, Path] = {}\n",
        "    if not mlp_checkpoints_dir.is_dir():\n",
        "        logging.error(f\"MLP checkpoints directory not found: {mlp_checkpoints_dir.resolve()}\")\n",
        "        return {}\n",
        "    logging.info(f\"Mapping MLP model files from {mlp_checkpoints_dir.resolve()}...\")\n",
        "    for pth_file in mlp_checkpoints_dir.glob(\"model_*_best.pth\"):\n",
        "        try:\n",
        "            filename_part = pth_file.name.split(\"model_\")[1].split(\"_best.pth\")[0]\n",
        "            if filename_part in filename_map:\n",
        "                target_key = filename_map[filename_part]\n",
        "                if target_key in all_mapped_model_paths:\n",
        "                    logging.warning(f\"Duplicate mapping for target key '{target_key}'. Overwriting.\")\n",
        "                all_mapped_model_paths[target_key] = pth_file\n",
        "            # else:\n",
        "                # logging.debug(f\"Filename part '{filename_part}' not in map. Skipping {pth_file.name}\")\n",
        "        except IndexError:\n",
        "            logging.warning(f\"Could not parse filename part from {pth_file.name}. Skipping.\")\n",
        "    logging.info(f\"Found {len(all_mapped_model_paths)} MLP model paths based on map.\")\n",
        "    return all_mapped_model_paths\n",
        "\n",
        "def load_whisper_model(model_id: str, device: torch.device) -> Tuple[Optional[WhisperForConditionalGeneration], Optional[WhisperProcessor]]:\n",
        "    logging.info(f\"Loading Whisper model '{model_id}' to {device}...\")\n",
        "    try:\n",
        "        processor = WhisperProcessor.from_pretrained(model_id)\n",
        "        model = WhisperForConditionalGeneration.from_pretrained(model_id).to(device)\n",
        "        model.eval()\n",
        "        logging.info(f\"Whisper model '{model_id}' loaded to {device}.\")\n",
        "        return model, processor\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading Whisper model '{model_id}': {e}\", exc_info=True)\n",
        "        return None, None\n",
        "\n",
        "# --- Function to load a SINGLE MLP model ---\n",
        "def load_single_mlp_model(\n",
        "    model_path: Path,\n",
        "    target_key: str, # For logging\n",
        "    mlp_device: torch.device,\n",
        "    use_half_cfg: bool,\n",
        "    use_compile_cfg: bool\n",
        ") -> Optional[nn.Module]:\n",
        "    target_dtype = torch.float16 if use_half_cfg and mlp_device.type == 'cuda' else torch.float32\n",
        "    compile_mode = \"reduce-overhead\"\n",
        "    logging.debug(f\"Loading MLP for '{target_key}' from {model_path} to {mlp_device} (Half: {use_half_cfg}, Compile: {use_compile_cfg})\")\n",
        "    try:\n",
        "        model_instance = FullEmbeddingMLP(\n",
        "            seq_len=WHISPER_SEQ_LEN, embed_dim=WHISPER_EMBED_DIM,\n",
        "            projection_dim=PROJECTION_DIM_FOR_FULL_EMBED,\n",
        "            mlp_hidden_dims=MLP_HIDDEN_DIMS,\n",
        "            mlp_dropout_rates=MLP_DROPOUTS\n",
        "        )\n",
        "        state_dict_content = torch.load(model_path, map_location='cpu')\n",
        "        actual_state_dict = state_dict_content # Assuming raw state_dict from HF\n",
        "\n",
        "        needs_stripping = any(k.startswith(\"_orig_mod.\") for k in actual_state_dict.keys())\n",
        "        if needs_stripping:\n",
        "            stripped_state_dict = {\n",
        "                k[len(\"_orig_mod.\"):] if k.startswith(\"_orig_mod.\") else k: v\n",
        "                for k, v in actual_state_dict.items()\n",
        "            }\n",
        "            actual_state_dict = stripped_state_dict\n",
        "\n",
        "        model_instance.load_state_dict(actual_state_dict)\n",
        "        model_instance.eval()\n",
        "\n",
        "        if mlp_device.type == 'cuda' and use_half_cfg:\n",
        "            model_instance = model_instance.to(dtype=target_dtype)\n",
        "        model_instance = model_instance.to(mlp_device)\n",
        "\n",
        "        if use_compile_cfg and hasattr(torch, 'compile') and mlp_device.type == 'cuda' and torch.__version__ >= \"2.0.0\":\n",
        "            try:\n",
        "                model_instance = torch.compile(model_instance, mode=compile_mode)\n",
        "            except Exception as e_compile:\n",
        "                logging.warning(f\"torch.compile failed for MLP '{target_key}': {e_compile}. Using uncompiled.\")\n",
        "\n",
        "        logging.debug(f\"Successfully loaded MLP for '{target_key}'.\")\n",
        "        return model_instance\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load/prepare MLP for '{target_key}' from {model_path}: {e}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "# --- Determine Devices ---\n",
        "_whisper_device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "WHISPER_DEVICE = torch.device(_whisper_device_type)\n",
        "_mlp_device_type = \"cpu\" if USE_CPU_OFFLOADING_FOR_MLPS else _whisper_device_type\n",
        "MLP_DEVICE = torch.device(_mlp_device_type)\n",
        "\n",
        "logging.info(f\"Whisper will run on: {WHISPER_DEVICE}\")\n",
        "logging.info(f\"MLPs will run on: {MLP_DEVICE}\")\n",
        "\n",
        "# --- Preparations ---\n",
        "# 1. Create output directories\n",
        "BATCH_INPUT_AUDIO_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "BATCH_OUTPUT_HTML_REPORT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
        "BATCH_OUTPUT_JSON_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Download demo audio files into the BATCH_INPUT_AUDIO_FOLDER\n",
        "download_demo_audio_files(BATCH_INPUT_AUDIO_FOLDER, DEMO_AUDIO_FILES_TO_DOWNLOAD)\n",
        "\n",
        "# 3. Download MLP checkpoints from Hugging Face\n",
        "downloaded_mlp_checkpoints_dir = download_hf_mlp_checkpoints(HF_MLP_REPO_ID, LOCAL_MLP_MODELS_DOWNLOAD_DIR)\n",
        "\n",
        "# 4. Load Whisper Model (this stays loaded)\n",
        "whisper_model_global, whisper_processor_global = load_whisper_model(WHISPER_MODEL_ID, WHISPER_DEVICE)\n",
        "if not whisper_model_global or not whisper_processor_global:\n",
        "    raise RuntimeError(\"Failed to load Whisper model. Cannot proceed.\")\n",
        "\n",
        "# 5. Get paths and map for all MLP models (these will be loaded one by one later)\n",
        "# This dictionary {target_key: model_path_object} is crucial for Cells 3 & 4.\n",
        "all_mlp_model_paths_dict: Dict[str, Path] = get_mlp_model_paths_map(\n",
        "    downloaded_mlp_checkpoints_dir,\n",
        "    FILENAME_PART_TO_TARGET_KEY_MAP\n",
        ")\n",
        "if not all_mlp_model_paths_dict:\n",
        "    raise RuntimeError(\"No MLP model paths could be mapped or found. Cannot proceed.\")\n",
        "\n",
        "logging.info(f\"--- Cell 2 Setup Complete. Whisper model loaded. {len(all_mlp_model_paths_dict)} MLP model paths identified. ---\")\n",
        "logging.info(f\"Demo audio files are in: {BATCH_INPUT_AUDIO_FOLDER.resolve()}\")\n",
        "logging.info(f\"HTML report will be saved to: {BATCH_OUTPUT_HTML_REPORT_FILE.resolve()}\")\n",
        "logging.info(f\"JSON annotations will be saved to: {BATCH_OUTPUT_JSON_FOLDER.resolve()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GyDuf_rGEqM8",
        "outputId": "9f0e762d-6dcd-4daa-ae7f-def26dbca22b"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Batch HTML Report Generation (One MLP at a Time, with new HTML structure)\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "import matplotlib.pyplot as plt\n",
        "import base64 # For embedding images and audio in HTML\n",
        "import io\n",
        "import torch # For softmax\n",
        "\n",
        "try:\n",
        "    from pydub import AudioSegment\n",
        "    PYDUB_AVAILABLE = True # Assume it was set in Cell 1 or 2 if used\n",
        "except ImportError:\n",
        "    PYDUB_AVAILABLE = False\n",
        "    # logging.warning(\"pydub not available for audio embedding in HTML.\") # Logging setup in Cell 1/2\n",
        "\n",
        "\n",
        "# --- Helper functions (find_audio_files_in_folder, get_prediction_with_single_mlp,\n",
        "# --- get_whisper_embedding_for_audio, convert_audio_to_base64_mp3_for_html,\n",
        "# --- generate_waveform_plot_base64) are assumed to be the same as the previous good version.\n",
        "# --- Ensure they are defined or copy them here if running this cell standalone after Cell 2.\n",
        "\n",
        "# Re-define if necessary from previous correct version (or ensure Cell 2 definitions are accessible)\n",
        "def find_audio_files_in_folder(input_dir: Path) -> List[Path]:\n",
        "    audio_files = []\n",
        "    # Assuming SUPPORTED_AUDIO_EXTENSIONS is defined in Cell 2\n",
        "    for ext in SUPPORTED_AUDIO_EXTENSIONS:\n",
        "        audio_files.extend(list(input_dir.rglob(f'*{ext}')))\n",
        "    if not audio_files:\n",
        "        logging.warning(f\"No audio files found in {input_dir.resolve()}.\")\n",
        "    else:\n",
        "        logging.info(f\"Found {len(audio_files)} audio files in {input_dir.resolve()} for HTML report.\")\n",
        "    return sorted(audio_files)\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_prediction_with_single_mlp(\n",
        "    whisper_embedding: torch.Tensor,\n",
        "    mlp_model: nn.Module,\n",
        "    current_mlp_device: torch.device\n",
        ") -> float:\n",
        "    embedding_for_mlp = whisper_embedding.to(current_mlp_device)\n",
        "    try:\n",
        "        current_mlp_dtype = next(mlp_model.parameters()).dtype\n",
        "        prediction_tensor = mlp_model(embedding_for_mlp.to(current_mlp_dtype))\n",
        "        return prediction_tensor.item()\n",
        "    except Exception as e:\n",
        "        # logging.error(f\"Error predicting with a single MLP: {e}\", exc_info=True) # logging from Cell 1/2\n",
        "        return float('nan')\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_whisper_embedding_for_audio(\n",
        "    audio_waveform: np.ndarray,\n",
        "    loaded_whisper_model: nn.Module,\n",
        "    loaded_whisper_processor: any,\n",
        "    current_whisper_device: torch.device\n",
        ") -> Optional[torch.Tensor]:\n",
        "    try:\n",
        "        # Assuming SAMPLING_RATE, WHISPER_SEQ_LEN, WHISPER_EMBED_DIM defined in Cell 2\n",
        "        input_features = loaded_whisper_processor(\n",
        "            audio_waveform, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\"\n",
        "        ).input_features.to(current_whisper_device).to(loaded_whisper_model.dtype)\n",
        "\n",
        "        encoder_outputs = loaded_whisper_model.get_encoder()(input_features=input_features)\n",
        "        embedding = encoder_outputs.last_hidden_state\n",
        "\n",
        "        current_seq_len = embedding.shape[1]\n",
        "        if current_seq_len < WHISPER_SEQ_LEN:\n",
        "            padding = torch.zeros((1, WHISPER_SEQ_LEN - current_seq_len, WHISPER_EMBED_DIM),\n",
        "                                  device=current_whisper_device, dtype=embedding.dtype)\n",
        "            embedding = torch.cat((embedding, padding), dim=1)\n",
        "        elif current_seq_len > WHISPER_SEQ_LEN:\n",
        "            embedding = embedding[:, :WHISPER_SEQ_LEN, :]\n",
        "        return embedding\n",
        "    except Exception as e:\n",
        "        # logging.error(f\"Error generating Whisper embedding: {e}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "def convert_audio_to_base64_mp3_for_html(audio_path_str: str) -> Optional[str]:\n",
        "    if not PYDUB_AVAILABLE: return None\n",
        "    try:\n",
        "        # Assuming SAMPLING_RATE defined in Cell 2\n",
        "        audio = AudioSegment.from_file(audio_path_str)\n",
        "        audio = audio.set_channels(1).set_frame_rate(SAMPLING_RATE)\n",
        "        mp3_buffer = io.BytesIO()\n",
        "        audio.export(mp3_buffer, format=\"mp3\", bitrate=\"96k\")\n",
        "        return base64.b64encode(mp3_buffer.getvalue()).decode('utf-8')\n",
        "    except Exception as e:\n",
        "        # logging.warning(f\"Pydub/ffmpeg error for {audio_path_str}: {e}. No audio player.\")\n",
        "        return None\n",
        "\n",
        "def generate_waveform_plot_base64(waveform_data: np.ndarray, sr: int) -> str:\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 2.5))\n",
        "        librosa.display.waveshow(waveform_data, sr=sr, color='royalblue', alpha=0.7)\n",
        "        plt.title(\"Waveform\", fontsize=10)\n",
        "        plt.xlabel(\"Time (s)\", fontsize=8); plt.ylabel(\"Amplitude\", fontsize=8)\n",
        "        plt.xticks(fontsize=7); plt.yticks(fontsize=7)\n",
        "        plt.tight_layout()\n",
        "        img_buffer = io.BytesIO()\n",
        "        plt.savefig(img_buffer, format='png', bbox_inches='tight'); plt.close()\n",
        "        img_buffer.seek(0)\n",
        "        return base64.b64encode(img_buffer.read()).decode('utf-8')\n",
        "    except Exception as e:\n",
        "        # logging.warning(f\"Waveform plot failed: {e}\")\n",
        "        return \"\"\n",
        "# --- End of re-definitions/imports ---\n",
        "\n",
        "\n",
        "def generate_batch_html_report_updated(\n",
        "    all_files_scores: Dict[Path, Dict[str, float]], # {filepath: {dim_key: raw_score, ...}}\n",
        "    target_emotion_keys_list: List[str], # The 40 primary emotion keys\n",
        "    all_attribute_keys_list: List[str], # All other keys considered attributes\n",
        "    output_html_path: Path\n",
        "):\n",
        "    # HTML Explanation Section (as provided by user)\n",
        "    explanation_html = \"\"\"\n",
        "        <div class=\"explanation-section\">\n",
        "            <h3>Interpretation of Scores</h3>\n",
        "            <p>The models predict raw scores. For the 40 Emotional Categories, these raw scores are also used to calculate a normalized <strong>Softmax Probability</strong>, indicating the relative likelihood of each emotion.\n",
        "            Higher raw scores (shown in parentheses for emotions, or directly for attributes) generally suggest a stronger presence or intensity, aligning with the original annotation scales used during training.</p>\n",
        "\n",
        "            <h4>Emotional Categories (40)</h4>\n",
        "            <p><em>Original Annotation Scale: 0 (Not present at all) to 4 (Extremely present).</em><br>\n",
        "            The table below displays: <strong>Softmax Probability</strong> (Raw Model Score)</p>\n",
        "\n",
        "            <h4>Attribute Dimensions</h4>\n",
        "            <p><em>Original Annotation Scale: Varies per dimension (detailed below).</em><br>\n",
        "            The table for these dimensions displays: Raw Model Score</p>\n",
        "\n",
        "            <div class=\"dimension-details-section\">\n",
        "                <h4>Details for Attribute Dimensions:</h4>\n",
        "                <p><strong>Valence:</strong> <em>Range: -3 (Ext. Negative) to +3 (Ext. Positive).</em> 0=Neutral.</p>\n",
        "                <p><strong>Arousal:</strong> <em>Range: 0 (Very Calm) to 4 (Very Excited).</em> 2=Neutral.</p>\n",
        "                <p><strong>Submissive vs. Dominant:</strong> <em>Range: -3 (Ext. Submissive) to +3 (Ext. Dominant).</em> 0=Neutral.</p>\n",
        "                <p><strong>Age:</strong> <em>Range: 0 (Infant/Toddler) to 6 (Very Old).</em> (e.g., 2=Teenager, 4=Adult).</p>\n",
        "                <p><strong>Gender:</strong> <em>Range: -2 (Very Masculine) to +2 (Very Feminine).</em> 0=Neutral/Unsure.</p>\n",
        "                <p><strong>Serious vs. Humorous:</strong> <em>Range: 0 (Very Serious) to 4 (Very Humorous).</em> 2=Neutral.</p>\n",
        "                <p><strong>Vulnerable vs. Emotionally Detached:</strong> <em>Range: 0 (Very Vulnerable) to 4 (Very Detached).</em> 2=Neutral.</p>\n",
        "                <p><strong>Confident vs. Hesitant:</strong> <em>Range: 0 (Very Confident) to 4 (Very Hesitant).</em> 2=Neutral.</p>\n",
        "                <p><strong>Warm vs. Cold:</strong> <em>Range: -2 (Very Cold) to +2 (Very Warm).</em> 0=Neutral.</p>\n",
        "                <p><strong>Monotone vs. Expressive:</strong> <em>Range: 0 (Very Monotone) to 4 (Very Expressive).</em> 2=Neutral.</p>\n",
        "                <p><strong>High-Pitched vs. Low-Pitched:</strong> <em>Range: 0 (Very High-Pitched) to 4 (Very Low-Pitched).</em> 2=Neutral.</p>\n",
        "                <p><strong>Soft vs. Harsh:</strong> <em>Range: -2 (Very Harsh) to +2 (Very Soft).</em> 0=Neutral.</p>\n",
        "                <p><strong>Authenticity:</strong> <em>Range: 0 (Very Artificial) to 4 (Very Genuine).</em> 2=Neutral.</p>\n",
        "                <p><strong>Recording Quality:</strong> <em>Range: 0 (Very Low) to 4 (Very High).</em> 2=Decent.</p>\n",
        "                <p><strong>Background Noise:</strong> <em>Range: 0 (No Noise) to 3 (Intense Noise).</em></p>\n",
        "            </div>\n",
        "        </div>\n",
        "    \"\"\"\n",
        "\n",
        "    html_content = [f\"\"\"\n",
        "<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><title>Audio Inference Report</title>\n",
        "<style>\n",
        "    body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 15px; background-color: #f0f2f5; color: #333; font-size: 14px; }}\n",
        "    .report-container {{ max-width: 1000px; margin: auto; }}\n",
        "    .audio-item {{ background-color: #fff; border: 1px solid #e0e0e0; margin-bottom: 20px; padding: 15px; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.07); }}\n",
        "    h1 {{ text-align: center; color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; margin-bottom: 10px; }} /* Reduced margin-bottom */\n",
        "    h2 {{ margin-top: 0; color: #34495e; font-size: 1.3em; border-bottom: 1px dashed #bdc3c7; padding-bottom: 6px; margin-bottom: 12px; }}\n",
        "    h3 {{ color: #555; font-size: 1.1em; margin-top:15px; margin-bottom:8px;}}\n",
        "    h4 {{ color: #666; font-size: 1.0em; margin-top:10px; margin-bottom:5px;}}\n",
        "    table {{ width: 100%; border-collapse: collapse; margin-bottom: 15px; font-size: 0.9em;}}\n",
        "    th, td {{ border: 1px solid #ddd; padding: 7px; text-align: left; }}\n",
        "    th {{ background-color: #ecf0f1; font-weight: 600; }}\n",
        "    .top1 {{ background-color: #ffdddd !important; }} .top2 {{ background-color: #ffe8cc !important; }} .top3 {{ background-color: #ddffdd !important; }}\n",
        "    audio {{ width: 100%; margin-top: 8px; margin-bottom: 8px; }}\n",
        "    .waveform-img {{ width:100%; max-width:500px; margin-bottom:10px; border:1px solid #eee; border-radius:4px; display:block; margin-left:auto; margin-right:auto;}}\n",
        "    .explanation-section {{ background-color: #e9ecef; padding: 15px; border-radius: 5px; margin-bottom: 25px; border: 1px solid #ced4da;}}\n",
        "    .explanation-section p {{font-size: 0.95em; line-height: 1.5;}}\n",
        "    .dimension-details-section p {{ margin-bottom: 3px; font-size:0.9em; }}\n",
        "    .dimension-details-section strong {{ color: #34495e; }}\n",
        "</style></head><body><div class=\"report-container\"><h1>Audio Inference Report</h1>\n",
        "{explanation_html}\n",
        "\"\"\"] # Insert explanation here\n",
        "\n",
        "    if not all_files_scores:\n",
        "        html_content.append(\"<p>No audio files were processed or no results to display.</p>\")\n",
        "\n",
        "    for audio_file_path, raw_predictions in all_files_scores.items():\n",
        "        audio_file_name = audio_file_path.name\n",
        "        html_content.append(f\"<div class='audio-item'><h2>File: {audio_file_name}</h2>\")\n",
        "\n",
        "        try:\n",
        "            wf_data, wf_sr = librosa.load(str(audio_file_path), sr=SAMPLING_RATE, mono=True)\n",
        "            waveform_b64 = generate_waveform_plot_base64(wf_data, wf_sr)\n",
        "            if waveform_b64:\n",
        "                html_content.append(f\"<img src='data:image/png;base64,{waveform_b64}' alt='Waveform' class='waveform-img'/>\")\n",
        "            del wf_data\n",
        "        except Exception: pass\n",
        "\n",
        "        base64_audio_mp3 = convert_audio_to_base64_mp3_for_html(str(audio_file_path))\n",
        "        if base64_audio_mp3:\n",
        "            html_content.append(f\"<audio controls src='data:audio/mp3;base64,{base64_audio_mp3}'></audio>\")\n",
        "        else:\n",
        "            html_content.append(\"<p><i>Audio player not available.</i></p>\")\n",
        "\n",
        "        # Separate raw scores and calculate softmax for emotions\n",
        "        emotion_raw_scores_dict = {k: raw_predictions.get(k, float('nan')) for k in target_emotion_keys_list}\n",
        "        attribute_raw_scores_dict = {k: raw_predictions.get(k, float('nan')) for k in all_attribute_keys_list if k in raw_predictions}\n",
        "\n",
        "        # Softmax calculation\n",
        "        emotion_scores_for_softmax = [emotion_raw_scores_dict.get(k, -float('inf')) for k in target_emotion_keys_list] # Use -inf for missing, to avoid NaN in softmax\n",
        "        with torch.no_grad():\n",
        "            softmax_probs_tensor = torch.softmax(torch.tensor(emotion_scores_for_softmax, dtype=torch.float32), dim=0)\n",
        "        softmax_probs_dict = {k: softmax_probs_tensor[i].item() for i, k in enumerate(target_emotion_keys_list)}\n",
        "\n",
        "        # Sort by raw score for highlighting\n",
        "        sorted_emotions_by_raw_score = sorted(emotion_raw_scores_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "        top_raw_score_keys = [item[0] for item in sorted_emotions_by_raw_score[:3]]\n",
        "\n",
        "\n",
        "        html_content.append(\"<h3>Emotional Categories (40)</h3><table>\")\n",
        "        # Display in the original predefined order\n",
        "        emotions_in_table_order = target_emotion_keys_list[:]\n",
        "        num_rows_target = (len(emotions_in_table_order) + 1) // 2\n",
        "        for i in range(num_rows_target):\n",
        "            html_content.append(\"<tr>\")\n",
        "            for col in range(2):\n",
        "                idx = i + col * num_rows_target\n",
        "                if idx < len(emotions_in_table_order):\n",
        "                    key = emotions_in_table_order[idx]\n",
        "                    softmax_val = softmax_probs_dict.get(key, float('nan'))\n",
        "                    raw_val = emotion_raw_scores_dict.get(key, float('nan'))\n",
        "\n",
        "                    css_class = \"\"\n",
        "                    if key == (top_raw_score_keys[0] if len(top_raw_score_keys)>0 else None): css_class = \"top1\"\n",
        "                    elif key == (top_raw_score_keys[1] if len(top_raw_score_keys)>1 else None): css_class = \"top2\"\n",
        "                    elif key == (top_raw_score_keys[2] if len(top_raw_score_keys)>2 else None): css_class = \"top3\"\n",
        "\n",
        "                    html_content.append(f\"<td class='{css_class}'>{key}</td><td class='{css_class}'>{softmax_val:.4f} ({raw_val:.4f})</td>\")\n",
        "                else:\n",
        "                    html_content.append(\"<td></td><td></td>\")\n",
        "            html_content.append(\"</tr>\")\n",
        "        html_content.append(\"</table>\")\n",
        "\n",
        "        if attribute_raw_scores_dict:\n",
        "            html_content.append(\"<h3>Attribute Dimensions</h3><table>\")\n",
        "            sorted_additional = sorted(attribute_raw_scores_dict.items())\n",
        "            num_rows_additional = (len(sorted_additional) + 1) // 2\n",
        "            for i in range(num_rows_additional):\n",
        "                html_content.append(\"<tr>\")\n",
        "                for col in range(2):\n",
        "                    idx = i + col * num_rows_additional\n",
        "                    if idx < len(sorted_additional):\n",
        "                        key, score = sorted_additional[idx]\n",
        "                        html_content.append(f\"<td>{key}</td><td>{score:.4f}</td>\")\n",
        "                    else:\n",
        "                        html_content.append(\"<td></td><td></td>\")\n",
        "                html_content.append(\"</tr>\")\n",
        "            html_content.append(\"</table>\")\n",
        "        html_content.append(\"</div>\")\n",
        "\n",
        "    html_content.append(\"</div></body></html>\")\n",
        "    final_html = \"\".join(html_content)\n",
        "\n",
        "    try:\n",
        "        with open(output_html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(final_html)\n",
        "        logging.info(f\"Batch HTML report saved to: {output_html_path.resolve()}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error writing HTML report: {e}\", exc_info=True)\n",
        "\n",
        "    return final_html\n",
        "\n",
        "\n",
        "# --- Main execution for Cell 3 ---\n",
        "logging.info(\"--- Starting Cell 3: Batch HTML Report Generation (Updated) ---\")\n",
        "audio_files_for_html_report = find_audio_files_in_folder(BATCH_INPUT_AUDIO_FOLDER) # BATCH_INPUT_AUDIO_FOLDER from Cell 2\n",
        "aggregated_results_for_html: Dict[Path, Dict[str, float]] = {fp: {} for fp in audio_files_for_html_report}\n",
        "\n",
        "if not audio_files_for_html_report:\n",
        "    logging.warning(\"No audio files in input folder for HTML report. Skipping.\")\n",
        "    display(HTML(\"<p><b>No audio files found in input folder. HTML report generation skipped.</b></p>\"))\n",
        "else:\n",
        "    if not whisper_model_global or not whisper_processor_global: # from Cell 2\n",
        "         raise RuntimeError(\"Whisper model not available for Cell 3.\")\n",
        "    if not all_mlp_model_paths_dict: # from Cell 2\n",
        "         raise RuntimeError(\"MLP model paths not available for Cell 3.\")\n",
        "\n",
        "    total_mlps_to_process = len(all_mlp_model_paths_dict)\n",
        "    mlp_processed_count = 0\n",
        "\n",
        "    for mlp_target_key, mlp_model_path in all_mlp_model_paths_dict.items(): # all_mlp_model_paths_dict from Cell 2\n",
        "        mlp_processed_count += 1\n",
        "        logging.info(f\"Processing MLP {mlp_processed_count}/{total_mlps_to_process}: '{mlp_target_key}' for HTML report\")\n",
        "\n",
        "        # MLP_DEVICE, USE_HALF_PRECISION_FOR_MLPS, USE_TORCH_COMPILE_FOR_MLPS from Cell 2\n",
        "        current_mlp_model = load_single_mlp_model( # load_single_mlp_model from Cell 2\n",
        "            mlp_model_path, mlp_target_key, MLP_DEVICE,\n",
        "            USE_HALF_PRECISION_FOR_MLPS, USE_TORCH_COMPILE_FOR_MLPS\n",
        "        )\n",
        "        if not current_mlp_model:\n",
        "            logging.error(f\"Skipping MLP '{mlp_target_key}' due to loading error.\")\n",
        "            for audio_f_path in audio_files_for_html_report:\n",
        "                 aggregated_results_for_html[audio_f_path][mlp_target_key] = float('nan')\n",
        "            continue\n",
        "\n",
        "        for audio_file_path in audio_files_for_html_report:\n",
        "            try:\n",
        "                # SAMPLING_RATE, MAX_AUDIO_SECONDS from Cell 2\n",
        "                waveform, sr = librosa.load(str(audio_file_path), sr=SAMPLING_RATE, mono=True)\n",
        "                max_samples = int(MAX_AUDIO_SECONDS * SAMPLING_RATE)\n",
        "                if len(waveform) > max_samples: waveform = waveform[:max_samples]\n",
        "\n",
        "                # WHISPER_DEVICE from Cell 2\n",
        "                whisper_embedding = get_whisper_embedding_for_audio(\n",
        "                    waveform, whisper_model_global, whisper_processor_global, WHISPER_DEVICE\n",
        "                )\n",
        "                del waveform; gc.collect()\n",
        "\n",
        "                if whisper_embedding is not None:\n",
        "                    prediction = get_prediction_with_single_mlp(\n",
        "                        whisper_embedding, current_mlp_model, MLP_DEVICE\n",
        "                    )\n",
        "                    aggregated_results_for_html[audio_file_path][mlp_target_key] = prediction\n",
        "                    del whisper_embedding;\n",
        "                else:\n",
        "                    aggregated_results_for_html[audio_file_path][mlp_target_key] = float('nan')\n",
        "            except Exception as e_audio:\n",
        "                logging.error(f\"Error processing audio {audio_file_path.name} for MLP {mlp_target_key}: {e_audio}\")\n",
        "                aggregated_results_for_html[audio_file_path][mlp_target_key] = float('nan')\n",
        "\n",
        "            if WHISPER_DEVICE.type == 'cuda': torch.cuda.empty_cache()\n",
        "\n",
        "        del current_mlp_model\n",
        "        gc.collect()\n",
        "        if MLP_DEVICE.type == 'cuda': torch.cuda.empty_cache()\n",
        "\n",
        "    # Derive attribute keys (all keys from MLP models that are NOT in the 40 emotion list)\n",
        "    # all_mlp_model_paths_dict.keys() gives all predictable dimension keys\n",
        "    # TARGET_EMOTION_KEYS_FOR_REPORT is defined in Cell 2\n",
        "    all_predictable_keys = set(all_mlp_model_paths_dict.keys())\n",
        "    emotion_keys_set = set(TARGET_EMOTION_KEYS_FOR_REPORT)\n",
        "    attribute_keys_derived = sorted(list(all_predictable_keys - emotion_keys_set))\n",
        "\n",
        "    # BATCH_OUTPUT_HTML_REPORT_FILE from Cell 2\n",
        "    final_html_output = generate_batch_html_report_updated(\n",
        "        aggregated_results_for_html,\n",
        "        TARGET_EMOTION_KEYS_FOR_REPORT, # from Cell 2\n",
        "        attribute_keys_derived,\n",
        "        BATCH_OUTPUT_HTML_REPORT_FILE\n",
        "    )\n",
        "    display(HTML(final_html_output))\n",
        "    logging.info(\"--- Cell 3: Batch HTML Report Generation (Updated) Finished ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "TeTsSzzAE3Ib",
        "outputId": "d6191a10-41c6-4873-b1fd-11298e3044ef"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Batch JSON Annotation (One MLP at a Time)\n",
        "\n",
        "def batch_annotate_to_json(\n",
        "    input_audio_folder: Path,\n",
        "    output_json_folder: Path,\n",
        "    mlp_model_paths_map: Dict[str, Path], # {target_key: model_path}\n",
        "    global_whisper_model: WhisperForConditionalGeneration,\n",
        "    global_whisper_processor: WhisperProcessor\n",
        "):\n",
        "    audio_files_for_json = find_audio_files_in_folder(input_audio_folder)\n",
        "    output_json_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if not audio_files_for_json:\n",
        "        logging.warning(\"No audio files in input folder for JSON annotation. Skipping.\")\n",
        "        return\n",
        "\n",
        "    logging.info(f\"Starting JSON batch annotation for {len(audio_files_for_json)} files...\")\n",
        "    total_files = len(audio_files_for_json)\n",
        "\n",
        "    for i, audio_file_path in enumerate(audio_files_for_json):\n",
        "        logging.info(f\"--- JSON Processing file {i+1}/{total_files}: {audio_file_path.name} ---\")\n",
        "        current_file_all_scores: Dict[str, float] = {}\n",
        "\n",
        "        # Get Whisper embedding for this audio file (once)\n",
        "        # logging.debug(f\"  Getting Whisper embedding for JSON: {audio_file_path.name}\")\n",
        "        try:\n",
        "            waveform, sr = librosa.load(str(audio_file_path), sr=SAMPLING_RATE, mono=True)\n",
        "            max_samples = int(MAX_AUDIO_SECONDS * SAMPLING_RATE)\n",
        "            if len(waveform) > max_samples: waveform = waveform[:max_samples]\n",
        "\n",
        "            audio_whisper_embedding = get_whisper_embedding_for_audio(\n",
        "                waveform, global_whisper_model, global_whisper_processor, WHISPER_DEVICE\n",
        "            )\n",
        "            del waveform; gc.collect()\n",
        "        except Exception as e_emb:\n",
        "            logging.error(f\"Could not get embedding for {audio_file_path.name}: {e_emb}. Skipping this file for JSON.\")\n",
        "            if WHISPER_DEVICE.type == 'cuda': torch.cuda.empty_cache()\n",
        "            continue # Skip to next audio file\n",
        "\n",
        "        if audio_whisper_embedding is None:\n",
        "            logging.warning(f\"Whisper embedding failed for {audio_file_path.name}. Skipping this file for JSON.\")\n",
        "            if WHISPER_DEVICE.type == 'cuda': torch.cuda.empty_cache()\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Inner loop: Iterate through MLPs for the current audio file\n",
        "        total_mlps_to_process_for_file = len(mlp_model_paths_map)\n",
        "        mlp_processed_count_for_file = 0\n",
        "        for mlp_target_key, mlp_model_path in mlp_model_paths_map.items():\n",
        "            mlp_processed_count_for_file +=1\n",
        "            # logging.debug(f\"    MLP {mlp_processed_count_for_file}/{total_mlps_to_process_for_file} ('{mlp_target_key}') for {audio_file_path.name}\")\n",
        "\n",
        "            current_mlp_model = load_single_mlp_model(\n",
        "                mlp_model_path, mlp_target_key, MLP_DEVICE,\n",
        "                USE_HALF_PRECISION_FOR_MLPS, USE_TORCH_COMPILE_FOR_MLPS\n",
        "            )\n",
        "            if not current_mlp_model:\n",
        "                logging.error(f\"Skipping MLP '{mlp_target_key}' for file '{audio_file_path.name}' due to loading error.\")\n",
        "                current_file_all_scores[mlp_target_key] = float('nan')\n",
        "                continue\n",
        "\n",
        "            prediction = get_prediction_with_single_mlp(\n",
        "                audio_whisper_embedding, current_mlp_model, MLP_DEVICE # Pass the cached embedding\n",
        "            )\n",
        "            current_file_all_scores[mlp_target_key] = prediction\n",
        "\n",
        "            del current_mlp_model # Unload current MLP\n",
        "            gc.collect()\n",
        "            if MLP_DEVICE.type == 'cuda': torch.cuda.empty_cache()\n",
        "\n",
        "        del audio_whisper_embedding # Free embedding for this audio file\n",
        "        if WHISPER_DEVICE.type == 'cuda': torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "        # Save all collected scores for the current audio file to JSON\n",
        "        relative_path_from_input = audio_file_path.relative_to(input_audio_folder)\n",
        "        output_json_subfolder = output_json_folder / relative_path_from_input.parent\n",
        "        output_json_subfolder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        output_json_filename = audio_file_path.stem + \".json\"\n",
        "        output_json_path = output_json_subfolder / output_json_filename\n",
        "\n",
        "        try:\n",
        "            with open(output_json_path, 'w') as f:\n",
        "                json.dump(current_file_all_scores, f, indent=2)\n",
        "            logging.info(f\"Saved annotations to {output_json_path}\")\n",
        "        except IOError as e:\n",
        "            logging.error(f\"Error writing JSON for {audio_file_path.name}: {e}\")\n",
        "        except TypeError as e: # Handle potential non-serializable if NaNs are not handled by json\n",
        "             logging.error(f\"TypeError saving JSON for {audio_file_path.name} (likely NaN issue): {e}\")\n",
        "             # Try again by converting NaNs to null or string\n",
        "             cleaned_scores = {k: (None if np.isnan(v) else v) for k,v in current_file_all_scores.items()}\n",
        "             try:\n",
        "                 with open(output_json_path, 'w') as f:\n",
        "                     json.dump(cleaned_scores, f, indent=2)\n",
        "                 logging.info(f\"Saved annotations (with NaN->null) to {output_json_path}\")\n",
        "             except Exception as e2:\n",
        "                 logging.error(f\"Still failed to save JSON for {audio_file_path.name} after NaN handling: {e2}\")\n",
        "\n",
        "\n",
        "    logging.info(f\"--- JSON Batch Annotation Finished for {len(audio_files_for_json)} files. ---\")\n",
        "\n",
        "\n",
        "# --- Main execution for Cell 4 ---\n",
        "logging.info(\"--- Starting Cell 4: Batch JSON Annotation ---\")\n",
        "\n",
        "# Using whisper_model_global, whisper_processor_global, and all_mlp_model_paths_dict from Cell 2\n",
        "if not whisper_model_global or not whisper_processor_global:\n",
        "     raise RuntimeError(\"Whisper model not available for Cell 4.\")\n",
        "if not all_mlp_model_paths_dict:\n",
        "     raise RuntimeError(\"MLP model paths not available for Cell 4.\")\n",
        "\n",
        "batch_annotate_to_json(\n",
        "    BATCH_INPUT_AUDIO_FOLDER,\n",
        "    BATCH_OUTPUT_JSON_FOLDER,\n",
        "    all_mlp_model_paths_dict, # From Cell 2\n",
        "    whisper_model_global,     # From Cell 2\n",
        "    whisper_processor_global  # From Cell 2\n",
        ")\n",
        "display(HTML(f\"<p><b>JSON batch annotation complete. Check logs and output folder: '{BATCH_OUTPUT_JSON_FOLDER.resolve()}'</b></p>\"))\n",
        "logging.info(\"--- Cell 4: Batch JSON Annotation Finished ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJbNHd0DZwP-"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Standalone Video Processing Script with Emotion Subtitles\n",
        "\n",
        "# --- How to Use This Script ---\n",
        "# 1. Dependencies:\n",
        "#    This script requires several Python libraries. If you haven't installed them in your\n",
        "#    current environment, uncomment and run the following pip install command (or install them manually):\n",
        "#    !pip install transformers torch torchvision torchaudio moviepy librosa numpy --quiet\n",
        "#\n",
        "# 2. Configure Paths (IMPORTANT!):\n",
        "#    - HARDCODED_INPUT_FOLDERS: List of paths to folders containing your input video files.\n",
        "#      Example: [\"/content/my_videos_folder_1\", \"/content/another_video_collection\"]\n",
        "#    - HARDCODED_OUTPUT_FOLDER: Path where processed videos and SRT files will be saved.\n",
        "#      Example: \"/content/processed_emotion_videos\"\n",
        "#    - HARDCODED_MLP_MODELS_DIR: ABSOLUTE PATH to the directory where the\n",
        "#      'Empathic-Insight-Voice-Small' MLP model checkpoints (*.pth files) are stored.\n",
        "#      This is the folder you downloaded from laion/Empathic-Insight-Voice-Small,\n",
        "#      e.g., the 'empathic_insight_voice_small_models_downloaded' folder from previous cells,\n",
        "#      or if you downloaded them elsewhere, point to that full path.\n",
        "#      Example: \"/content/empathic_insight_voice_small_models_downloaded\" OR\n",
        "#               \"/mnt/nvme/empathic-insights-voice-small/\" (as in your example)\n",
        "#\n",
        "# 3. FFmpeg (Optional but Recommended for Burned-in Subtitles):\n",
        "#    - For burning subtitles directly into the video, FFmpeg is required.\n",
        "#    - If FFmpeg is in your system's PATH, HARDCODED_FFMPEG_PATH can be `None`.\n",
        "#    - Otherwise, provide the full path to the ffmpeg executable.\n",
        "#      Example: HARDCODED_FFMPEG_PATH = \"/usr/bin/ffmpeg\"\n",
        "#    - If FFmpeg is not found, SRT files will still be generated, but videos with\n",
        "#      burned-in subtitles will not.\n",
        "#\n",
        "# 4. Run the Script:\n",
        "#    - After configuring, execute this cell.\n",
        "#\n",
        "# 5. Output:\n",
        "#    - For each input video, an SRT file with emotion predictions per chunk will be created.\n",
        "#    - If FFmpeg is available, a new MP4 video file with these emotions burned in as\n",
        "#      subtitles will also be created.\n",
        "#    - All output files will be in the HARDCODED_OUTPUT_FOLDER.\n",
        "#      Filenames will be like: <InputFolderName>_<VideoBaseName>_<Suffix>.srt/.mp4\n",
        "# --- End of How to Use ---\n",
        "\n",
        "# --- Start of Python Script ---\n",
        "# (The script content from your \"subtitle burn-in script\" example,\n",
        "#  adapted for this cell and requirements, will go here)\n",
        "\n",
        "# Ensure we are in a clean state for this script's execution context\n",
        "_video_script_initialized = False\n",
        "if '_video_script_initialized' not in globals() or not _video_script_initialized:\n",
        "    # Python standard library imports\n",
        "    import os\n",
        "    import gc\n",
        "    import logging\n",
        "    import time\n",
        "    from pathlib import Path\n",
        "    from typing import List, Dict, Tuple, Any, Optional\n",
        "    import sys\n",
        "    import shutil\n",
        "    import subprocess # For FFmpeg\n",
        "\n",
        "    # Third-party library imports\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    from moviepy.editor import VideoFileClip\n",
        "    import librosa\n",
        "    from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "    _video_script_initialized = True\n",
        "    print(\"Video script environment initialized/re-initialized.\")\n",
        "\n",
        "\n",
        "# --- Configuration Block (Edit these values directly) ---\n",
        "HARDCODED_INPUT_FOLDERS: List[str] = [\"/content/sample_input_videos\"] # << USER: EDIT THIS\n",
        "HARDCODED_OUTPUT_FOLDER: str = \"/content/processed_emotion_videos\"    # << USER: EDIT THIS\n",
        "HARDCODED_MLP_MODELS_DIR: str = \"/content/empathic_insight_voice_small_models_downloaded\" # << USER: EDIT THIS (e.g. result of Cell 2)\n",
        "\n",
        "HARDCODED_OUTPUT_SUFFIX: str = \"emotion_subs\"\n",
        "HARDCODED_CHUNK_DURATION_S: float = 2.5  # Duration of each audio chunk for emotion analysis\n",
        "HARDCODED_FFMPEG_PATH: Optional[str] = None # e.g., \"/usr/bin/ffmpeg\" or None to auto-detect\n",
        "HARDCODED_TEMP_AUDIO_BASE_NAME: str = \"temp_video_audio_\"\n",
        "\n",
        "# --- General Configuration ---\n",
        "DEFAULT_PROCESSING_SAMPLING_RATE: int = 16000 # For audio extraction\n",
        "DEFAULT_TEXT_TOP_N_EMOTIONS: int = 3 # How many top emotions to show in SRT\n",
        "VIDEO_EXTENSIONS = {'.mp4', '.mkv', '.avi', '.mov', '.webm', '.flv', '.wmv'}\n",
        "\n",
        "# --- Parameters for the Emotion Prediction MLP part (Must match Empathic-Insight-Voice-Small) ---\n",
        "USE_CPU_OFFLOADING_FOR_MLPS: bool = True\n",
        "USE_HALF_PRECISION_FOR_MLPS: bool = True\n",
        "USE_TORCH_COMPILE_FOR_MLPS: bool = True # Requires PyTorch 2.0+\n",
        "EMOTION_WHISPER_MODEL_ID: str = \"mkrausio/EmoWhisper-AnS-Small-v0.1\"\n",
        "MLP_SAMPLING_RATE: int = 16000 # For MLP model input\n",
        "WHISPER_SEQ_LEN: int = 1500\n",
        "WHISPER_EMBED_DIM: int = 768\n",
        "PROJECTION_DIM_FOR_FULL_EMBED: int = 64\n",
        "MLP_HIDDEN_DIMS: List[int] = [64, 32, 16]\n",
        "MLP_DROPOUTS: List[float] = [0.0, 0.1, 0.1, 0.1]\n",
        "\n",
        "TARGET_EMOTION_KEYS: List[str] = [\n",
        "    \"Amusement\", \"Elation\", \"Pleasure/Ecstasy\", \"Contentment\", \"Thankfulness/Gratitude\",\n",
        "    \"Affection\", \"Infatuation\", \"Hope/Enthusiasm/Optimism\", \"Triumph\", \"Pride\",\n",
        "    \"Interest\", \"Awe\", \"Astonishment/Surprise\", \"Concentration\", \"Contemplation\",\n",
        "    \"Relief\", \"Longing\", \"Teasing\", \"Impatience and Irritability\",\n",
        "    \"Sexual Lust\", \"Doubt\", \"Fear\", \"Distress\", \"Confusion\", \"Embarrassment\", \"Shame\",\n",
        "    \"Disappointment\", \"Sadness\", \"Bitterness\", \"Contempt\", \"Disgust\", \"Anger\",\n",
        "    \"Malevolence/Malice\", \"Sourness\", \"Pain\", \"Helplessness\", \"Fatigue/Exhaustion\",\n",
        "    \"Emotional Numbness\", \"Intoxication/Altered States of Consciousness\", \"Jealousy / Envy\"\n",
        "]\n",
        "assert len(TARGET_EMOTION_KEYS) == 40\n",
        "\n",
        "FILENAME_PART_TO_TARGET_KEY_MAP: Dict[str, str] = {\n",
        "    \"Affection\": \"Affection\", \"Age\": \"Age\", \"Amusement\": \"Amusement\", \"Anger\": \"Anger\",\n",
        "    \"Arousal\": \"Arousal\", \"Astonishment_Surprise\": \"Astonishment/Surprise\",\n",
        "    \"Authenticity\": \"Authenticity\", \"Awe\": \"Awe\", \"Background_Noise\": \"Background_Noise\",\n",
        "    \"Bitterness\": \"Bitterness\", \"Concentration\": \"Concentration\",\n",
        "    \"Confident_vs._Hesitant\": \"Confident_vs._Hesitant\", \"Confusion\": \"Confusion\",\n",
        "    \"Contemplation\": \"Contemplation\", \"Contempt\": \"Contempt\", \"Contentment\": \"Contentment\",\n",
        "    \"Disappointment\": \"Disappointment\", \"Disgust\": \"Disgust\", \"Distress\": \"Distress\",\n",
        "    \"Doubt\": \"Doubt\", \"Elation\": \"Elation\", \"Embarrassment\": \"Embarrassment\",\n",
        "    \"Emotional_Numbness\": \"Emotional Numbness\", \"Fatigue_Exhaustion\": \"Fatigue/Exhaustion\",\n",
        "    \"Fear\": \"Fear\", \"Gender\": \"Gender\", \"Helplessness\": \"Helplessness\",\n",
        "    \"High-Pitched_vs._Low-Pitched\": \"High-Pitched_vs._Low-Pitched\",\n",
        "    \"Hope_Enthusiasm_Optimism\": \"Hope/Enthusiasm/Optimism\",\n",
        "    \"Impatience_and_Irritability\": \"Impatience and Irritability\", \"Infatuation\": \"Infatuation\",\n",
        "    \"Interest\": \"Interest\",\n",
        "    \"Intoxication_Altered_States_of_Consciousness\": \"Intoxication/Altered States of Consciousness\",\n",
        "    \"Jealousy_&_Envy\": \"Jealousy / Envy\", \"Longing\": \"Longing\",\n",
        "    \"Malevolence_Malice\": \"Malevolence/Malice\", \"Monotone_vs._Expressive\": \"Monotone_vs._Expressive\",\n",
        "    \"Pain\": \"Pain\", \"Pleasure_Ecstasy\": \"Pleasure/Ecstasy\", \"Pride\": \"Pride\",\n",
        "    \"Recording_Quality\": \"Recording_Quality\", \"Relief\": \"Relief\", \"Sadness\": \"Sadness\",\n",
        "    \"Serious_vs._Humorous\": \"Serious_vs._Humorous\", \"Sexual_Lust\": \"Sexual Lust\",\n",
        "    \"Shame\": \"Shame\", \"Soft_vs._Harsh\": \"Soft_vs._Harsh\", \"Sourness\": \"Sourness\",\n",
        "    \"Submissive_vs._Dominant\": \"Submissive_vs._Dominant\", \"Teasing\": \"Teasing\",\n",
        "    \"Thankfulness_Gratitude\": \"Thankfulness/Gratitude\", \"Triumph\": \"Triumph\",\n",
        "    \"Valence\": \"Valence\", \"Vulnerable_vs._Emotionally_Detached\": \"Vulnerable_vs._Emotionally_Detached\",\n",
        "    \"Warm_vs._Cold\": \"Warm_vs._Cold\"\n",
        "}\n",
        "# --- End of Configuration Block ---\n",
        "\n",
        "# --- Global Variables for Models (initialized once per script run) ---\n",
        "_video_emotion_whisper_model: Optional[WhisperForConditionalGeneration] = None\n",
        "_video_emotion_whisper_processor: Optional[WhisperProcessor] = None\n",
        "_video_mlp_models_ensemble: Optional[Dict[str, nn.Module]] = None\n",
        "_video_emotion_whisper_device: Optional[torch.device] = None\n",
        "_video_mlp_device: Optional[torch.device] = None\n",
        "_video_MLP_MODELS_BASE_DIR_CONFIG: Optional[str] = None\n",
        "\n",
        "def setup_video_script_logging(log_level=logging.INFO):\n",
        "    \"\"\"Sets up basic logging for this script.\"\"\"\n",
        "    # Clear existing handlers for this specific logger context if needed,\n",
        "    # but usually basicConfig handles it if run once.\n",
        "    for handler in logging.root.handlers[:]:\n",
        "        logging.root.removeHandler(handler)\n",
        "        handler.close()\n",
        "    logging.basicConfig(\n",
        "        level=log_level,\n",
        "        format='%(asctime)s [VIDEO_SCRIPT] [%(levelname)-7s] %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S',\n",
        "        handlers=[logging.StreamHandler(sys.stdout)],\n",
        "        force=True # Ensure it takes over in notebook env\n",
        "    )\n",
        "\n",
        "class VideoFullEmbeddingMLP(nn.Module): # Identical to MLP class in previous cells\n",
        "    def __init__(self, seq_len, embed_dim, projection_dim, mlp_hidden_dims, mlp_dropout_rates):\n",
        "        super().__init__()\n",
        "        if len(mlp_dropout_rates) != len(mlp_hidden_dims) + 1:\n",
        "            raise ValueError(\"Dropout rates length error.\")\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.proj = nn.Linear(seq_len * embed_dim, projection_dim)\n",
        "        layers = [nn.ReLU(), nn.Dropout(mlp_dropout_rates[0])]\n",
        "        current_dim = projection_dim\n",
        "        for i, h_dim in enumerate(mlp_hidden_dims):\n",
        "            layers.extend([nn.Linear(current_dim, h_dim), nn.ReLU(), nn.Dropout(mlp_dropout_rates[i+1])])\n",
        "            current_dim = h_dim\n",
        "        layers.append(nn.Linear(current_dim, 1))\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 4 and x.shape[1] == 1: x = x.squeeze(1)\n",
        "        return self.mlp(self.proj(self.flatten(x)))\n",
        "\n",
        "def get_video_mlp_model_paths_map(mlp_models_dir_str, filename_map):\n",
        "    mlp_models_dir = Path(mlp_models_dir_str)\n",
        "    mapped_paths: Dict[str, Path] = {}\n",
        "    if not mlp_models_dir.is_dir():\n",
        "        logging.error(f\"[VIDEO_SCRIPT] MLP models directory not found: {mlp_models_dir.resolve()}\")\n",
        "        return {}\n",
        "    for fname_part, target_key in filename_map.items():\n",
        "        chkpt_path = mlp_models_dir / f\"model_{fname_part}_best.pth\"\n",
        "        if chkpt_path.is_file():\n",
        "            if target_key in mapped_paths:\n",
        "                 logging.warning(f\"[VIDEO_SCRIPT] Duplicate mapping for '{target_key}'. Overwriting.\")\n",
        "            mapped_paths[target_key] = chkpt_path\n",
        "    if not mapped_paths:\n",
        "        logging.warning(f\"[VIDEO_SCRIPT] No MLP models mapped from {mlp_models_dir_str}.\")\n",
        "    return mapped_paths\n",
        "\n",
        "def load_video_emotion_models_once():\n",
        "    global _video_emotion_whisper_model, _video_emotion_whisper_processor, _video_mlp_models_ensemble\n",
        "    global _video_emotion_whisper_device, _video_mlp_device, _video_MLP_MODELS_BASE_DIR_CONFIG\n",
        "\n",
        "    if _video_emotion_whisper_model is not None: return # Already loaded\n",
        "\n",
        "    _video_MLP_MODELS_BASE_DIR_CONFIG = HARDCODED_MLP_MODELS_DIR # Use hardcoded value\n",
        "    if not _video_MLP_MODELS_BASE_DIR_CONFIG or not Path(_video_MLP_MODELS_BASE_DIR_CONFIG).is_dir():\n",
        "        logging.critical(f\"[VIDEO_SCRIPT] MLP Models Directory '{_video_MLP_MODELS_BASE_DIR_CONFIG}' is invalid. Cannot load models.\")\n",
        "        raise FileNotFoundError(f\"Invalid MLP Models Directory: {_video_MLP_MODELS_BASE_DIR_CONFIG}\")\n",
        "\n",
        "    logging.info(\"[VIDEO_SCRIPT] Loading emotion prediction models for video script...\")\n",
        "    _video_emotion_whisper_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    _video_mlp_device = torch.device(\"cpu\") if USE_CPU_OFFLOADING_FOR_MLPS else _video_emotion_whisper_device\n",
        "\n",
        "    logging.info(f\"[VIDEO_SCRIPT] Emotion Whisper on: {_video_emotion_whisper_device}, MLPs on: {_video_mlp_device}\")\n",
        "\n",
        "    try:\n",
        "        _video_emotion_whisper_processor = WhisperProcessor.from_pretrained(EMOTION_WHISPER_MODEL_ID)\n",
        "        _video_emotion_whisper_model = WhisperForConditionalGeneration.from_pretrained(EMOTION_WHISPER_MODEL_ID)\n",
        "        _video_emotion_whisper_model = _video_emotion_whisper_model.to(_video_emotion_whisper_device).eval()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[VIDEO_SCRIPT] Error loading Emotion Whisper model: {e}\", exc_info=True); raise\n",
        "\n",
        "    all_model_paths = get_video_mlp_model_paths_map(_video_MLP_MODELS_BASE_DIR_CONFIG, FILENAME_PART_TO_TARGET_KEY_MAP)\n",
        "    if not all_model_paths:\n",
        "        raise ValueError(\"[VIDEO_SCRIPT] No MLP model paths found/mapped. Emotion prediction cannot proceed.\")\n",
        "\n",
        "    _video_mlp_models_ensemble = {}\n",
        "    target_dtype = torch.float16 if USE_HALF_PRECISION_FOR_MLPS and _video_mlp_device.type == 'cuda' else torch.float32\n",
        "\n",
        "    for target_key, model_path in all_model_paths.items():\n",
        "        try:\n",
        "            model_instance = VideoFullEmbeddingMLP(\n",
        "                WHISPER_SEQ_LEN, WHISPER_EMBED_DIM, PROJECTION_DIM_FOR_FULL_EMBED,\n",
        "                MLP_HIDDEN_DIMS, MLP_DROPOUTS\n",
        "            )\n",
        "            state_dict = torch.load(model_path, map_location='cpu')\n",
        "            actual_state_dict = state_dict # Assuming raw state_dict from HF\n",
        "            if any(k.startswith(\"_orig_mod.\") for k in actual_state_dict.keys()): # Handle torch.compile\n",
        "                actual_state_dict = {k[len(\"_orig_mod.\"):] if k.startswith(\"_orig_mod.\") else k: v for k, v in actual_state_dict.items()}\n",
        "\n",
        "            model_instance.load_state_dict(actual_state_dict)\n",
        "            model_instance.eval()\n",
        "            if _video_mlp_device.type == 'cuda' and USE_HALF_PRECISION_FOR_MLPS:\n",
        "                model_instance = model_instance.to(dtype=target_dtype)\n",
        "            model_instance = model_instance.to(_video_mlp_device)\n",
        "            if USE_TORCH_COMPILE_FOR_MLPS and hasattr(torch, 'compile') and _video_mlp_device.type == 'cuda' and torch.__version__ >= \"2.0.0\":\n",
        "                model_instance = torch.compile(model_instance, mode=\"reduce-overhead\")\n",
        "            _video_mlp_models_ensemble[target_key] = model_instance\n",
        "        except Exception as e:\n",
        "            logging.error(f\"[VIDEO_SCRIPT] Failed to load MLP '{target_key}': {e}\", exc_info=True)\n",
        "    logging.info(f\"[VIDEO_SCRIPT] Loaded {len(_video_mlp_models_ensemble)} MLP models.\")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_emotions_for_video_waveform(audio_waveform: np.ndarray) -> Optional[Dict[str, float]]:\n",
        "    if _video_emotion_whisper_model is None:\n",
        "        logging.error(\"[VIDEO_SCRIPT] Video emotion models not loaded. Call load_video_emotion_models_once().\")\n",
        "        return None\n",
        "    try:\n",
        "        input_features = _video_emotion_whisper_processor(\n",
        "            audio_waveform, sampling_rate=MLP_SAMPLING_RATE, return_tensors=\"pt\"\n",
        "        ).input_features.to(_video_emotion_whisper_device).to(_video_emotion_whisper_model.dtype)\n",
        "\n",
        "        embedding = _video_emotion_whisper_model.get_encoder()(input_features=input_features).last_hidden_state\n",
        "        current_seq_len = embedding.shape[1]\n",
        "        if current_seq_len < WHISPER_SEQ_LEN:\n",
        "            padding = torch.zeros((1, WHISPER_SEQ_LEN - current_seq_len, WHISPER_EMBED_DIM),\n",
        "                                  device=_video_emotion_whisper_device, dtype=embedding.dtype)\n",
        "            embedding = torch.cat((embedding, padding), dim=1)\n",
        "        elif current_seq_len > WHISPER_SEQ_LEN:\n",
        "            embedding = embedding[:, :WHISPER_SEQ_LEN, :]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[VIDEO_SCRIPT] Error generating Whisper embedding for video: {e}\", exc_info=True); return None\n",
        "\n",
        "    predictions: Dict[str, float] = {}\n",
        "    embedding_for_mlps = embedding.to(_video_mlp_device)\n",
        "    del embedding;\n",
        "    if _video_emotion_whisper_device.type == 'cuda' and _video_emotion_whisper_device != _video_mlp_device: torch.cuda.empty_cache()\n",
        "\n",
        "    for key, mlp in _video_mlp_models_ensemble.items():\n",
        "        try:\n",
        "            dtype = next(mlp.parameters()).dtype\n",
        "            pred_tensor = mlp(embedding_for_mlps.to(dtype))\n",
        "            predictions[key] = pred_tensor.item()\n",
        "        except Exception as e:\n",
        "            logging.error(f\"[VIDEO_SCRIPT] Error predicting with MLP '{key}': {e}\"); predictions[key] = float('nan')\n",
        "\n",
        "    del embedding_for_mlps;\n",
        "    if _video_mlp_device.type == 'cuda': torch.cuda.empty_cache();\n",
        "    gc.collect()\n",
        "    return predictions\n",
        "\n",
        "def get_video_top_n_emotions(predictions, emotion_keys, top_n):\n",
        "    relevant = {k: predictions.get(k, -float('inf')) for k in emotion_keys if k in predictions}\n",
        "    if not relevant: return [(\"N/A\", 0.0)]\n",
        "    return sorted(relevant.items(), key=lambda item: item[1], reverse=True)[:top_n]\n",
        "\n",
        "def extract_audio_from_video_moviepy(video_path, audio_output_path, target_sr):\n",
        "    logging.info(f\"[VIDEO_SCRIPT] Extracting audio: '{video_path}' -> '{audio_output_path}' @ {target_sr}Hz\")\n",
        "    clip = None\n",
        "    try:\n",
        "        clip = VideoFileClip(str(video_path))\n",
        "        if clip.audio is None: logging.error(f\"[VIDEO_SCRIPT] No audio in '{video_path}'.\"); return False\n",
        "        clip.audio.write_audiofile(str(audio_output_path), fps=target_sr, codec='pcm_s16le', logger=None)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[VIDEO_SCRIPT] Audio extraction error for '{video_path}': {e}\", exc_info=True); return False\n",
        "    finally:\n",
        "        if clip: clip.close();\n",
        "        if clip and clip.audio: clip.audio.close()\n",
        "\n",
        "\n",
        "def format_srt_time(seconds: float) -> str:\n",
        "    millis = int(round((seconds - int(seconds)) * 1000))\n",
        "    s, m, h = int(seconds), 0, 0\n",
        "    if s >= 60: m = s // 60; s %= 60\n",
        "    if m >= 60: h = m // 60; m %= 60\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d},{millis:03d}\"\n",
        "\n",
        "def write_video_srt_file(filepath: Path, entries: List[Dict[str, Any]]):\n",
        "    logging.info(f\"[VIDEO_SCRIPT] Writing {len(entries)} SRT entries to: {filepath}\")\n",
        "    try:\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            for i, entry in enumerate(entries):\n",
        "                f.write(f\"{i + 1}\\n{entry['start_str']} --> {entry['end_str']}\\n{entry['text']}\\n\\n\")\n",
        "    except IOError as e:\n",
        "        logging.error(f\"[VIDEO_SCRIPT] Failed to write SRT {filepath}: {e}\", exc_info=True)\n",
        "\n",
        "\n",
        "def process_single_video_for_srt(video_path, srt_path, temp_audio_path, chunk_dur, proc_sr, top_n_disp):\n",
        "    if not extract_audio_from_video_moviepy(video_path, temp_audio_path, proc_sr):\n",
        "        logging.error(f\"[VIDEO_SCRIPT] Audio extraction failed for {video_path.name}. Skipping.\"); return False\n",
        "    if not temp_audio_path.is_file() or temp_audio_path.stat().st_size == 0:\n",
        "        logging.error(f\"[VIDEO_SCRIPT] Temp audio {temp_audio_path} invalid. Skipping {video_path.name}.\"); return False\n",
        "\n",
        "    try:\n",
        "        total_dur = librosa.get_duration(path=str(temp_audio_path))\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[VIDEO_SCRIPT] Cannot get duration of {temp_audio_path}: {e}. Skipping.\"); return False\n",
        "\n",
        "    num_chunks = int(np.floor(total_dur / chunk_dur)) if total_dur >= chunk_dur else (1 if total_dur > 0.1 else 0)\n",
        "    if num_chunks == 0:\n",
        "        logging.info(f\"[VIDEO_SCRIPT] No chunks for {video_path.name}. SRT will be empty.\"); write_video_srt_file(srt_path, []); return True\n",
        "\n",
        "    srt_entries = []\n",
        "    for i in range(num_chunks):\n",
        "        start_s, end_s = i * chunk_dur, (i + 1) * chunk_dur\n",
        "        end_s = min(end_s, total_dur) # Ensure end_s doesn't exceed total_dur for the last chunk\n",
        "        actual_chunk_dur = end_s - start_s\n",
        "        if actual_chunk_dur < 0.1 : continue # Skip tiny remainder\n",
        "\n",
        "        logging.info(f\"[VIDEO_SCRIPT] Chunk {i+1}/{num_chunks} for {video_path.name}: {start_s:.2f}s-{end_s:.2f}s\")\n",
        "        try:\n",
        "            # MLP_SAMPLING_RATE is used for loading the segment for MLPs\n",
        "            segment_wf, _ = librosa.load(str(temp_audio_path), sr=MLP_SAMPLING_RATE, mono=True, offset=start_s, duration=actual_chunk_dur)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"[VIDEO_SCRIPT] Error loading chunk for {video_path.name}: {e}\"); continue\n",
        "        if len(segment_wf) == 0: logging.warning(f\"[VIDEO_SCRIPT] Empty waveform for chunk in {video_path.name}\"); continue\n",
        "\n",
        "        raw_preds = predict_emotions_for_video_waveform(segment_wf)\n",
        "        emo_text = \"No predictions\"\n",
        "        if raw_preds:\n",
        "            top_emos = get_video_top_n_emotions(raw_preds, TARGET_EMOTION_KEYS, top_n_disp)\n",
        "            emo_text = \"\\n\".join([f\"{e}: {s:.2f}\" for e, s in top_emos if e != \"N/A\"]) or \"N/A\"\n",
        "\n",
        "        srt_entries.append({'start_str': format_srt_time(start_s), 'end_str': format_srt_time(end_s), 'text': emo_text})\n",
        "        del segment_wf, raw_preds; gc.collect();\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "    write_video_srt_file(srt_path, srt_entries)\n",
        "    if temp_audio_path.exists():\n",
        "        try: os.remove(temp_audio_path)\n",
        "        except OSError as e: logging.warning(f\"[VIDEO_SCRIPT] Could not delete temp audio {temp_audio_path}: {e}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def find_ffmpeg(user_path=None):\n",
        "    candidates = [user_path] if user_path else []\n",
        "    candidates.extend([\"ffmpeg\", \"ffmpeg.exe\"])\n",
        "    for cmd in candidates:\n",
        "        try:\n",
        "            if subprocess.run([cmd, \"-version\"], capture_output=True, timeout=5, check=True).returncode == 0:\n",
        "                logging.info(f\"[VIDEO_SCRIPT] Found FFmpeg: {cmd}\"); return cmd\n",
        "        except Exception: pass\n",
        "    logging.error(\"[VIDEO_SCRIPT] FFmpeg not found. Burn-in skipped.\"); return None\n",
        "\n",
        "def burn_subs_ffmpeg(vid_in, srt_in, vid_out, ffmpeg_exe):\n",
        "    if not srt_in.is_file(): logging.error(f\"[VIDEO_SCRIPT] SRT {srt_in} not found for burn-in.\"); return False\n",
        "    logging.info(f\"[VIDEO_SCRIPT] Burning subs: '{srt_in.name}' into '{vid_in.name}' -> '{vid_out.name}'\")\n",
        "\n",
        "    # FFmpeg requires careful path escaping, especially on Windows\n",
        "    srt_path_ffmpeg = str(srt_in.resolve()).replace('\\\\', '/')\n",
        "    if sys.platform == \"win32\": # Double escape colons for Windows drive letters\n",
        "        drive, tail = os.path.splitdrive(srt_path_ffmpeg)\n",
        "        if drive: srt_path_ffmpeg = drive.replace(\":\", \"\\\\\\\\:\") + tail\n",
        "\n",
        "    cmd = [\n",
        "        ffmpeg_exe, \"-y\", \"-i\", str(vid_in),\n",
        "        \"-vf\", f\"subtitles='{srt_path_ffmpeg}':force_style='FontName=Arial,Fontsize=20,PrimaryColour=&HFFFFFF&,BorderStyle=1,Outline=1,OutlineColour=&H000000&,Shadow=0.5,Alignment=2'\", # White text, black outline, bottom center\n",
        "        \"-c:v\", \"libx264\", \"-preset\", \"fast\", \"-crf\", \"23\",\n",
        "        \"-c:a\", \"aac\", \"-b:a\", \"128k\", str(vid_out)\n",
        "    ]\n",
        "    logging.debug(f\"[VIDEO_SCRIPT] FFmpeg cmd: {' '.join(cmd)}\")\n",
        "    try:\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True, check=False) # check=False to inspect stderr\n",
        "        if result.returncode != 0:\n",
        "            logging.error(f\"[VIDEO_SCRIPT] FFmpeg error for {vid_in.name}:\\n{result.stderr}\")\n",
        "            return False\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logging.error(f\"[VIDEO_SCRIPT] FFmpeg execution error for {vid_in.name}: {e}\", exc_info=True); return False\n",
        "\n",
        "\n",
        "def video_script_main_flow():\n",
        "    setup_video_script_logging()\n",
        "    logging.info(\"[VIDEO_SCRIPT] --- Starting Video Processing Script ---\")\n",
        "\n",
        "    # Create output folder if it doesn't exist\n",
        "    output_dir = Path(HARDCODED_OUTPUT_FOLDER)\n",
        "    try:\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    except OSError as e:\n",
        "        logging.critical(f\"[VIDEO_SCRIPT] Cannot create output directory {output_dir}: {e}. Aborting.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        load_video_emotion_models_once()\n",
        "    except Exception as e:\n",
        "        logging.critical(f\"[VIDEO_SCRIPT] Failed to load emotion models: {e}. Aborting.\", exc_info=True)\n",
        "        return\n",
        "\n",
        "    ffmpeg_executable = find_ffmpeg(HARDCODED_FFMPEG_PATH)\n",
        "\n",
        "    all_videos = []\n",
        "    for folder_str in HARDCODED_INPUT_FOLDERS:\n",
        "        in_folder = Path(folder_str)\n",
        "        if not in_folder.is_dir():\n",
        "            logging.warning(f\"[VIDEO_SCRIPT] Input folder {in_folder} not found. Skipping.\")\n",
        "            continue\n",
        "        for item in in_folder.rglob('*'): # rglob for recursive search\n",
        "            if item.is_file() and item.suffix.lower() in VIDEO_EXTENSIONS:\n",
        "                all_videos.append(item)\n",
        "\n",
        "    if not all_videos:\n",
        "        logging.info(\"[VIDEO_SCRIPT] No video files found in specified input folders. Exiting.\")\n",
        "        return\n",
        "\n",
        "    logging.info(f\"[VIDEO_SCRIPT] Found {len(all_videos)} videos to process.\")\n",
        "\n",
        "    for idx, video_file in enumerate(all_videos):\n",
        "        logging.info(f\"[VIDEO_SCRIPT] Processing video {idx+1}/{len(all_videos)}: {video_file.name}\")\n",
        "        start_vid_time = time.time()\n",
        "\n",
        "        # Create unique names to avoid collisions if multiple input folders have same video names\n",
        "        input_parent_dir_name = video_file.parent.name if video_file.parent.name != \".\" else \"root_input\"\n",
        "        base_name_for_output = f\"{input_parent_dir_name}_{video_file.stem}\"\n",
        "\n",
        "        srt_file = output_dir / f\"{base_name_for_output}_{HARDCODED_OUTPUT_SUFFIX}.srt\"\n",
        "        temp_audio_file = output_dir / f\"{HARDCODED_TEMP_AUDIO_BASE_NAME}{base_name_for_output}.wav\" # Unique temp audio\n",
        "\n",
        "        srt_ok = process_single_video_for_srt(\n",
        "            video_file, srt_file, temp_audio_file,\n",
        "            HARDCODED_CHUNK_DURATION_S, DEFAULT_PROCESSING_SAMPLING_RATE, DEFAULT_TEXT_TOP_N_EMOTIONS\n",
        "        )\n",
        "\n",
        "        if srt_ok and ffmpeg_executable and srt_file.is_file() and srt_file.stat().st_size > 0:\n",
        "            output_video_file = output_dir / f\"{base_name_for_output}_{HARDCODED_OUTPUT_SUFFIX}.mp4\"\n",
        "            burn_subs_ffmpeg(video_file, srt_file, output_video_file, ffmpeg_executable)\n",
        "        elif not ffmpeg_executable:\n",
        "            logging.info(f\"[VIDEO_SCRIPT] SRT file for {video_file.name} is at {srt_file}. FFmpeg burn-in skipped.\")\n",
        "        elif not srt_ok or not srt_file.is_file() or srt_file.stat().st_size == 0:\n",
        "            logging.warning(f\"[VIDEO_SCRIPT] SRT generation failed or empty for {video_file.name}. Burn-in skipped.\")\n",
        "\n",
        "\n",
        "        logging.info(f\"[VIDEO_SCRIPT] Finished {video_file.name} in {time.time() - start_vid_time:.2f}s\")\n",
        "        gc.collect() # Clean up per video\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "    logging.info(\"[VIDEO_SCRIPT] --- Video Processing Script Finished ---\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\": # This check is good practice but might not be strictly necessary in a Colab cell\n",
        "    # Create dummy input folder and a dummy MLP model dir for testing if they don't exist\n",
        "    # This is just for making the cell runnable standalone without prior setup from other cells.\n",
        "    # In a real workflow, HARDCODED_MLP_MODELS_DIR should point to the actual downloaded models.\n",
        "\n",
        "    # Dummy MLP model directory setup for testing this cell standalone\n",
        "    example_mlp_dir = Path(HARDCODED_MLP_MODELS_DIR)\n",
        "    if not example_mlp_dir.exists():\n",
        "        print(f\"[INFO_FOR_TESTING] MLP model dir '{example_mlp_dir}' not found. Please set HARDCODED_MLP_MODELS_DIR correctly.\")\n",
        "        print(\"[INFO_FOR_TESTING] For a quick test, you can create a dummy file like:\")\n",
        "        print(f\"  !mkdir -p {example_mlp_dir}\")\n",
        "        print(f\"  !touch {example_mlp_dir}/model_Amusement_best.pth\")\n",
        "        # This dummy file won't work for actual predictions but allows the script to run further.\n",
        "\n",
        "    # Dummy input video directory setup\n",
        "    example_input_video_dir = Path(HARDCODED_INPUT_FOLDERS[0] if HARDCODED_INPUT_FOLDERS else \"./dummy_videos\")\n",
        "    if not example_input_video_dir.exists():\n",
        "        example_input_video_dir.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"[INFO_FOR_TESTING] Created dummy input video directory: {example_input_video_dir}\")\n",
        "        print(f\"[INFO_FOR_TESTING] Please place some .mp4 (or other supported) videos in it to test.\")\n",
        "        # You might want to download a short sample video for testing:\n",
        "        # !wget -q -O {example_input_video_dir}/sample_video.mp4 http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4\n",
        "        # print(f\"[INFO_FOR_TESTING] Downloaded a sample video to {example_input_video_dir}/sample_video.mp4 for testing.\")\n",
        "\n",
        "\n",
        "    # Execute the main flow\n",
        "    video_script_main_flow()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
